{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) 2024 Manuel Razo. This work is licensed under a [Creative Commons\n",
    "Attribution License CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/).\n",
    "All code contained herein is licensed under an [MIT\n",
    "license](https://opensource.org/licenses/MIT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project package\n",
    "import Antibiotic\n",
    "\n",
    "# Import package to handle DataFrames\n",
    "import DataFrames as DF\n",
    "import CSV\n",
    "\n",
    "# Import library for Bayesian inference\n",
    "import Turing\n",
    "import MCMCChains\n",
    "import FillArrays\n",
    "\n",
    "# Import library to list files\n",
    "import Glob\n",
    "\n",
    "# Import packages to work with data\n",
    "import DataFrames as DF\n",
    "\n",
    "# Load CairoMakie for plotting\n",
    "using CairoMakie\n",
    "import PairPlots\n",
    "import ColorSchemes\n",
    "\n",
    "# Import basic math libraries\n",
    "import LsqFit\n",
    "import StatsBase\n",
    "import LinearAlgebra\n",
    "import Random\n",
    "import Distances\n",
    "import Distributions\n",
    "\n",
    "# Activate backend\n",
    "CairoMakie.activate!()\n",
    "\n",
    "# Set PBoC Plotting style\n",
    "Antibiotic.viz.theme_makie!()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference of $IC_{50}$ values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will perform Bayesian inference on the $IC_{50}$ values of\n",
    "the antibiotic resistance landscape. For this, we will use the raw `OD620`\n",
    "measurements provided by Iwasawa et al. (2022).\n",
    "\n",
    "Let's begin by loading the data into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into a DataFrame\n",
    "df = CSV.read(\n",
    "    \"$(git_root())/data/Iwasawa_2022/iwasawa_tidy.csv\", DF.DataFrame\n",
    ")\n",
    "\n",
    "first(df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To double-check that the structure of the table makes sense, let's plot the time\n",
    "series for one example to see if the sequence agrees with the expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data to use\n",
    "data = df[\n",
    "    (df.antibiotic.==\"KM\").&(df.env.==\"Parent_in_KM\").&(df.strain_num.==13).&.!(df.blank).&(df.concentration_ugmL.>0),\n",
    "    :]\n",
    "# Remove blank measurement\n",
    "# Group data by day\n",
    "df_group = DF.groupby(data, :day)\n",
    "\n",
    "# Initialize figure\n",
    "fig = Figure(size=(500, 300))\n",
    "\n",
    "# Add axis\n",
    "ax = Axis(\n",
    "    fig[1, 1],\n",
    "    xlabel=\"antibiotic concentration\",\n",
    "    ylabel=\"OD₆₂₀\",\n",
    "    xscale=log2\n",
    ")\n",
    "\n",
    "# Define colors for plot\n",
    "colors = get(ColorSchemes.Blues_9, LinRange(0.25, 1, length(df_group)))\n",
    "\n",
    "# Loop through days\n",
    "for (i, d) in enumerate(df_group)\n",
    "    # Sort data by concentration\n",
    "    DF.sort!(d, :concentration_ugmL)\n",
    "    # Plot scatter line\n",
    "    scatterlines!(\n",
    "        ax, d.concentration_ugmL, d.OD, color=colors[i], label=\"$(first(d.day))\"\n",
    "    )\n",
    "end # for\n",
    "\n",
    "# Add legend to plot\n",
    "fig[1, 2] = Legend(\n",
    "    fig, ax, \"day\", framevisible=false, nbanks=3, labelsize=10\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functional form used by the authors to fit the data is\n",
    "$$\n",
    "f(x) = \\frac{a}\n",
    "{1+\\exp \\left[b\\left(\\log _2 x-\\log _2 \\mathrm{IC}_{50}\\right)\\right]} + c\n",
    "\\tag{1}\n",
    "$$\n",
    "where $a$, $b$, and $c$ are nuisance parameters of the model, $\\mathrm{IC}_{50}$\n",
    "is the parameter of interest, and $x$ is the antibiotic concentration. We can\n",
    "define a function to compute this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@doc raw\"\"\"\n",
    "    logistic(logx, a, b, c, logic50)\n",
    "\n",
    "Compute the logistic function used to model the relationship between antibiotic\n",
    "concentration and bacterial growth, using log2 inputs for concentration and\n",
    "IC₅₀.\n",
    "\n",
    "This function implements the following equation:\n",
    "\n",
    "f(x) = a / (1 + exp(b * (log(x) - log(IC₅₀)))) + c\n",
    "\n",
    "# Arguments\n",
    "- `logx`: log of the antibiotic concentration (input variable)\n",
    "- `a`: Maximum effect parameter (difference between upper and lower asymptotes)\n",
    "- `b`: Slope parameter (steepness of the curve)\n",
    "- `c`: Minimum effect parameter (lower asymptote)\n",
    "- `logic50`: log of the IC₅₀ parameter\n",
    "\n",
    "# Returns\n",
    "The computed effect (e.g., optical density) for the given log₂ antibiotic\n",
    "concentration and parameters.\n",
    "\n",
    "Note: This function is vectorized and can handle array inputs for `log2x`.\n",
    "\"\"\"\n",
    "function logistic(logx, a, b, c, logic50)\n",
    "    return @. a / (1.0 + exp(b * (logx - logic50))) + c\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian definition of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our particular problem in question we have our variate—-the\n",
    "log-concentration $x$--and our covariate--the log-optical density $y$. In\n",
    "this context if we were to write a condition likelihood it would take the form\n",
    "$$\n",
    "\\pi(y \\mid f(x), x),\n",
    "\\tag{2}\n",
    "$$\n",
    "where $f(x)$ is a function that depends on the variates. A process is define as\n",
    "setting a probability distribution over functions rather than over numbers.\n",
    "Formally this becomes an infinite-dimensional Hilbert space of functions, or as\n",
    "I like to thing about it: there are infinite possible functional relationships\n",
    "between variates and covariates. The question is then how to set a probability\n",
    "distribution over this infinite dimensional space? We can limit ourselves to a\n",
    "subset of possible functions. In particular a Gaussian process, just as a\n",
    "regular Gaussian distribution, is defined by a mean function $m$ and a\n",
    "covariance function $k$, i.e.\n",
    "$$\n",
    "f \\sim \\mathcal{GP}(m, k).\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "What this means is that given our alele frequency measurements $y$ measured at\n",
    "time $t$ we want to compute $f(t)$. For this we write Bayes theorem as\n",
    "$$\n",
    "\\pi(f(x_*) \\mid y, x_*) = \n",
    "\\frac{\\pi(y \\mid f(x_*), x_*) \\pi(f(x_*) \\mid x_*)}{\\pi(y \\mid x_*)},\n",
    "\\tag{4}\n",
    "$$\n",
    "where $x_*$ indicates that we **sampled a finite number of concentrations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and covariance functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean function $m(x)$ defines the basal value to which all realizations of\n",
    "the Gaussian process will converge. In other words, the average of many many\n",
    "samples of the Gaussian process would converge to this value. As it is common\n",
    "practice, given the lack of *a priori* information of such convergence value, we\n",
    "will set $m(x)$ to be zero.\n",
    "\n",
    "The covariance function, also known as covariance kernel $k(x_1, x_2)$, sets the\n",
    "variation of the Gaussian process around the mean function. Larger covariance\n",
    "between two variates (time points) $x_1$ and $x_2$, means that their outputs\n",
    "$f(x_1)$ and $f(x_2)$ cannot vary that much between them. We then expect that\n",
    "the covariance between two time points should decay as the time gap between them\n",
    "increases. From the several possible covariance functions that are commonly use\n",
    "in practice we will focus on the so-called *exponential quadratic* kernel,\n",
    "$$\n",
    "k(x_1, x_2) = \\alpha^2 \\exp \\left(-\\frac{(x_1 - x_2)^2}{\\rho^2} \\right).\n",
    "\\tag{5}\n",
    "$$\n",
    "Here $\\alpha$, known as the *marginal standard deviation*, controls the expected\n",
    "variation in the variate output, while $\\rho$, known as the *length scale* (or\n",
    "in our specific case the time scale), sets the scale by which the variate\n",
    "outputs change with respect to the distance between covariates. Intuitively one\n",
    "can think of $\\alpha$ as controlling the amplitude of the wiggles in the\n",
    "function, and $\\rho$ controls the frequency of such wiggles. These are known as\n",
    "the **hyperparameters**, and basically allow this non-parametric inference to be\n",
    "fine-tuned.\n",
    "\n",
    "Let's define the covariance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@doc raw\"\"\"\n",
    "    cov_exp_quad(x, α, ρ; offset=1e-10)\n",
    "\n",
    "Compute the Exponentiated Quadratic (RBF) covariance matrix.\n",
    "\n",
    "# Arguments\n",
    "- `x::AbstractVector`: Points where to evaluate the kernel.\n",
    "- `α::Real`: Marginal standard deviation (output scale).\n",
    "- `ρ::Real`: Length-scale parameter.\n",
    "\n",
    "# Optional Keyword Arguments\n",
    "- `offset::Real=1e-10`: Small value added to the diagonal for numerical\n",
    "  stability.\n",
    "\n",
    "# Returns\n",
    "- `Matrix`: The computed covariance matrix.\n",
    "\n",
    "# Description\n",
    "This function computes the Exponentiated Quadratic (also known as Radial Basis\n",
    "Function or Gaussian) covariance matrix for the given input points. The\n",
    "covariance between two points x_i and x_j is given by:\n",
    "\n",
    "k(x_i, x_j) = α² * exp(-(x_i - x_j)² / (2ρ²)) + δ_ij * offset\n",
    "\n",
    "where δ_ij is the Kronecker delta (1 if i=j, 0 otherwise).\n",
    "\"\"\"\n",
    "function cov_exp_quad(\n",
    "    x::AbstractVector,\n",
    "    α::Real,\n",
    "    ρ::Real;\n",
    "    offset::Real=1e-10\n",
    ")\n",
    "    # Compute pairwise squared distances\n",
    "    dist_matrix = Distances.pairwise(Distances.SqEuclidean(), x)\n",
    "\n",
    "    # Compute covariance matrix\n",
    "    cov_matrix = @. α^2 * exp(-dist_matrix / (2ρ^2))\n",
    "\n",
    "    # Add offset to diagonal for numerical stability\n",
    "    cov_matrix[LinearAlgebra.diagind(cov_matrix)] .+= offset\n",
    "\n",
    "    return cov_matrix\n",
    "end\n",
    "\n",
    "@doc raw\"\"\"\n",
    "    cov_exp_quad(x₁, x₂, α, ρ)\n",
    "\n",
    "Compute the Exponentiated Quadratic (RBF) covariance matrix between two sets of\n",
    "points.\n",
    "\n",
    "# Arguments\n",
    "- `x₁::AbstractVector`: First set of points.\n",
    "- `x₂::AbstractVector`: Second set of points.\n",
    "- `α::Real`: Marginal standard deviation (output scale).\n",
    "- `ρ::Real`: Length-scale parameter.\n",
    "\n",
    "# Optional Keyword Arguments\n",
    "- `offset::Real=1e-10`: Small value added to the diagonal for numerical\n",
    "  stability.\n",
    "\n",
    "# Returns\n",
    "- `Matrix`: The computed cross-covariance matrix.\n",
    "\n",
    "# Description\n",
    "This function computes the Exponentiated Quadratic (also known as Radial Basis\n",
    "Function or Gaussian) covariance matrix between two sets of points. The\n",
    "covariance between points x₁_i and x₂_j is given by:\n",
    "\n",
    "k(x₁_i, x₂_j) = α² * exp(-(x₁_i - x₂_j)² / (2ρ²))\n",
    "\"\"\"\n",
    "function cov_exp_quad(\n",
    "    x₁::AbstractVector,\n",
    "    x₂::AbstractVector,\n",
    "    α::Real,\n",
    "    ρ::Real,\n",
    "    offset::Real=1e-10\n",
    ")\n",
    "    # Compute pairwise squared distances\n",
    "    dist_matrix = Distances.pairwise(Distances.SqEuclidean(), x₁, x₂)\n",
    "\n",
    "    # Compute covariance matrix\n",
    "    cov_matrix = @. α^2 * exp(-dist_matrix / (2ρ^2))\n",
    "\n",
    "    # Add offset to diagonal for numerical stability\n",
    "    cov_matrix[LinearAlgebra.diagind(cov_matrix)] .+= offset\n",
    "\n",
    "    return cov_matrix\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we chose to use the exponential quadratic kernel, our prior distribution\n",
    "over $f(x_*)$ now takes the form $\\pi(f \\mid \\alpha, \\rho)$. Given that $f$ is a\n",
    "Gaussian process, the prior should be a Gaussian distribution of the form\n",
    "$$\n",
    "f(x_*) \\sim \\mathcal{N}(0, k(x_*, x_*')).\n",
    "\\tag{6}\n",
    "$$\n",
    "Let's sample from this prior distribution to get a better intuition for how both\n",
    "hyper parameters affect the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(42)\n",
    "\n",
    "# Define number of time points\n",
    "n_points = 300\n",
    "# Define time points to evaluate\n",
    "x_array = collect(LinRange(-3, 3, n_points))\n",
    "\n",
    "# Define single marginal standard deviation\n",
    "α = 1.0\n",
    "\n",
    "# Define different length scales\n",
    "ρ = [1.0, 0.5, 0.25]\n",
    "\n",
    "# Inititalize array to save samples\n",
    "y = Array{Float64}(undef, length(ρ), n_points)\n",
    "\n",
    "# Loop through length scales\n",
    "for (i, r) in enumerate(ρ)\n",
    "    # Build covariance matrix\n",
    "    K = cov_exp_quad(x_array, α, r)\n",
    "\n",
    "    # Generate samples\n",
    "    y[i, :] = rand(Distributions.MvNormal(zeros(n_points), K))\n",
    "end # for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at this synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inititalize figure\n",
    "fig = Figure(size=(400, 300))\n",
    "\n",
    "# Add axis\n",
    "ax = Axis(\n",
    "    fig[1, 1],\n",
    "    xlabel=\"x\",\n",
    "    ylabel=\"f(x)\"\n",
    ")\n",
    "\n",
    "# Loop through each random sample\n",
    "for (i, y_) in enumerate(eachrow(y))\n",
    "    scatter!(\n",
    "        ax,\n",
    "        x_array,\n",
    "        y_,\n",
    "        color=ColorSchemes.seaborn_colorblind[i],\n",
    "        label=\"ρ = $(ρ[i])\",\n",
    "        markersize=6,\n",
    "    )\n",
    "end # for\n",
    "\n",
    "# Add legend to plot\n",
    "Legend(fig[1, 2], ax)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can clearly see the effect of the length scale parameter. Let's now draw many many samples using the same set of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(42)\n",
    "\n",
    "# Define number of points\n",
    "n_points = 200\n",
    "# Define time points to evaluate\n",
    "x_array = collect(LinRange(-3, 3, n_points))\n",
    "\n",
    "# Set number of samples\n",
    "n_samples = 500\n",
    "\n",
    "# Define hypeerparameters\n",
    "α = 1.0\n",
    "ρ = 1.0\n",
    "\n",
    "# Compute covariance matrix\n",
    "K = cov_exp_quad(x_array, α, ρ)\n",
    "\n",
    "# Generate samples\n",
    "samples = rand(Distributions.MvNormal(zeros(n_points), K), n_samples)\n",
    "\n",
    "# Inititalize figure\n",
    "fig = Figure(size=(450, 300))\n",
    "\n",
    "# Add axis\n",
    "ax = Axis(\n",
    "    fig[1, 1],\n",
    "    xlabel=\"x\",\n",
    "    ylabel=\"f(x)\"\n",
    ")\n",
    "\n",
    "# Loop through each random sample\n",
    "for (i, y_) in enumerate(eachcol(samples))\n",
    "    lines!(\n",
    "        ax,\n",
    "        x_array,\n",
    "        y_,\n",
    "        color=(ColorSchemes.seaborn_colorblind[1], 0.15),\n",
    "    )\n",
    "end # for\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only discussed the prior $\\pi(f(x_*) \\mid x_*, \\alpha, \\rho)$.\n",
    "But our choice of covariance kernel demands us to define priors for the\n",
    "hyperparameters themselves. Once we implement the actual inference in\n",
    "`Turing.jl` we'll discuss more the form that the priors $\\pi(\\alpha)$ and\n",
    "$\\pi(\\rho)$ will take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumption for our Gaussian process is that we expect our observable optical\n",
    "density $y$ to follow the resulting function $f(x_*)$, perhaps with some error,\n",
    "i.e.\n",
    "$$\n",
    "y_i = f(x_i) + \\epsilon_i,\n",
    "\\tag{7}\n",
    "$$\n",
    "where $\\epsilon_i$ is the deviation of the $i^{th}$ point from the Gaussian\n",
    "process. We can then model the observable as a Normally distributed random\n",
    "variable with uniform error $\\sigma$,\n",
    "$$\n",
    "y_i \\sim \\mathcal{N}(f(x_i), \\sigma).\n",
    "\\tag{8}\n",
    "$$\n",
    "This particular choice of likelihood has the very convenient property of being\n",
    "the **conjugate likelihood** to a Gaussian process prior. What this means is\n",
    "that the posterior distribution for $f(x_*)$ takes the same functional form as\n",
    "the prior, i.e.,\n",
    "$$\n",
    "f(x_*) \\mid y \\sim \n",
    "\\mathcal{GP}(m(x_*), k(x_*, x_*') + \\delta_{x_*, x_*'}\\, \\sigma^2),\n",
    "\\tag{9}\n",
    "$$\n",
    "where $\\delta_{x_*, x_*'}$ is the Kronecker delta. Let's quickly take a look at\n",
    "what this implies. For $x_* \\neq x_*'$ our mean function $m(x_*)$ will be zero\n",
    "given our previous assumption. The covariance function for this case takes the\n",
    "form of the usual exponential quadratic kernel given that $\\delta_{x_*, x_*'} =\n",
    "0$ for $x_* \\neq t'$. For the specific case where $t = t'$ the quadratic kernel\n",
    "for this case would be zero. But given our assumption of our measurements $y$\n",
    "being normally distributed, the error in the measurement $\\sigma$ now enters the\n",
    "problem.\n",
    "\n",
    "When we have multiple observations, for example the time series we obtain for\n",
    "growth curves $\\mathbf{x} = [x_0, x_1, \\ldots, x_n]$, we define\n",
    "$$\n",
    "\\mathbf{K}_y \\equiv K(\\mathbf{x}, \\mathbf{x}) + \\sigma^2 \\mathbf{I},\n",
    "\\tag{10}\n",
    "$$\n",
    "where $\\mathbf{I}$ is the identity matrix. Then we have a likelihood function of\n",
    "the form\n",
    "$$\n",
    "f(\\mathbf{x}) \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{K}_y).\n",
    "\\tag{11}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to declare the `Turing` model using the `@model` macro. We\n",
    "will define the following priors for the $\\alpha$, $\\rho$, and $\\sigma$\n",
    "parameters:\n",
    "$$\n",
    "\\alpha \\sim \\mathcal{N}(\\mu_\\alpha, \\sigma_\\alpha),\n",
    "\\tag{12}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\rho \\sim \\text{Lognormal}(\\mu_\\rho, \\sigma_\\rho),\n",
    "\\tag{13}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\sigma \\sim \\text{Lognormal}(\\mu_\\sigma, \\sigma_\\sigma),\n",
    "\\tag{14}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare our turing model\n",
    "Turing.@model function logistic_gp(\n",
    "    x, y, α_prior, ρ_prior, σ_prior\n",
    ")\n",
    "    # Priors\n",
    "    α ~ Turing.LogNormal(α_prior...)\n",
    "    ρ ~ Turing.LogNormal(ρ_prior...)\n",
    "    σ ~ Turing.LogNormal(σ_prior...)\n",
    "\n",
    "    # Compute Exponentiated quadratic covariance function\n",
    "    cov_exp = cov_exp_quad(x, α, ρ; offset=1E-10) +\n",
    "              (LinearAlgebra.I(length(x)) * σ^2)\n",
    "\n",
    "    # Check if covariance matrix is positive definite. If not, set probability\n",
    "    # to -∞. NOTE: I tried everything to make sure the matrix was positive \n",
    "    # definite, but I couldn't get it to work. This is a way to get around the\n",
    "    # problem\n",
    "    if !LinearAlgebra.isposdef(cov_exp)\n",
    "        Turing.@addlogprob! -Inf\n",
    "        return\n",
    "    end\n",
    "\n",
    "    # Likelihood\n",
    "    return y ~ Turing.MvNormal(zeros(length(x)), cov_exp)\n",
    "end # @model function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run the sampler. Specifically we will use the `NUTS`\n",
    "algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(42)\n",
    "# Group data by antibiotic, environment, and day\n",
    "df_group = DF.groupby(\n",
    "    df[(.!df.blank).&(df.concentration_ugmL.>0), :],\n",
    "    [:antibiotic, :env, :day, :strain_num]\n",
    ")\n",
    "\n",
    "# Extract data\n",
    "data = df_group[42]\n",
    "\n",
    "# Extract data\n",
    "x = log.(data.concentration_ugmL)\n",
    "y = log.(data.OD)\n",
    "\n",
    "# Define model\n",
    "model = logistic_gp(x, y, [0.0, 1.0], [-0.4, 0.4], [0.0, 1.0])\n",
    "\n",
    "# Define number of steps\n",
    "n_burnin = 2_000\n",
    "n_samples = 1_000\n",
    "\n",
    "chain = Turing.sample(\n",
    "    model, Turing.NUTS(), Turing.MCMCThreads(), n_burnin + n_samples, 4\n",
    ")\n",
    "\n",
    "# Remove burnin\n",
    "chain = chain[n_burnin+1:end]\n",
    "\n",
    "Turing.summarystats(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the trace and density plots for the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameters\n",
    "params = names(chain, :parameters)\n",
    "\n",
    "# Extract number of chains\n",
    "n_chains = size(chain, 3)\n",
    "# Extract number of samples\n",
    "n_samples = size(chain, 1)\n",
    "\n",
    "# Initialize figure\n",
    "fig = Figure(size=(600, 400))\n",
    "\n",
    "# Define colors\n",
    "colors = ColorSchemes.seaborn_colorblind\n",
    "\n",
    "# Loop through parameters\n",
    "for (i, param) in enumerate(params)\n",
    "    # Add axis for chain iteration\n",
    "    ax_trace = Axis(fig[i, 1]; ylabel=string(param))\n",
    "    # Inititalize axis for density plot\n",
    "    ax_density = Axis(fig[i, 2]; ylabel=string(param))\n",
    "    # Loop through chains\n",
    "    for chn in 1:n_chains\n",
    "        # Extract values\n",
    "        values = chain[:, param, chn]\n",
    "        # Plot traces of walker\n",
    "        lines!(\n",
    "            ax_trace,\n",
    "            1:n_samples,\n",
    "            values;\n",
    "            label=string(chain),\n",
    "            color=(colors[chn], 0.3)\n",
    "        )\n",
    "        # Plot density\n",
    "        density!(\n",
    "            ax_density, values; label=string(chain), color=(colors[chn], 0.3)\n",
    "        )\n",
    "    end # for\n",
    "\n",
    "    # Hide y-axis decorations\n",
    "    hideydecorations!(ax_trace; label=false)\n",
    "    hideydecorations!(ax_density; label=false)\n",
    "\n",
    "    # Check if it is bottom plot\n",
    "    if i < length(params)\n",
    "        # hide x-axis decoratiosn\n",
    "        hidexdecorations!(ax_trace; grid=false)\n",
    "    else\n",
    "        # add x-label\n",
    "        ax_trace.xlabel = \"iteration\"\n",
    "        ax_density.xlabel = \"parameter estimate\"\n",
    "    end # if\n",
    "end # for\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have everything in place to compute the posterior predictive checks for\n",
    "the growth curve. This is the \"unique\" part of the implementation, where, on top\n",
    "of making predictions for unobserved time points (interpolating between the data\n",
    "with as much resolution as desired), we will also infer the derivative of the\n",
    "measured time series.\n",
    "\n",
    "The math behind this is a little tricky and will be explained elsewhere.\n",
    "\n",
    "First, we define a function to generate posterior predictive checks (PPC) for a\n",
    "Gaussian process, including both function values and derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@doc raw\"\"\"\n",
    "    gp_ppc_rng(x_ppc, y, x_data, α, ρ, σ; offset=1E-10)\n",
    "\n",
    "Generate posterior predictive checks (PPC) for a Gaussian process, including\n",
    "both function values and derivatives.\n",
    "\n",
    "# Arguments\n",
    "- `x_ppc::AbstractVector`: Points where to evaluate PPC.\n",
    "- `y::AbstractVector`: Observed measurements.\n",
    "- `x_data::AbstractVector`: Observation points.\n",
    "- `α::Real`: Marginal standard deviation.\n",
    "- `ρ::Real`: Length scale.\n",
    "- `σ::Real`: Measurement error standard deviation.\n",
    "- `offset::Real=1E-10`: Small value added for numerical stability.\n",
    "\n",
    "# Returns\n",
    "- `Vector`: Concatenated vector of PPC samples for function values and\n",
    "  derivatives.\n",
    "\"\"\"\n",
    "function gp_ppc_rng(\n",
    "    x_ppc::AbstractVector,\n",
    "    y::AbstractVector,\n",
    "    x_data::AbstractVector,\n",
    "    α::Real,\n",
    "    ρ::Real,\n",
    "    σ::Real;\n",
    "    offset::Real=1E-10\n",
    ")\n",
    "    N_data, N_ppc = length(y), length(x_ppc)\n",
    "\n",
    "    # Build necessary covariance matrices\n",
    "    ## 1. Build bottom left covariance matrix K₂₂\n",
    "    ### 1.1 Build Kx*x*\n",
    "    K_xs_xs = cov_exp_quad(x_ppc, α, ρ; offset)\n",
    "\n",
    "    ### 1.2 Initialize and compute d1x_Kx*x*, d2x_Kx*x*, and dxx_Kx*x*\n",
    "    d1x_K_xs_xs = similar(K_xs_xs)\n",
    "    d2x_K_xs_xs = similar(K_xs_xs)\n",
    "    d2xx_K_xs_xs = similar(K_xs_xs)\n",
    "\n",
    "    ### 1.4 Compute derivatives of the matrices by multiplying by corresponding\n",
    "    ### prefactors\n",
    "    @inbounds for i in 1:N_ppc, j in 1:N_ppc\n",
    "        diff = x_ppc[i] - x_ppc[j]\n",
    "        diff_sq = diff^2\n",
    "        factor = K_xs_xs[i, j] / ρ^2\n",
    "        d1x_K_xs_xs[i, j] = -diff * factor\n",
    "        d2x_K_xs_xs[i, j] = diff * factor\n",
    "        d2xx_K_xs_xs[i, j] = (1 - diff_sq / ρ^2) * factor\n",
    "    end\n",
    "\n",
    "    ### 1.5 Concatenate matrices\n",
    "    K_22 = [K_xs_xs d1x_K_xs_xs'; d1x_K_xs_xs d2xx_K_xs_xs]\n",
    "\n",
    "    ## 2. Compute top right and bottom left matrices K₁₂, K₂₁\n",
    "    ### 2.1 Build Kxx*\n",
    "    K_x_xs = cov_exp_quad(x_data, x_ppc, α, ρ)\n",
    "    K_xs_x = cov_exp_quad(x_ppc, x_data, α, ρ)\n",
    "\n",
    "    ### 2.2 Initialize and compute d1x_Kx*x and d2x_Kxx*\n",
    "    d2x_K_x_xs = Matrix{Float64}(undef, N_data, N_ppc)\n",
    "    d1x_K_xs_x = Matrix{Float64}(undef, N_ppc, N_data)\n",
    "\n",
    "    ### 2.3 Compute derivative of matrices by multiplying by corresponding\n",
    "    ### prefactors\n",
    "    @inbounds for i in 1:N_data, j in 1:N_ppc\n",
    "        d2x_K_x_xs[i, j] = (x_data[i] - x_ppc[j]) * K_x_xs[i, j] / ρ^2\n",
    "        d1x_K_xs_x[j, i] = -(x_ppc[j] - x_data[i]) * K_xs_x[j, i] / ρ^2\n",
    "    end\n",
    "\n",
    "    ### 2.5 Concatenate matrices\n",
    "    K_12 = [K_x_xs d2x_K_x_xs]\n",
    "    K_21 = [K_xs_x; d1x_K_xs_x]\n",
    "\n",
    "    ## 3. Solve equation Kxx * a = y\n",
    "    ### 3.1 Generate covariance matrix for the data Kxx\n",
    "    K_x_x = cov_exp_quad(x_data, α, ρ) .+ LinearAlgebra.I(N_data) * σ^2\n",
    "\n",
    "    ### 3.2 Perform Cholesky decomposition Kxx = Lxx * Lxx'\n",
    "    L_x_x = LinearAlgebra.cholesky(K_x_x).L\n",
    "\n",
    "    ### 3.3 Solve for b = inv(Lxx) y taking advantage that Lxx is a triangular\n",
    "    ### matrix\n",
    "    b = L_x_x \\ y\n",
    "\n",
    "    ### 3.4 Solve a = inv(Lxx') b taking advantage that Lxx is a triangular\n",
    "    ### matrix. Recall that a = inv(Kxx) y\n",
    "    a = L_x_x' \\ b\n",
    "\n",
    "    ## 4. Compute conditional mean ⟨[f(x*), dx*f(x*)] | f(x)⟩\n",
    "    mean_conditional = K_21 * a\n",
    "\n",
    "    ## 5. Evaluate v = inv(Lxx) * Kxx*\n",
    "    v = L_x_x \\ K_12\n",
    "\n",
    "    ## 6. Evaluate v' = inv(Lxx) * Kx*x\n",
    "    v_prime = (L_x_x \\ K_21')'\n",
    "\n",
    "    ## 7. Compute conditional covariance\n",
    "    cov_conditional = K_22 - v_prime * v .+ LinearAlgebra.I(2 * N_ppc) * offset\n",
    "\n",
    "    # Generate random samples given the conditional mean and covariance\n",
    "    return rand(Distributions.MvNormal(\n",
    "        mean_conditional, LinearAlgebra.Symmetric(cov_conditional)\n",
    "    ))\n",
    "end # function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@doc raw\"\"\"\n",
    "    gp_ppc_samples(chain, x_ppc, y, x_data; offset=1E-10, multithread=true)\n",
    "\n",
    "Generate posterior predictive checks (PPC) for the experimental data and its\n",
    "unobserved derivative from a Gaussian process, given MCMC chain samples.\n",
    "\n",
    "# Arguments\n",
    "- `chain::MCMCChains.Chains`: Samples from the MCMC run generated with\n",
    "  Turing.jl. The chain is assumed to have three parameters in the following\n",
    "  order:\n",
    "    - `α::AbstractFloat`: Marginal standard deviation.\n",
    "    - `ρ::AbstractFloat`: scale.\n",
    "    - `σ::AbstractFloat`: Measurement error. \n",
    "- `x_ppc::AbstractVector`: Points where to evaluate PPC.\n",
    "- `y::AbstractVector`: Observed measurements.\n",
    "- `x_data::AbstractVector`: Observation points.\n",
    "- `offset::AbstractFloat=1E-10`: Extra value added for numerical stability.\n",
    "- `multithread::Bool=true`: Whether to use multithreading for computations.\n",
    "\n",
    "# Returns\n",
    "- `y_predict::Array{Float64, 3}`: Predictions for the observation process.\n",
    "- `dy_predict::Array{Float64, 3}`: Predictions for the derivative of the\n",
    "  observation process.\n",
    "\"\"\"\n",
    "function gp_ppc_samples(\n",
    "    chain::MCMCChains.Chains,\n",
    "    x_ppc::AbstractVector,\n",
    "    y::AbstractVector,\n",
    "    x_data::AbstractVector;\n",
    "    offset::AbstractFloat=1E-10,\n",
    "    multithread::Bool=true\n",
    ")\n",
    "    # Get the number of prediction points\n",
    "    N_predict = length(x_ppc)\n",
    "    # Get the parameter names from the chain\n",
    "    params = names(chain, :parameters)\n",
    "    # Get the number of chains\n",
    "    n_chains = size(chain, 3)\n",
    "    # Get the number of samples\n",
    "    n_samples = size(chain, 1)\n",
    "    # Extract the parameter samples from the chain\n",
    "    chain_samples = MCMCChains.get_params(chain)\n",
    "\n",
    "    # Initialize arrays to store predictions\n",
    "    f_predict = Array{Float64}(undef, n_samples, n_chains, 2 * N_predict)\n",
    "    y_predict = Array{Float64}(undef, n_samples, n_chains, N_predict)\n",
    "    dy_predict = Array{Float64}(undef, n_samples, n_chains, N_predict)\n",
    "\n",
    "    # Define a function to process each sample\n",
    "    process_sample = (i, j) -> begin\n",
    "        # Extract parameter values for this sample\n",
    "        α_, ρ_, σ_ = (chain_samples[params[k]].data[i, j] for k in 1:3)\n",
    "        # Generate predictions using the GP model\n",
    "        f_predict[i, j, :] = gp_ppc_rng(\n",
    "            x_ppc, y, x_data, α_, ρ_, σ_; offset=offset\n",
    "        )\n",
    "        for n in 1:N_predict\n",
    "            # Generate noisy observations from the GP predictions\n",
    "            y_predict[i, j, n] = rand(\n",
    "                Distributions.Normal(f_predict[i, j, n], σ_)\n",
    "            )\n",
    "            # Generate derivative predictions\n",
    "            dy_predict[i, j, n] = rand(\n",
    "                Distributions.Normal(f_predict[i, j, N_predict+n], offset)\n",
    "            )\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Process samples using multithreading if enabled\n",
    "    if multithread\n",
    "        Threads.@threads for i in 1:n_samples\n",
    "            for j in 1:n_chains\n",
    "                process_sample(i, j)\n",
    "            end\n",
    "        end\n",
    "    else\n",
    "        # Process samples sequentially if multithreading is disabled\n",
    "        for i in 1:n_samples, j in 1:n_chains\n",
    "            process_sample(i, j)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Return the predictions for observations and derivatives\n",
    "    return y_predict, dy_predict\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these functions in place, we can now generate the posterior predictive\n",
    "checks for the growth curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of points to predict\n",
    "n_points = 100\n",
    "# Define time points to evaluate\n",
    "x_array = collect(LinRange(minimum(x), maximum(x), n_points))\n",
    "\n",
    "# Generate posterior predictive checks\n",
    "y_predict, dy_predict = gp_ppc_samples(chain, x_array, y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the generated samples in a sensible way, let's compute a few quantiles\n",
    "that we'll display with different shades of color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantiles to compute\n",
    "qs = [0.95, 0.68, 0.5]\n",
    "\n",
    "# Inititalize array to save quantiles\n",
    "y_quant = [Array{Float64}(undef, size(y_predict, 3), 2) for x = 1:length(qs)]\n",
    "dy_quant = [Array{Float64}(undef, size(y_predict, 3), 2) for x = 1:length(qs)]\n",
    "\n",
    "# Loop through quantiles\n",
    "for (i, q) in enumerate(qs)\n",
    "    # Loop through time points\n",
    "    for j = 1:size(y_predict, 3)\n",
    "        # Flatten data\n",
    "        y_data = vec(y_predict[:, :, j])\n",
    "        dy_data = vec(dy_predict[:, :, j])\n",
    "\n",
    "        # Compute uper and lower quantile\n",
    "        y_quant[i][j, :] = StatsBase.quantile(\n",
    "            y_data, [(1.0 - q) / 2.0, 1.0 - (1.0 - q) / 2.0]\n",
    "        )\n",
    "\n",
    "        dy_quant[i][j, :] = StatsBase.quantile(\n",
    "            dy_data, [(1.0 - q) / 2.0, 1.0 - (1.0 - q) / 2.0]\n",
    "        )\n",
    "    end # for\n",
    "end # for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to plot the inferred time series along with the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize figure\n",
    "fig = Figure(size=(350, 500))\n",
    "\n",
    "# Add ax\n",
    "ax1 = Axis(\n",
    "    fig[1, 1],\n",
    "    xlabel=\"log(concentration)\",\n",
    "    ylabel=\"log(OD)\"\n",
    ")\n",
    "\n",
    "ax2 = Axis(\n",
    "    fig[2, 1],\n",
    "    xlabel=\"log(concentration)\",\n",
    "    ylabel=\"d(log(OD))/dt\"\n",
    ")\n",
    "\n",
    "# Define colors\n",
    "colors = get(ColorSchemes.Blues_9, LinRange(0, 0.75, 3))\n",
    "\n",
    "# Loop through quantiles\n",
    "for (i, q) in enumerate(qs)\n",
    "    # Add confidence interval for observation\n",
    "    band!(\n",
    "        ax1,\n",
    "        x_array,\n",
    "        y_quant[i][:, 1],\n",
    "        y_quant[i][:, 2],\n",
    "        color=(colors[i], 0.75)\n",
    "    )\n",
    "\n",
    "    # Add confidence interval for derivative\n",
    "    band!(\n",
    "        ax2,\n",
    "        x_array,\n",
    "        dy_quant[i][:, 1],\n",
    "        dy_quant[i][:, 2],\n",
    "        color=(colors[i], 0.75)\n",
    "    )\n",
    "end # for\n",
    "\n",
    "# Add growth curve scatter plot\n",
    "scatter!(ax1, x, y, markersize=5, color=:black)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function logistic(logx, params)\n",
    "    return logistic(logx, params...)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    fit_logistic_and_detect_outliers(log2x, y; threshold=3)\n",
    "\n",
    "Fit a logistic model to the given data and detect outliers based on residuals.\n",
    "\n",
    "This function performs the following steps:\n",
    "1. Fits a logistic model to the log2-transformed x-values and y-values.\n",
    "2. Calculates residuals between the fitted model and actual y-values.\n",
    "3. Identifies outliers as points with residuals exceeding a specified threshold.\n",
    "\n",
    "# Arguments\n",
    "- `log2x`: Array of log2-transformed x-values (typically concentrations).\n",
    "- `y`: Array of y-values (typically optical density measurements).\n",
    "- `threshold`: Number of standard deviations beyond which a point is considered\n",
    "  an outlier. Default is 3.\n",
    "\n",
    "# Returns\n",
    "- A boolean array indicating which points are outliers (true for outliers).\n",
    "\n",
    "# Notes\n",
    "- The function uses a logistic model of the form: y = a / (1 + exp(b * (log2x -\n",
    "    log2ic50))) + c\n",
    "- Initial parameter guesses are made based on the input data.\n",
    "- The LsqFit package is used for curve fitting.\n",
    "- Outliers are determined by comparing the absolute residuals to the threshold *\n",
    "  standard deviation of residuals.\n",
    "\"\"\"\n",
    "function fit_logistic_and_detect_outliers(logx, logy; threshold=3)\n",
    "    # Initial parameter guess\n",
    "    p0 = [0.1, 1.0, maximum(logy) - minimum(logy), StatsBase.median(logx)]\n",
    "\n",
    "    # Fit the logistic model\n",
    "    fit = LsqFit.curve_fit(logistic, logx, logy, p0)\n",
    "\n",
    "    # Calculate residuals\n",
    "    residuals = logy - logistic(logx, fit.param)\n",
    "\n",
    "    # Calculate standard deviation of residuals\n",
    "    σ = std(residuals)\n",
    "\n",
    "    # Identify outliers\n",
    "    outliers_idx = abs.(residuals) .> threshold * σ\n",
    "\n",
    "    # Return outlier indices\n",
    "    return outliers_idx\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(42)\n",
    "# Group data by antibiotic, environment, and day\n",
    "df_group = DF.groupby(\n",
    "    df[(.!df.blank).&(df.concentration_ugmL.>0), :],\n",
    "    [:antibiotic, :env, :day, :strain_num]\n",
    ")\n",
    "\n",
    "# Extract data\n",
    "data = df_group[42]\n",
    "\n",
    "# Find outliers\n",
    "outliers_idx = fit_logistic_and_detect_outliers(\n",
    "    log.(data.concentration_ugmL), log.(data.OD), threshold=2\n",
    ")\n",
    "# Remove outliers\n",
    "data_clean = data[.!outliers_idx, :]\n",
    "\n",
    "# Extract data\n",
    "x = log.(data_clean.concentration_ugmL)\n",
    "y = log.(data_clean.OD)\n",
    "\n",
    "# Define model\n",
    "model = logistic_gp(x, y, [0.0, 1.0], [-0.4, 0.4], [0.0, 1.0])\n",
    "\n",
    "# Define number of steps\n",
    "n_burnin = 2_500\n",
    "n_samples = 500\n",
    "\n",
    "chain = Turing.sample(\n",
    "    model, Turing.NUTS(), Turing.MCMCThreads(), n_burnin + n_samples, 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of points to predict\n",
    "n_points = 100\n",
    "# Define time points to evaluate\n",
    "x_array = collect(LinRange(minimum(x), maximum(x), n_points))\n",
    "\n",
    "# Generate posterior predictive checks\n",
    "y_predict, dy_predict = gp_ppc_samples(chain, x_array, y, x)\n",
    "\n",
    "# Define quantiles to compute\n",
    "qs = [0.95, 0.68, 0.5]\n",
    "\n",
    "# Inititalize array to save quantiles\n",
    "y_quant = [Array{Float64}(undef, size(y_predict, 3), 2) for x = 1:length(qs)]\n",
    "dy_quant = [Array{Float64}(undef, size(y_predict, 3), 2) for x = 1:length(qs)]\n",
    "\n",
    "# Loop through quantiles\n",
    "for (i, q) in enumerate(qs)\n",
    "    # Loop through time points\n",
    "    for j = 1:size(y_predict, 3)\n",
    "        # Flatten data\n",
    "        y_data = vec(y_predict[:, :, j])\n",
    "        dy_data = vec(dy_predict[:, :, j])\n",
    "\n",
    "        # Compute uper and lower quantile\n",
    "        y_quant[i][j, :] = StatsBase.quantile(\n",
    "            y_data, [(1.0 - q) / 2.0, 1.0 - (1.0 - q) / 2.0]\n",
    "        )\n",
    "\n",
    "        dy_quant[i][j, :] = StatsBase.quantile(\n",
    "            dy_data, [(1.0 - q) / 2.0, 1.0 - (1.0 - q) / 2.0]\n",
    "        )\n",
    "    end # for\n",
    "end # for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
