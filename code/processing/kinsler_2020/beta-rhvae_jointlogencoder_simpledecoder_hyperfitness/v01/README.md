# `RHVAE v05`

## `RHVAE` specific features

- latent space dimensionality: 2
- number of neurons in hidden layers: 128
- activation function: leaky ReLU
- temperature parameter: 0.5
- regularization parameter: 1E-2
- number of centroids: 256
- centroid determination: k-medoids

## `RHVAE` training

- training data: standardized posterior samples from the `BarBay.jl`
  **hierarchical replicate model** inference on the Kinsler et al. (2020)
  dataset.
- training split: 85% training, 15% validation
- loss function hyperparameters:
    - leapfrog step size: 1E-3
    - leapfrog steps: 10
    - initial tempering temperature: 0.3
    - ELBO prefactors:
        - logp_prefactor = [10.0f0, 0.1f0, 0.1f0]
        - logq_prefactor = [0.1f0, 0.1f0, 0.1f0]
- optimizer: ADAM
- learning rate: 1E-3
- samples in batch: 256
- samples to compute loss: 256
- random seed: 42

## Notes

This training was performed using fitness posterior samples obtained using
`BarBay.jl` **hierarchical replicate model** on the raw data from Kinsler et al.
(2020). This means that only environments with at least two replicates were used
for training.

At each iteration, the data was generated by sampling from a normal distribution
with mean and standard deviation given by the posterior samples. Specifically,
for each data point, we sampled from N(μ, σ), where μ and σ are the mean and
standard deviation of the posterior distribution for that point.
