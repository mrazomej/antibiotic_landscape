<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Manuel Razo-Mejia">
<meta name="author" content="Madhav Mani">
<meta name="author" content="Dmitri Petrov">
<meta name="keywords" content="variational autoencoders, evolutionary dynamics, antibiotic resistance">

<title>Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-3f13cc09417cf69baaf74afa0e2e3cf6.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./paper.html"> 
<span class="menu-text">Main Text</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./supplementary.html" aria-current="page"> 
<span class="menu-text">Supplementary Material</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-code" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Code</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-code">    
        <li>
    <a class="dropdown-item" href="./code.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./code/metropolis_hastings_evolution.html">
 <span class="dropdown-text">Metropolis-Hastings Evolution</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./code/evolutionary_dynamics.html">
 <span class="dropdown-text">Evolutionary Dynamics Simulation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./code/rhvae_sim.html">
 <span class="dropdown-text">RHVAE training on Simulated Data</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./code/iwasawa_bayesian_inference.html">
 <span class="dropdown-text">Bayesian Inference of <span class="math inline">\(IC_{50}\)</span> Values</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./code/metropolis_kimura_evolution.html">
 <span class="dropdown-text">Metropolis-Kimura Evolution</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/mrazomej/antibiotic_landscape"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/mrazomej"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#supplementary-materials" id="toc-supplementary-materials" class="nav-link active" data-scroll-target="#supplementary-materials">Supplementary Materials</a>
  <ul class="collapse">
  <li><a href="#bayesian-inference-of-ic_50-resistance-values" id="toc-bayesian-inference-of-ic_50-resistance-values" class="nav-link" data-scroll-target="#bayesian-inference-of-ic_50-resistance-values">Bayesian inference of <span class="math inline">\(IC_{50}\)</span> resistance values</a>
  <ul class="collapse">
  <li><a href="#experimental-design" id="toc-experimental-design" class="nav-link" data-scroll-target="#experimental-design">Experimental design</a></li>
  <li><a href="#bayesian-inference-of-ic_50-values" id="toc-bayesian-inference-of-ic_50-values" class="nav-link" data-scroll-target="#bayesian-inference-of-ic_50-values">Bayesian inference of <span class="math inline">\(IC_{50}\)</span> values</a></li>
  <li><a href="#mcmc-sampling-of-the-posterior" id="toc-mcmc-sampling-of-the-posterior" class="nav-link" data-scroll-target="#mcmc-sampling-of-the-posterior">MCMC sampling of the posterior</a></li>
  </ul></li>
  <li><a href="#sec-metropolis" id="toc-sec-metropolis" class="nav-link" data-scroll-target="#sec-metropolis">Metropolis-Hastings Evolutionary Dynamics</a>
  <ul class="collapse">
  <li><a href="#landscape-description" id="toc-landscape-description" class="nav-link" data-scroll-target="#landscape-description">Landscape Description</a></li>
  <li><a href="#metropolis-hastings-algorithm" id="toc-metropolis-hastings-algorithm" class="nav-link" data-scroll-target="#metropolis-hastings-algorithm">Metropolis-Hastings Algorithm</a></li>
  <li><a href="#effect-of-inverse-temperature" id="toc-effect-of-inverse-temperature" class="nav-link" data-scroll-target="#effect-of-inverse-temperature">Effect of Inverse Temperature</a></li>
  </ul></li>
  <li><a href="#riemannian-hamiltonian-variational-autoencoders-mathematical-background" id="toc-riemannian-hamiltonian-variational-autoencoders-mathematical-background" class="nav-link" data-scroll-target="#riemannian-hamiltonian-variational-autoencoders-mathematical-background">Riemannian Hamiltonian Variational Autoencoders Mathematical Background</a>
  <ul class="collapse">
  <li><a href="#variational-autoencoder-model-setting" id="toc-variational-autoencoder-model-setting" class="nav-link" data-scroll-target="#variational-autoencoder-model-setting">Variational Autoencoder Model Setting</a></li>
  <li><a href="#mcmc-and-vi-salimans-kingma-2015" id="toc-mcmc-and-vi-salimans-kingma-2015" class="nav-link" data-scroll-target="#mcmc-and-vi-salimans-kingma-2015">MCMC and VI (Salimans &amp; Kingma 2015)</a></li>
  <li><a href="#sec-normalizing-flows" id="toc-sec-normalizing-flows" class="nav-link" data-scroll-target="#sec-normalizing-flows">Normalizing Flows (Rezende &amp; Mohamed 2015)</a></li>
  <li><a href="#hamiltonian-markov-chain-monte-carlo-hmc" id="toc-hamiltonian-markov-chain-monte-carlo-hmc" class="nav-link" data-scroll-target="#hamiltonian-markov-chain-monte-carlo-hmc">Hamiltonian Markov Chain Monte Carlo (HMC)</a></li>
  <li><a href="#hamiltonian-variational-autoencoder-hvae" id="toc-hamiltonian-variational-autoencoder-hvae" class="nav-link" data-scroll-target="#hamiltonian-variational-autoencoder-hvae">Hamiltonian Variational Autoencoder (HVAE)</a></li>
  <li><a href="#riemannian-hamiltonian-mcmc" id="toc-riemannian-hamiltonian-mcmc" class="nav-link" data-scroll-target="#riemannian-hamiltonian-mcmc">Riemannian Hamiltonian MCMC</a></li>
  <li><a href="#riemannian-hamiltonian-variational-autoencoder" id="toc-riemannian-hamiltonian-variational-autoencoder" class="nav-link" data-scroll-target="#riemannian-hamiltonian-variational-autoencoder">Riemannian Hamiltonian Variational Autoencoder</a></li>
  </ul></li>
  <li><a href="#sec-nonlinear" id="toc-sec-nonlinear" class="nav-link" data-scroll-target="#sec-nonlinear">Advantages of Nonlinear Manifold Learning for Fitness Landscapes</a>
  <ul class="collapse">
  <li><a href="#theoretical-framework-the-causal-manifold" id="toc-theoretical-framework-the-causal-manifold" class="nav-link" data-scroll-target="#theoretical-framework-the-causal-manifold">Theoretical Framework: The Causal Manifold</a></li>
  <li><a href="#limitations-of-linear-methods" id="toc-limitations-of-linear-methods" class="nav-link" data-scroll-target="#limitations-of-linear-methods">Limitations of Linear Methods</a></li>
  <li><a href="#mathematical-advantages-of-nonlinear-manifolds" id="toc-mathematical-advantages-of-nonlinear-manifolds" class="nav-link" data-scroll-target="#mathematical-advantages-of-nonlinear-manifolds">Mathematical Advantages of Nonlinear Manifolds</a></li>
  <li><a href="#nonlinear-methods-for-manifold-learning" id="toc-nonlinear-methods-for-manifold-learning" class="nav-link" data-scroll-target="#nonlinear-methods-for-manifold-learning">Nonlinear Methods for Manifold Learning</a></li>
  <li><a href="#comparative-advantages-of-nonlinear-methods" id="toc-comparative-advantages-of-nonlinear-methods" class="nav-link" data-scroll-target="#comparative-advantages-of-nonlinear-methods">Comparative Advantages of Nonlinear Methods</a></li>
  <li><a href="#empirical-evidence-and-caveats" id="toc-empirical-evidence-and-caveats" class="nav-link" data-scroll-target="#empirical-evidence-and-caveats">Empirical Evidence and Caveats</a></li>
  </ul></li>
  <li><a href="#cross-validation-methodology-for-evaluating-latent-space-predictive-power" id="toc-cross-validation-methodology-for-evaluating-latent-space-predictive-power" class="nav-link" data-scroll-target="#cross-validation-methodology-for-evaluating-latent-space-predictive-power">Cross-Validation Methodology for Evaluating Latent Space Predictive Power</a>
  <ul class="collapse">
  <li><a href="#theoretical-background" id="toc-theoretical-background" class="nav-link" data-scroll-target="#theoretical-background">Theoretical Background</a></li>
  <li><a href="#linear-bi-cross-validation-svd" id="toc-linear-bi-cross-validation-svd" class="nav-link" data-scroll-target="#linear-bi-cross-validation-svd">Linear Bi-Cross Validation (SVD)</a></li>
  <li><a href="#non-linear-cross-validation-vae-and-rhvae" id="toc-non-linear-cross-validation-vae-and-rhvae" class="nav-link" data-scroll-target="#non-linear-cross-validation-vae-and-rhvae">Non-Linear Cross-Validation (VAE and RHVAE)</a></li>
  <li><a href="#implementation-details" id="toc-implementation-details" class="nav-link" data-scroll-target="#implementation-details">Implementation Details</a></li>
  <li><a href="#comparative-analysis" id="toc-comparative-analysis" class="nav-link" data-scroll-target="#comparative-analysis">Comparative Analysis</a></li>
  <li><a href="#cross-validation-comparison-of-2d-and-3d-latent-spaces" id="toc-cross-validation-comparison-of-2d-and-3d-latent-spaces" class="nav-link" data-scroll-target="#cross-validation-comparison-of-2d-and-3d-latent-spaces">Cross-Validation comparison of 2D and 3D latent spaces</a></li>
  </ul></li>
  <li><a href="#geodesics-in-latent-space" id="toc-geodesics-in-latent-space" class="nav-link" data-scroll-target="#geodesics-in-latent-space">Geodesics in Latent Space</a></li>
  <li><a href="#geometry-informed-latent-space-reconstruction-of-antibiotic-resistance-landscapes" id="toc-geometry-informed-latent-space-reconstruction-of-antibiotic-resistance-landscapes" class="nav-link" data-scroll-target="#geometry-informed-latent-space-reconstruction-of-antibiotic-resistance-landscapes">Geometry-Informed Latent Space Reconstruction of Antibiotic Resistance Landscapes</a>
  <ul class="collapse">
  <li><a href="#trajectories-in-experimentally-reconstructed-fitness-landscapes" id="toc-trajectories-in-experimentally-reconstructed-fitness-landscapes" class="nav-link" data-scroll-target="#trajectories-in-experimentally-reconstructed-fitness-landscapes">Trajectories in Experimentally-reconstructed Fitness Landscapes</a></li>
  </ul></li>
  <li><a href="#neural-network-architecture" id="toc-neural-network-architecture" class="nav-link" data-scroll-target="#neural-network-architecture">Neural Network Architecture</a>
  <ul class="collapse">
  <li><a href="#rhvae-overview" id="toc-rhvae-overview" class="nav-link" data-scroll-target="#rhvae-overview">RHVAE Overview</a></li>
  <li><a href="#model-parameters" id="toc-model-parameters" class="nav-link" data-scroll-target="#model-parameters">Model Parameters</a></li>
  <li><a href="#encoder-architecture" id="toc-encoder-architecture" class="nav-link" data-scroll-target="#encoder-architecture">Encoder Architecture</a></li>
  <li><a href="#decoder-architecture" id="toc-decoder-architecture" class="nav-link" data-scroll-target="#decoder-architecture">Decoder Architecture</a></li>
  <li><a href="#metric-chain-architecture" id="toc-metric-chain-architecture" class="nav-link" data-scroll-target="#metric-chain-architecture">Metric Chain Architecture</a></li>
  <li><a href="#data-preprocessing" id="toc-data-preprocessing" class="nav-link" data-scroll-target="#data-preprocessing">Data Preprocessing</a></li>
  <li><a href="#implementation-details-1" id="toc-implementation-details-1" class="nav-link" data-scroll-target="#implementation-details-1">Implementation Details</a></li>
  </ul></li>
  <li><a href="#neural-geodesic-architecture" id="toc-neural-geodesic-architecture" class="nav-link" data-scroll-target="#neural-geodesic-architecture">Neural Geodesic Architecture</a>
  <ul class="collapse">
  <li><a href="#model-parameters-1" id="toc-model-parameters-1" class="nav-link" data-scroll-target="#model-parameters-1">Model Parameters</a></li>
  <li><a href="#neural-geodesic-architecture-1" id="toc-neural-geodesic-architecture-1" class="nav-link" data-scroll-target="#neural-geodesic-architecture-1">Neural Geodesic Architecture</a></li>
  <li><a href="#architectural-details" id="toc-architectural-details" class="nav-link" data-scroll-target="#architectural-details">Architectural Details</a></li>
  <li><a href="#implementation-details-2" id="toc-implementation-details-2" class="nav-link" data-scroll-target="#implementation-details-2">Implementation Details</a></li>
  <li><a href="#integration-with-rhvae" id="toc-integration-with-rhvae" class="nav-link" data-scroll-target="#integration-with-rhvae">Integration with RHVAE</a></li>
  </ul></li>
  <li><a href="#latent-space-alignment-via-procrustes-analysis" id="toc-latent-space-alignment-via-procrustes-analysis" class="nav-link" data-scroll-target="#latent-space-alignment-via-procrustes-analysis">Latent Space Alignment via Procrustes Analysis</a>
  <ul class="collapse">
  <li><a href="#mathematical-formulation" id="toc-mathematical-formulation" class="nav-link" data-scroll-target="#mathematical-formulation">Mathematical Formulation</a></li>
  <li><a href="#algorithmic-procedure" id="toc-algorithmic-procedure" class="nav-link" data-scroll-target="#algorithmic-procedure">Algorithmic Procedure</a></li>
  <li><a href="#significance-for-comparative-analysis" id="toc-significance-for-comparative-analysis" class="nav-link" data-scroll-target="#significance-for-comparative-analysis">Significance for Comparative Analysis</a></li>
  </ul></li>
  <li><a href="#sec-metropolis-kimura" id="toc-sec-metropolis-kimura" class="nav-link" data-scroll-target="#sec-metropolis-kimura">Alternative Metropolis-Kimura Evolutionary Dynamics</a>
  <ul class="collapse">
  <li><a href="#metropolis-kimura-algorithm" id="toc-metropolis-kimura-algorithm" class="nav-link" data-scroll-target="#metropolis-kimura-algorithm">Metropolis-Kimura Algorithm</a></li>
  <li><a href="#effect-of-population-size" id="toc-effect-of-population-size" class="nav-link" data-scroll-target="#effect-of-population-size">Effect of Population Size</a></li>
  <li><a href="#main-text-results" id="toc-main-text-results" class="nav-link" data-scroll-target="#main-text-results">Main Text Results</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="supplementary.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Manuel Razo-Mejia <a href="https://orcid.org/0000-0002-9510-0527" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Department of Biology, Stanford University
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Madhav Mani <a href="mailto:madhav.mani@gmail.com" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0002-5812-4167" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            NSF-Simons Center for Quantitative Biology, Northwestern University
          </p>
        <p class="affiliation">
            Department of Engineering Sciences and Applied Mathematics, Northwestern University
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Dmitri Petrov <a href="mailto:dpetrov@stanford.edu" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0002-3664-9130" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Department of Biology, Stanford University
          </p>
        <p class="affiliation">
            Stanford Cancer Institute, Stanford University School of Medicine
          </p>
        <p class="affiliation">
            Chan Zuckerberg Biohub
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  

<div>
  <div class="keywords">
    <div class="block-title">Keywords</div>
    <p>variational autoencoders, evolutionary dynamics, antibiotic resistance</p>
  </div>
</div>

</header>


<section id="supplementary-materials" class="level1 unnumbered">
<h1 class="unnumbered">Supplementary Materials</h1>
<!-- notes on Bayesian inference -->
<section id="bayesian-inference-of-ic_50-resistance-values" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-inference-of-ic_50-resistance-values">Bayesian inference of <span class="math inline">\(IC_{50}\)</span> resistance values</h2>
<p>The experimental data used throughout this work was kindly provided by the first author of the excellent paper <span class="citation" data-cites="iwasawa2022"><a href="#ref-iwasawa2022" role="doc-biblioref">[1]</a></span>. In there, the authors evolved <em>E. coli</em> strains on different antibiotics via an automated liquid handling platform. Here, we will detail our analysis of the raw data to infer the <span class="math inline">\(IC_{50}\)</span> values of the strains on a panel of antibiotics.</p>
<section id="experimental-design" class="level3">
<h3 class="anchored" data-anchor-id="experimental-design">Experimental design</h3>
<p>To justify the analysis, we first need to understand the experimental design. To determine the antibiotic resistance on multiple antibiotics, the authors adopted a 96-well plate design depicted in <a href="#fig-iwasawa-design" class="quarto-xref">Figure&nbsp;1</a>. On a 96-well plate, each row contained a titration of one of eight antibiotics. This format allowed measuring the optical density of the bacterial culture in each well as a function of each of the antibiotic concentrations. To evolve the strains on a particular antibiotic (e.g., tetracycline as in <a href="#fig-iwasawa-design" class="quarto-xref">Figure&nbsp;1</a>), the authors propagated the strains every day by inoculating a fresh plate with the same panel of antibiotic titrations solely from the well with the highest tetracycline concentration in which there was still measurable growth the day before. In this way, the authors selected for the strains with the highest antibiotic resistance while still measuring the effects of this adaptation on the other antibiotics.</p>
<div id="fig-iwasawa-design" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-iwasawa-design-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig/supplementary/figSI_iwasawa_experiment.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-iwasawa-design-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: <strong>Iwasawa et al.&nbsp;experimental design.</strong> Schematic of the experimental design used by Iwasawa et al. <span class="citation" data-cites="iwasawa2022"><a href="#ref-iwasawa2022" role="doc-biblioref">[1]</a></span>. On a 96-well plate, each of the eight rows contained a titration of one of eight antibiotics. For strains evolved in tetracycline (<code>TET</code>), the plate for each day <span class="math inline">\(t+1\)</span> was inoculated by propagating from the well with the highest tetracycline concentration in which there was still measurable growth on day <span class="math inline">\(t\)</span> to all 96 wells.
</figcaption>
</figure>
</div>
<p><a href="#fig-ic50-progression" class="quarto-xref">Figure&nbsp;2</a> shows a typical progression of the optical density curves over the course of the experimental evolution. When adapted to a particular antibiotic, the optical density curves with their sigmoidal shape shifted to the right, indicating that the strains were able to grow at higher concentrations of the antibiotic.</p>
<div id="fig-ic50-progression" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ic50-progression-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig/supplementary/figSI_ic50_progression.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ic50-progression-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <strong>Iwasawa et al.&nbsp;typical experimental data.</strong> The optical density curves of the bacterial culture in the evolution antibiotic over the course of the experiment. The curves are shifted to the right, indicating that the strains were able to grow at higher concentrations of the antibiotic.
</figcaption>
</figure>
</div>
</section>
<section id="bayesian-inference-of-ic_50-values" class="level3">
<h3 class="anchored" data-anchor-id="bayesian-inference-of-ic_50-values">Bayesian inference of <span class="math inline">\(IC_{50}\)</span> values</h3>
<p>Given the natural sigmoidal shape of the optical density curves, we can model the optical density <span class="math inline">\(OD(x)\)</span> of the bacterial culture as a function of the antibiotic concentration <span class="math inline">\(x\)</span>. Following the approach of <span class="citation" data-cites="iwasawa2022"><a href="#ref-iwasawa2022" role="doc-biblioref">[1]</a></span>, we model the optical density as a function of the antibiotic concentration <span class="math inline">\(x\)</span> as</p>
<p><span id="eq-ic50-model"><span class="math display">\[
f(x) = \frac{a}
{1+\exp \left[b\left(\log _2 x-\log _2 \mathrm{IC}_{50}\right)\right]} + c
\tag{1}\]</span></span></p>
<p>where <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and <span class="math inline">\(c\)</span> are nuisance parameters of the model, <span class="math inline">\(\mathrm{IC}_{50}\)</span> is the parameter of interest, and <span class="math inline">\(x\)</span> is the antibiotic concentration. We can define a function to compute this model.</p>
<p>Given the model presented in <a href="#eq-ic50-model" class="quarto-xref">Equation&nbsp;1</a>, and the data, our objective is to infer the value of all parameters (including the nuisance parameters). By Bayes theorem, we write <span id="eq-bayes-ic50"><span class="math display">\[
\pi(\mathrm{IC}_{50}, a, b, c \mid \text{data}) =
\frac{\pi(\text{data} \mid \mathrm{IC}_{50}, a, b, c)
\pi(\mathrm{IC}_{50}, a, b, c)}
{\pi(\text{data})},
\tag{2}\]</span></span></p>
<p>where <span class="math inline">\(\text{data}\)</span> consists of the pairs of antibiotic concentration and optical density. Let’s begin by defining the likelihood function. For simplicity, we assume each datum is independent and identically distributed (i.i.d.) and write</p>
<p><span id="eq-bayes-ic50-likelihood"><span class="math display">\[
\pi(\text{data} \mid \mathrm{IC}_{50}, a, b, c) =
\prod_{i=1}^n \pi(d_i \mid \mathrm{IC}_{50}, a, b, c),
\tag{3}\]</span></span></p>
<p>where <span class="math inline">\(d_i = (x_i, y_i)\)</span> is the <span class="math inline">\(i\)</span>-th pair of antibiotic concentration and optical density, respectively, and <span class="math inline">\(n\)</span> is the total number of data points. We assume that our experimental measurements can be expressed as</p>
<p><span id="eq-bayes-ic50-likelihood-data"><span class="math display">\[
y_i = f(x_i, \mathrm{IC}_{50}, a, b, c) + \epsilon_i,
\tag{4}\]</span></span></p>
<p>where <span class="math inline">\(\epsilon_i\)</span> is the experimental error. Furthermore, we assume that the experimental error is normally distributed, i.e.,</p>
<p><span id="eq-bayes-ic50-likelihood-error"><span class="math display">\[
\epsilon_i \sim \mathcal{N}(0, \sigma^2),
\tag{5}\]</span></span></p>
<p>where <span class="math inline">\(\sigma^2\)</span> is an unknown variance parameter that must be included in our inference. Notice that we assume the same variance parameter for all data points since <span class="math inline">\(\sigma^2\)</span> is not indexed by <span class="math inline">\(i\)</span>.</p>
<p>Given this likelihood function, we must update our inference on the parameters as</p>
<p><span id="eq-bayes-ic50-posterior"><span class="math display">\[
\pi(\mathrm{IC}_{50}, a, b, c, \sigma^2 \mid \text{data}) =
\frac{\pi(\text{data} \mid \mathrm{IC}_{50}, a, b, c, \sigma^2)
\pi(\mathrm{IC}_{50}, a, b, c, \sigma^2)}
{\pi(\text{data})},
\tag{6}\]</span></span></p>
<p>to include the new parameter <span class="math inline">\(\sigma^2\)</span>. Our likelihood function is then of the form</p>
<p><span id="eq-bayes-ic50-likelihood-model"><span class="math display">\[
y_i \mid \mathrm{IC}_{50}, a, b, c, \sigma^2 \sim
\mathcal{N}(f(x_i, \mathrm{IC}_{50}, a, b, c), \sigma^2).
\tag{7}\]</span></span></p>
<p>For the prior, we assume that all parameters are independent and write <span id="eq-bayes-ic50-prior"><span class="math display">\[
\pi(\mathrm{IC}_{50}, a, b, c, \sigma^2) =
\pi(\mathrm{IC}_{50}) \pi(a) \pi(b) \pi(c) \pi(\sigma^2).
\tag{8}\]</span></span></p>
<p>Let’s detail each prior.</p>
<ol type="1">
<li><p><span class="math inline">\(\mathrm{IC}_{50}\)</span>: The <span class="math inline">\(IC_{50}\)</span> is a strictly positive parameter. However, we will fit for <span class="math inline">\(\log_2(\mathrm{IC}_{50})\)</span>. Thus, we will use a normal prior for <span class="math inline">\(\log_2(\mathrm{IC}_{50})\)</span>. This means we have <span id="eq-bayes-ic50-prior-ic50"><span class="math display">\[
\log_2(\mathrm{IC}_{50}) \sim
\mathcal{N}(
\mu_{\log_2(\mathrm{IC}_{50})}, \sigma_{\log_2(\mathrm{IC}_{50})}^2
).
\tag{9}\]</span></span></p></li>
<li><p><span class="math inline">\(a\)</span>: This nuisance parameter scales the logistic function. Again, the natural scale for this parameter is a strictly positive real number. Thus, we will use a lognormal prior for <span class="math inline">\(a\)</span>. This means we have <span id="eq-bayes-ic50-prior-a"><span class="math display">\[
a \sim \text{LogNormal}(\mu_a, \sigma_a^2).
\tag{10}\]</span></span></p></li>
<li><p><span class="math inline">\(b\)</span>: This parameter controls the steepness of the logistic function. Again, the natural scale for this parameter is a strictly positive real number. Thus, we will use a lognormal prior for <span class="math inline">\(b\)</span>. This means we have <span id="eq-bayes-ic50-prior-b"><span class="math display">\[
b \sim \text{LogNormal}(\mu_b, \sigma_b^2).
\tag{11}\]</span></span></p></li>
<li><p><span class="math inline">\(c\)</span>: This parameter controls the minimum value of the logistic function. Since this is a strictly positive real number that does not necessarily scale with the data, we will use a half-normal prior for <span class="math inline">\(c\)</span>. This means we have <span id="eq-bayes-ic50-prior-c"><span class="math display">\[
c \sim \text{Half-}\mathcal{N}(0, \sigma_c^2).
\tag{12}\]</span></span></p></li>
<li><p><span class="math inline">\(\sigma^2\)</span>: This parameter controls the variance of the experimental error. Since this is a strictly positive real number that does not necessarily scale with the data, we will use a half-normal prior for <span class="math inline">\(\sigma^2\)</span>. This means we have <span id="eq-bayes-ic50-prior-sigma"><span class="math display">\[
\sigma^2 \sim \text{Half-}\mathcal{N}(0, \sigma_{\sigma^2}^2).
\tag{13}\]</span></span></p></li>
</ol>
<section id="residual-based-outlier-detection" class="level4">
<h4 class="anchored" data-anchor-id="residual-based-outlier-detection">Residual-based outlier detection</h4>
<p>To identify and remove outliers from our optical density measurements, we implemented a residual-based outlier detection method. First, we fit the logistic model (<a href="#eq-ic50-model" class="quarto-xref">Equation&nbsp;1</a>) to the data using a deterministic least-squares approach. We then computed the residuals between the fitted model and the experimental measurements. Points with residuals exceeding a threshold of two standard deviations from the mean residual were classified as outliers and removed from subsequent analysis. This approach helped eliminate experimental artifacts while preserving the underlying sigmoidal relationship between antibiotic concentration and optical density.</p>
</section>
</section>
<section id="mcmc-sampling-of-the-posterior" class="level3">
<h3 class="anchored" data-anchor-id="mcmc-sampling-of-the-posterior">MCMC sampling of the posterior</h3>
<p>With this setup, we can sample the posterior distribution using Markov Chain Monte Carlo (MCMC) methods. In particular, we used the <code>Turing.jl</code> package <span class="citation" data-cites="ge2018"><a href="#ref-ge2018" role="doc-biblioref">[2]</a></span> implementation of the No-U-Turn Sampler (NUTS). All code is available in the GitHub repository for this work. <a href="#fig-ic50-ppc" class="quarto-xref">Figure&nbsp;3</a> shows the posterior predictive checks of the posterior distribution for the <span class="math inline">\(IC_{50}\)</span> parameter. Briefly, the posterior predictive checks are a visual method to assess the fit of the model to the data. The idea is to generate samples from the posterior distribution and compare them to the data. If the model is a good fit, the samples should be able to reproduce the data.</p>
<p>The effectiveness of the residual-based outlier detection method is demonstrated in <a href="#fig-ic50-ppc" class="quarto-xref">Figure&nbsp;3</a>, where the posterior predictive checks show significantly tighter credible intervals after outlier removal, particularly in regions of high antibiotic concentration where experimental noise tends to be more pronounced.</p>
<div id="fig-ic50-ppc" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ic50-ppc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig/supplementary/figSI_ic50_ppc.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ic50-ppc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: <strong>Posterior predictive check.</strong> The posterior predictive check of the posterior distribution for the <span class="math inline">\(IC_{50}\)</span> parameter inference. The blue line is the mean of the posterior distribution, the different shades of blue represent the 95%, 68%, and 50% credible regions. The black dots are the raw optical density measurements as provided by Iwasawa et al. <span class="citation" data-cites="iwasawa2022"><a href="#ref-iwasawa2022" role="doc-biblioref">[1]</a></span>.
</figcaption>
</figure>
</div>
<!-- notes on Metropolis-Hastings -->
</section>
</section>
<section id="sec-metropolis" class="level2">
<h2 class="anchored" data-anchor-id="sec-metropolis">Metropolis-Hastings Evolutionary Dynamics</h2>
<p>In the main text, we describe a simple algorithm for evolving a population as a biased random walk on a phenotypic space controlled by both a fitness function and a genotype-phenotype density function. Here, we provide a more detailed description of the algorithm and its implementation.</p>
<section id="landscape-description" class="level3">
<h3 class="anchored" data-anchor-id="landscape-description">Landscape Description</h3>
<p>We assume that phenotypes driving adaptation can be described by real-valued numbers. Let <span class="math inline">\(\underline{x}(t) = (x_1(t), x_2(t), \cdots, x_N(t))\)</span> be an <span class="math inline">\(N\)</span> dimensional vector describing the phenotype of a population at time <span class="math inline">\(t\)</span>. The fitness for a given environment <span class="math inline">\(E\)</span> is a scalar function <span class="math inline">\(F_E(\underline{x})\)</span> such that</p>
<p><span id="eq-fitnes"><span class="math display">\[
F_E: \mathbb{R}^N \to \mathbb{R},
\tag{14}\]</span></span></p>
<p>i.e, the coordinates in phenotype space map to a fitness value. Throughout this work, we consider a series of Gaussian fitness peaks, i.e.,</p>
<p><span id="eq-fitness-peaks"><span class="math display">\[
F_E(\underline{x}) =
\sum_{p=1}^P A_p
\exp\left(
    -\frac{1}{2}
    (\underline{x} - \underline{\hat{x}}^{(p)})^T
    \Sigma_p^{-1}
    (\underline{x} - \underline{\hat{x}}^{(p)})
\right),
\tag{15}\]</span></span></p>
<p>where <span class="math inline">\(P\)</span> is the number of peaks, <span class="math inline">\(A_p\)</span> is the amplitude of the <span class="math inline">\(p\)</span>-th peak, <span class="math inline">\(\underline{\hat{x}}^{(p)}\)</span> is the <span class="math inline">\(N\)</span>-dimensional vector of optimal phenotypes for the <span class="math inline">\(p\)</span>-th peak, and <span class="math inline">\(\Sigma_p\)</span> is the <span class="math inline">\(N \times N\)</span> covariance matrix for the <span class="math inline">\(p\)</span>-th peak. This formulation allows for multiple peaks in the fitness landscape, each potentially having different heights, widths, and covariance structures between dimensions. The covariance matrix <span class="math inline">\(\Sigma_p\)</span> captures potential interactions between different phenotypic dimensions, allowing for more complex and realistic fitness landscapes.</p>
<p>In the same vein, we define a genotype-phenotype density function <span class="math inline">\(GP(\underline{x})\)</span> as a scalar function that maps a phenotype to a mutational effect, i.e.,</p>
<p><span id="eq-genotype-phenotype-density"><span class="math display">\[
GP: \mathbb{R}^N \to \mathbb{R},
\tag{16}\]</span></span></p>
<p>where <span class="math inline">\(GP(\underline{x})\)</span> is related to the probability of a genotype mapping to a phenotype <span class="math inline">\(\underline{x}\)</span>. As a particular density function we will consider a set of negative Gaussian peaks that will serve as “mutational barriers” that limit the range of phenotypes that can be reached. The deeper the peak, the less probable it is that a genotype will map to a phenotype within the peak. This idea captured by the following mutational landscape</p>
<p><span id="eq-genotype-phenotype-density-peaks"><span class="math display">\[
GP(\underline{x}) =
\sum_{p=1}^P -B_p
\exp\left(
    -\frac{1}{2}
    (\underline{x} - \underline{\hat{x}}^{(p)})^T
    \Sigma_p^{-1}
    (\underline{x} - \underline{\hat{x}}^{(p)})
\right),
\tag{17}\]</span></span></p>
<p>where <span class="math inline">\(B_p\)</span> is the depth of the <span class="math inline">\(p\)</span>-th peak, and <span class="math inline">\(\underline{\hat{x}}^{(p)}\)</span> is the <span class="math inline">\(N\)</span>-dimensional vector of optimal phenotypes for the <span class="math inline">\(p\)</span>-th peak.</p>
</section>
<section id="metropolis-hastings-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="metropolis-hastings-algorithm">Metropolis-Hastings Algorithm</h3>
<p>The Metropolis-Hastings algorithm is a Markov Chain Monte Carlo (MCMC) method used to sample from a target probability distribution <span class="math inline">\(\pi(\underline{x})\)</span>. In our evolutionary context, <span class="math inline">\(\pi(\underline{x})\)</span> can be thought of as the steady-state distribution of phenotypes under the combined influence of selection and mutation. We can define this distribution as</p>
<p><span id="eq-metropolis-hastings-target-distribution"><span class="math display">\[
\pi(\underline{x}) \propto \exp\left(
    -\beta U(\underline{x})
\right),
\tag{18}\]</span></span></p>
<p>where <span class="math inline">\(U(\underline{x})\)</span> is the potential function defined as</p>
<p><span id="eq-metropolis-hastings-potential-function"><span class="math display">\[
U(\underline{x}) = \ln F_E(\underline{x}) + \ln GP(\underline{x}).
\tag{19}\]</span></span></p>
<p>This functional form of the potential function is motivated by the desired property of the potential function being a sum of the fitness and genotype-phenotype density functions. <span class="math inline">\(\beta\)</span> in <a href="#eq-metropolis-hastings-target-distribution" class="quarto-xref">Equation&nbsp;18</a> is an inverse temperature parameter (from statistical mechanics) that controls the level of stochasticity in the system. A higher <span class="math inline">\(\beta\)</span> means the system is more likely to move towards higher fitness and genotype-phenotype density. In a sense, <span class="math inline">\(\beta\)</span> can be related to the effective population size, controlling the directness of the evolutionary dynamics.</p>
<p>At each step, we propose a new phenotype <span class="math inline">\(\underline{x}{\prime}\)</span> from a proposal distribution <span class="math inline">\(q(\underline{x}{\prime} | \underline{x})\)</span>. For simplicity, we can use a symmetric proposal distribution, such as a Gaussian centered at the current phenotype. Further, we constrain the proposal distribution to be symmetric, i.e.,</p>
<p><span id="eq-metropolis-hastings-proposal-distribution-symmetry"><span class="math display">\[
q(\underline{x}{\prime} | \underline{x}) =
q(\underline{x} | \underline{x}{\prime}).
\tag{20}\]</span></span></p>
<p>This symmetry simplifies the acceptance probability later on.</p>
<p>The acceptance probability <span class="math inline">\(P_{\text{accept}}\)</span> for moving from <span class="math inline">\(\underline{x}\)</span> to <span class="math inline">\(\underline{x}{\prime}\)</span> is given by</p>
<p><span id="eq-metropolis-hastings-acceptance-probability"><span class="math display">\[
P_{\text{accept}} =
\min\left(1,
    \frac{\pi(\underline{x}{\prime}) q(\underline{x} | \underline{x}{\prime})}
    {\pi(\underline{x}) q(\underline{x}{\prime} | \underline{x})}
\right).
\tag{21}\]</span></span></p>
<p>Given the symmetry of the proposal distribution, <span class="math inline">\(q(\underline{x}{\prime} |
\underline{x}) = q(\underline{x} | \underline{x}{\prime})\)</span> , so the acceptance probability simplifies to</p>
<p><span id="eq-metropolis-hastings-acceptance-probability-simplified"><span class="math display">\[
P_{\text{accept}} = \min\left(
    1,
    \frac{\pi(\underline{x}{\prime})}{\pi(\underline{x})}
    \right).
\tag{22}\]</span></span></p>
<p>Substituting <a href="#eq-metropolis-hastings-target-distribution" class="quarto-xref">Equation&nbsp;18</a> and <a href="#eq-metropolis-hastings-potential-function" class="quarto-xref">Equation&nbsp;19</a> into <a href="#eq-metropolis-hastings-acceptance-probability-simplified" class="quarto-xref">Equation&nbsp;22</a>, we get:</p>
<p><span id="eq-metropolis-hastings-acceptance-probability-simplified-2"><span class="math display">\[
P_{\text{accept}} = \min\left(
    1,
    e^{\beta [\ln F_E(\underline{x}{\prime}) + \ln M(\underline{x}{\prime})] -
    \beta [\ln F_E(\underline{x}) + \ln M(\underline{x})]}
    \right).
\tag{23}\]</span></span></p>
<p>This can be rewritten as</p>
<p><span id="eq-acceptance"><span class="math display">\[
P_{\text{accept}} = \min\left(
    1,
    \frac{F_E(\underline{x}{\prime})^β GP(\underline{x}{\prime})^β}
    {F_E(\underline{x})^β GP(\underline{x})^β}
\right),
\tag{24}\]</span></span></p>
<p>This expression shows that the acceptance probability depends on the difference in fitness and genotype-phenotype density between the current and proposed phenotypes.</p>
</section>
<section id="effect-of-inverse-temperature" class="level3">
<h3 class="anchored" data-anchor-id="effect-of-inverse-temperature">Effect of Inverse Temperature</h3>
<p><a href="#fig-metropolis-inverse-temperature" class="quarto-xref">Figure&nbsp;4</a> shows the effect of the inverse temperature parameter <span class="math inline">\(\beta\)</span> influences evolutionary trajectories in our Metropolis-Hastings framework. Through a series of panels showing both phenotypic trajectories (upper) and fitness dynamics (lower), we can observe how different <span class="math inline">\(\beta\)</span> values affect the balance between exploration and exploitation in phenotypic space.</p>
<p>At low <span class="math inline">\(\beta\)</span> values (left panels), the evolutionary trajectories exhibit highly stochastic behavior, with populations taking meandering paths through the phenotypic landscape. This represents a regime where genetic drift plays a dominant role. As <span class="math inline">\(\beta\)</span> increases (moving right), the trajectories become increasingly deterministic, with populations taking more direct paths toward fitness peaks, while avoiding genotype-to-phenotype density valleys. This transition reflects a shift from drift-dominated to selection-dominated dynamics, where populations more strictly follow the local fitness gradient. The fitness trajectories in the lower panels further reinforce this pattern, showing more erratic fitness fluctuations at low <span class="math inline">\(\beta\)</span> and more monotonic increases at high <span class="math inline">\(\beta\)</span>, consistent with stronger selective pressures.</p>
<div id="fig-metropolis-inverse-temperature" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-metropolis-inverse-temperature-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig/supplementary/figSI_sim_beta_effect.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-metropolis-inverse-temperature-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <strong>Effect of inverse temperature on adaptive dynamics</strong>. Upper panels: Population trajectories on a landscape with a single fitness peak and four genotype-phenotype density peaks. Black dashed lines indicate fitness landscape contours, while white solid lines show genotype-phenotype density contours. Colored lines represent independent population trajectories, with initial positions (crosses) and final positions (triangles). Lower panels: Temporal evolution of population fitness. Panels from left to right show increasing values of inverse temperature (<span class="math inline">\(\beta\)</span>), demonstrating increasingly deterministic adaptive trajectories.
</figcaption>
</figure>
</div>
<!-- notes on RHVAE -->
</section>
</section>
<section id="riemannian-hamiltonian-variational-autoencoders-mathematical-background" class="level2">
<h2 class="anchored" data-anchor-id="riemannian-hamiltonian-variational-autoencoders-mathematical-background">Riemannian Hamiltonian Variational Autoencoders Mathematical Background</h2>
<p>The bulk of this work makes use of the variational autoencoder variation developed by <span class="citation" data-cites="chadebec2020"><a href="#ref-chadebec2020" role="doc-biblioref">[3]</a></span> known as Riemannian Hamiltonian Variational Autoencoder (RHVAE). Understanding this model requires some background that we provide in the following sections. It is not necessary to follow every mathematical derivation in detail for these sections, as they don’t directly pertain to the main contribution of this work. However, we believe that some intuition behind some of the concepts that inspired the RHVAE model can go a long way to understand the model. We invite the reader to refer to the original paper <span class="citation" data-cites="chadebec2020"><a href="#ref-chadebec2020" role="doc-biblioref">[3]</a></span> and the references therein for additional details.</p>
<p>Let us start by reviewing the variational autoencoder model setting and the variational inference framework.</p>
<section id="variational-autoencoder-model-setting" class="level3">
<h3 class="anchored" data-anchor-id="variational-autoencoder-model-setting">Variational Autoencoder Model Setting</h3>
<p>Given a data set <span class="math inline">\(\underline{x}^{1:N} \in \mathcal{X} \subseteq \mathbb{R}^D\)</span>, where the superscript denotes a set of <span class="math inline">\(N\)</span> observations, i.e., <span id="eq-data-set"><span class="math display">\[
\underline{x}^{1:N} =
\left\{
    \underline{x}_1, \underline{x}_2, \ldots, \underline{x}_N
\right\},
\tag{25}\]</span></span> where <span class="math inline">\(\underline{x}_i \in \mathbb{R}^D\)</span> for <span class="math inline">\(i = 1, 2, \ldots, N\)</span>, the variational autoencoder aims to fit a joint distribution <span class="math inline">\(π_\theta(\underline{x}, \underline{z})\)</span> where the data generative process is assumed to involve a set of latent—unobserved—continuous variables <span class="math inline">\(\underline{z} \in \mathcal{Z} \subseteq \mathbb{R}^d\)</span>, where usually <span class="math inline">\(d \ll D\)</span>. In other words, the VAE assumes that the data we get to observe is a consequence of a set of hidden variables <span class="math inline">\(\underline{z}\)</span> that we cannot measure directly. We assume that these variables live in a much lower-dimensional space than the one we observe and we are trying to learn this joint distribution such that, if desired, we can generate new data points by sampling from this distribution. Since the data we observe is a “consequence” of the latent variables, we express this joint distribution as</p>
<p><span id="eq-joint-dist"><span class="math display">\[
π_\theta(\underline{x}, \underline{z}) =
π_\theta(\underline{x}|\underline{z}) \, π_\text{prior}(\underline{z}),
\tag{26}\]</span></span></p>
<p>where <span class="math inline">\(π_\text{prior}(\underline{z})\)</span> is the prior distribution over latent variables and <span class="math inline">\(π_\theta(\underline{x}|\underline{z})\)</span> is the likelihood of the data given the latent variables. This looks exactly as the problem one confronts when performing Bayesian inference. What distinguishes the VAE from other Bayesian inference problems, as we will see, is that in many cases, we do not know the functional form of both the prior and the likelihood function that generated our data, but we have enough data samples that we hope to be able to learn these function using the neural networks function approximation capabilities. Thus, the objective of the VAE is to parameterize this likelihood function via a neural network with parameters <span class="math inline">\(\theta\)</span> given some prior distribution over the latent variables. For simplicity, it is common to choose a standard normal prior distribution, i.e., <span class="math inline">\(π_\text{prior}(\underline{z}) =
\mathcal{N}(\underline{0}, \underline{\underline{I}}_d)\)</span>, where <span class="math inline">\(\underline{\underline{I}}_d\)</span> is the <span class="math inline">\(d\)</span>-dimensional identity matrix <span class="citation" data-cites="kingma2014a kingma2019"><a href="#ref-kingma2014a" role="doc-biblioref">[4]</a>, <a href="#ref-kingma2019" role="doc-biblioref">[5]</a></span>. This way, once we have learned the parameters of the neural network, we can generate new data points by sampling from this prior distribution and running the resulting samples through the likelihood function encoded by this neural network.</p>
<p>Our objective then becomes to find the parameters <span class="math inline">\(\theta\)</span> that maximize the probability of observing the data we actually observed. This is equivalent to maximizing the so-called marginal likelihood of the data, i.e.,</p>
<p><span id="eq-marginal-likelihood"><span class="math display">\[
\max_\theta \, π_\theta(\underline{x}) =
\max_\theta \int d^d\underline{z} \, π_\theta(\underline{x}, \underline{z}).
\tag{27}\]</span></span></p>
<p>But there are two problems with this objective:</p>
<ol type="1">
<li>The marginalization over the latent variables is intractable for most interesting distributions.</li>
<li>The distribution <span class="math inline">\(π_\theta(\underline{x})\)</span> is in general unknown and we need to somehow approximate it.</li>
</ol>
<p>The way around these problems is to introduce a family of parametric distributions <span class="math inline">\(q_\phi(\underline{z} \mid \underline{x})\)</span>, i.e., distributions that can be completely characterized by a set of parameters (think of Gaussians being a parametric family on the mean and the variance), and to approximate the true marginal distribution with one of these parametric distributions. This conceptual change is at the core of approximate methods for Bayesian inference known as variational inference. The objective then becomes to find the parameters <span class="math inline">\(\phi\)</span> that, when used to approximate the posterior distribution <span class="math inline">\(π_\theta(\underline{z}|\underline{x})\)</span>, it is as close as possible to this posterior distribution. To mathematize this objective, we establish that we want to minimize the Kullback-Leibler divergence (also known as the relative entropy) between the variational distribution <span class="math inline">\(q_\phi(\underline{z} \mid \underline{x})\)</span> and the posterior distribution <span class="math inline">\(π_\theta(\underline{z} \mid \underline{x})\)</span>, i.e., we aim to find <span class="math inline">\(q_\phi^*\)</span> such that</p>
<p><span id="eq-variational-approx"><span class="math display">\[
q_\phi^*(\underline{z} \mid \underline{x}) =
\min_\phi \, D_{KL}\left(
    q_\phi(\underline{z} \mid \underline{x}) \|
    π_\theta(\underline{z} \mid \underline{x})
\right).
\tag{28}\]</span></span></p>
<p>After some algebraic manipulation involving the well-known Gibbs inequality, we can show that this objective is equivalent to maximizing the so-called evidence lower bound (ELBO), <span class="math inline">\(\mathcal{L}\)</span>, of the marginal likelihood, i.e.,</p>
<p><span id="eq-elbo"><span class="math display">\[
\mathcal{L} = \left\langle
    \log π_\theta(\underline{x}, \underline{z}) -
    \log q_\phi(\underline{z} \mid \underline{x})
\right\rangle_{q_\phi}
\leq \log π_\theta(\underline{x}).
\tag{29}\]</span></span></p>
<p>Thus, the optimization problem we want to solve invovles estimating the ELBO given some data <span class="math inline">\(\underline{x}\)</span>. However, the ELBO is an expectation over the variational distribution <span class="math inline">\(q_\phi(\underline{z} \mid \underline{x})\)</span>, which is unknown. Therefore, we need to somehow estimate this expectation. Arguably, the most important contribution of the VAE framework is the introduction of the reparametrization trick <span class="citation" data-cites="kingma2014a"><a href="#ref-kingma2014a" role="doc-biblioref">[4]</a></span>. This seemingly innocuous trick allows us to compute an unbiased estimate of the ELBO and its gradient, such that we can maximize it via gradient ascent. For more details on the reparametrization trick, we refer the reader to <span class="citation" data-cites="kingma2014a"><a href="#ref-kingma2014a" role="doc-biblioref">[4]</a></span>.</p>
<p>More recent attempts have focused on modifying <span class="math inline">\(q_\phi(\underline{z} \mid
\underline{x})\)</span> to get a better approximation of the true posterior <span class="math inline">\(π_\theta(\underline{z} \mid \underline{x})\)</span>. The basis of the Riemannian Hamiltonian Variational Autoencoder (RHVAE) framework takes inspiration from some of these approaches. Let’s gain some intuition on each of them.</p>
</section>
<section id="mcmc-and-vi-salimans-kingma-2015" class="level3">
<h3 class="anchored" data-anchor-id="mcmc-and-vi-salimans-kingma-2015">MCMC and VI (Salimans &amp; Kingma 2015)</h3>
<p>One such approach to improve the accuracy of the variational posterior approximation consists in adding a fix number of Markov Chain Monte Carlo (MCMC) steps to the variational posterior approximation, targeting the true posterior <span class="citation" data-cites="salimans2015"><a href="#ref-salimans2015" role="doc-biblioref">[6]</a></span>. In MCMC, rather than optimizing the parameters of a parametric distribution, we sample an initial position <span class="math inline">\(\underline{z}_0\)</span> from a simple distribution <span class="math inline">\(q(\underline{z}_0)\)</span> or <span class="math inline">\(q(\underline{z}_0 \mid \underline{x})\)</span> and subsequently, apply a stochastic transition operator <span class="math inline">\(T(\underline{z}_{t+1} \mid
\underline{z}_t)\)</span> to draw a new value <span class="math inline">\(\underline{z}_{t+1}\)</span> from the distribution</p>
<p><span id="eq-mcmc-transition"><span class="math display">\[
\underline{z}_{t + 1} \sim T(\underline{z}_{t+1} \mid \underline{z}_t).
\tag{30}\]</span></span></p>
<p>Applying this operator over and over again, we build a Markov chain</p>
<p><span id="eq-mcmc-chain"><span class="math display">\[
\underline{z}_0 \to \underline{z}_1 \to \underline{z}_2 \to \cdots
\to \underline{z}_\tau,
\tag{31}\]</span></span></p>
<p>whose long-term behavior—the so-called stationary distribution—is a very good approximation of the true posterior distribution <span class="math inline">\(π_\theta(\underline{z}
\mid \underline{x})\)</span>. In other words, by carefully engineering the transition operator <span class="math inline">\(T(\underline{z}_{t+1} \mid \underline{z}_t)\)</span>, and applying it over and over again, the “histogram” of the samples <span class="math inline">\(\{\underline{z}_t\}_{t=0}^\tau\)</span> will converge to the true posterior distribution <span class="math inline">\(π_\theta(\underline{z} \mid
\underline{x})\)</span>.</p>
<p>The central idea to combine MCMC with variational inference is to interpret the Markov chain</p>
<p><span id="eq-mcmc-chain-def"><span class="math display">\[
q_\phi(\underline{z} | \underline{x}) =
q_\phi(\underline{z}_0 \mid \underline{x})
\prod_{t=1}^\tau T(\underline{z}_t \mid \underline{z}_{t-1}, \underline{x})
\tag{32}\]</span></span></p>
<p>as a variational approximation in an expanded space of variables that includes the original latent variables <span class="math inline">\(\underline{z}\)</span> and the auxiliary variables <span class="math inline">\(\underline{Z} = \{\underline{z}_0, \underline{z}_1, \ldots,
\underline{z}_{\tau-1}\}\)</span>. Note that <span class="math inline">\(\underline{Z}\)</span> reaches up to <span class="math inline">\(\tau-1\)</span> steps, since we consider the final step <span class="math inline">\(\underline{z}_\tau\)</span> to be the original latent variable <span class="math inline">\(\underline{z}\)</span>. In other words, our variational distribution now writes <span id="eq-mcmc-chain-def-expanded"><span class="math display">\[
q_\phi(\underline{z}_\tau, \underline{Z} \mid \underline{x}) =
q_\phi(\underline{z}_0 \mid \underline{x})
\prod_{t=1}^\tau T(\underline{z}_t \mid \underline{z}_{t-1}, \underline{x}).
\tag{33}\]</span></span></p>
<p>Integrating this expression into the ELBO (<a href="#eq-elbo" class="quarto-xref">Equation&nbsp;29</a>) consists of substituting the variational distribution <span class="math inline">\(q_\phi(\underline{z} \mid \underline{x})\)</span> with the expression in <a href="#eq-mcmc-chain-def-expanded" class="quarto-xref">Equation&nbsp;33</a>, i.e., <span id="eq-elbo-aux"><span class="math display">\[
\mathcal{L}_{\text {aux }} =
\left\langle\log
    \left[
        \pi_\theta\left(\underline{x}, \underline{z}_T\right)
        r\left(\underline{Z} \mid \underline{z}_T, \underline{x}\right)
    \right] -
    \log q\left(\underline{Z}, \underline{z}_T \mid \underline{x}\right)
\right\rangle_{q\left(\underline{Z}, \underline{z}_T \mid \underline{x}\right)}
\tag{34}\]</span></span></p>
<p>where <span class="math inline">\(r(\underline{Z} \mid \underline{z}_T, \underline{x})\)</span> is an auxiliary inference distribution that we can choose freely. This auxiliary distribution is necessary to ensure we account for the auxiliary variables <span class="math inline">\(\underline{Z}\)</span> that are now part of our variational distribution. By splitting the joint distribution in <a href="#eq-mcmc-chain-def-expanded" class="quarto-xref">Equation&nbsp;33</a> as</p>
<p><span id="eq-mcmc-chain-def-expanded-2"><span class="math display">\[
q_\phi(\underline{z}_\tau, \underline{Z} \mid \underline{x}) =
q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x}) q_\phi(\underline{z}_\tau \mid \underline{x}),
\tag{35}\]</span></span></p>
<p>and substituting this expression into <a href="#eq-elbo-aux" class="quarto-xref">Equation&nbsp;34</a>, we get</p>
<p><span id="eq-elbo-aux-2"><span class="math display">\[
\mathcal{L}_{\text {aux }} =
\left\langle\log
    \left[
        \pi_\theta\left(\underline{x}, \underline{z}_T\right)
        r\left(\underline{Z} \mid \underline{z}_T, \underline{x}\right)
    \right] -
    \log \left[q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x}) q_\phi(\underline{z}_\tau \mid \underline{x})\right]
\right\rangle_{q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x}) q_\phi(\underline{z}_\tau \mid \underline{x})}.
\tag{36}\]</span></span></p>
<p>Rearranging the terms, we get <span id="eq-elbo-aux-3"><span class="math display">\[
\begin{aligned}
\mathcal{L}_{\text {aux }}
&amp; = \left\langle
    \log \frac{\pi_\theta\left(\underline{x}, \underline{z}_\tau\right)}{q_\phi(\underline{z}_\tau \mid \underline{x})} -
    \log \frac{q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x})}{r(\underline{Z} \mid \underline{z}_\tau, \underline{x})}
\right\rangle_{
    q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x})
    q_\phi(\underline{z}_\tau \mid \underline{x})
}, \\
&amp; = \left\langle
    \log \frac{\pi_\theta\left(\underline{x}, \underline{z}_\tau\right)}{q_\phi(\underline{z}_\tau \mid \underline{x})}
\right\rangle_{
    q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x})
    q_\phi(\underline{z}_\tau \mid \underline{x})
} -
\left\langle
    \log \frac{q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x})}{r(\underline{Z} \mid \underline{z}_\tau, \underline{x})}
\right\rangle_{
    q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x})
    q_\phi(\underline{z}_\tau \mid \underline{x})
}.
\end{aligned}
\tag{37}\]</span></span></p>
<p>From here, we note that the first term in <a href="#eq-elbo-aux-3" class="quarto-xref">Equation&nbsp;37</a> is the original ELBO with the extra expectation taken over the auxiliary variables <span class="math inline">\(\underline{Z}\)</span>. However, this term does not depend on the auxiliary variables <span class="math inline">\(\underline{Z}\)</span>, thus, we can take it out of the expectation, i.e.,</p>
<p><span id="eq-elbo-aux-4"><span class="math display">\[
\mathcal{L}_{\text {aux }} = \mathcal{L} -
\left\langle
    \log \frac{q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x})}{r(\underline{Z} \mid \underline{z}_\tau, \underline{x})}
\right\rangle_{
    q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x})
    q_\phi(\underline{z}_\tau \mid \underline{x})
}.
\tag{38}\]</span></span></p>
<p>For the second term, taking the expectation over the auxiliary variables makes it equivalent to the KL divergence between the variational distribution <span class="math inline">\(q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x})\)</span> and the auxiliary distribution <span class="math inline">\(r(\underline{Z} \mid \underline{z}_\tau, \underline{x})\)</span>, i.e.,</p>
<p><span id="eq-elbo-aux-5"><span class="math display">\[
\mathcal{L}_{\text {aux }} = \mathcal{L} -
\left\langle
    D_{K L}\left[
        q\left(\underline{Z} \mid \underline{z}_T, \underline{x}\right) \|
        r\left(\underline{Z} \mid \underline{z}_T, \underline{x}\right)
    \right]
\right\rangle_{q\left(\underline{z}_T \mid \underline{x}\right)}
\leq \mathcal{L} \leq \log [\pi_\theta(\underline{x})].
\tag{39}\]</span></span></p>
<p>where the inequality follows from the non-negativity of the KL divergence and the fact that the ELBO is a lower bound to the marginal likelihood. Since the variational distribution <span class="math inline">\(q_\phi(\underline{z}_\tau,\mid \underline{x})\)</span> we care about comes from the marginalization of the variational distribution over the auxiliary variables <span class="math inline">\(\underline{Z}\)</span>,</p>
<p><span id="eq-mcmc-marginalization"><span class="math display">\[
q_\phi(\underline{z}_\tau \mid \underline{x}) =
\int d\underline{Z} \,
q_\phi(\underline{Z}, \underline{z}_\tau \mid \underline{x}),
\tag{40}\]</span></span></p>
<p>it is now a very rich family of distributions that can be used to better approximate the true posterior distribution <span class="math inline">\(π_\theta(\underline{z} \mid
\underline{x})\)</span>. However, we are still left with the task of choosing the auxiliary distribution <span class="math inline">\(r(\underline{Z} \mid \underline{z}_\tau,
\underline{x})\)</span>. What Salimans &amp; Kingma <span class="citation" data-cites="salimans2015"><a href="#ref-salimans2015" role="doc-biblioref">[6]</a></span> propose is to define this distribution also as a Markov chain, but in reverse order, i.e.,</p>
<p><span id="eq-mcmc-chain-def-aux"><span class="math display">\[
r(\underline{Z} \mid \underline{z}_\tau, \underline{x}) =
\prod_{t=1}^\tau
T^\dagger(\underline{z}_{t-1} \mid \underline{z}_t, \underline{x}),
\tag{41}\]</span></span></p>
<p>where <span class="math inline">\(T^\dagger(\underline{z}_{t-1} \mid \underline{z}_t, \underline{x})\)</span> is the reverse of the transition operator <span class="math inline">\(T(\underline{z}_{t+1} \mid
\underline{z}_t, \underline{x})\)</span>. Notice that the dependence on <span class="math inline">\(\underline{z}_\tau\)</span> is dropped.</p>
<p>This choice of the auxiliary distribution leads to an upper bound on the marginal log likelihood, of the form</p>
<p><span id="eq-elbo-aux-6"><span class="math display">\[
\begin{aligned}
\log \pi_\theta(\underline{x}) &amp; \geq
\left\langle
    \log \pi_\theta\left(\underline{x}, \underline{z}_\tau\right) -
    \log q_\phi\left(
        \underline{z}_0, \ldots, \underline{z}_\tau \mid \underline{x}
    \right) +
    \log r\left(
        \underline{z}_0, \ldots, \underline{z}_{\tau-1} \mid \underline{x}, \underline{z}_\tau
    \right)
\right\rangle_{q_\phi} \\
&amp; = \left\langle
    \log \left[\frac{
        \pi_\theta\left(\underline{x}, \underline{z}_\tau\right)
        }{
            q_\phi\left(\underline{z}_0 \mid \underline{x}\right)
        }
    \right] +
    \sum_{t=1}^\tau \log \left[
        \frac{
            T^\dagger\left(\underline{z}_{t-1} \mid \underline{z}_t, \underline{x}\right)
        }{
            T\left(\underline{z}_t \mid \underline{z}_{t-1}, \underline{x}\right)
        }
    \right]
\right\rangle_{q_\phi}.
\end{aligned}
\tag{42}\]</span></span></p>
<p>In other words, we can interpret the Markov chain as a parametric distribution over the latent variables <span class="math inline">\(\underline{z}\)</span> that we can use to approximate the true posterior distribution <span class="math inline">\(π_\theta(\underline{z} \mid \underline{x})\)</span>. The unbiased estimator of the marginal likelihood is then given by</p>
<p><span id="eq-mcmc-approx"><span class="math display">\[
\hat{π}_\theta(\underline{x}) =
\frac{
    π_\theta(\underline{x}, \underline{z}_T)
    \prod_{t=1}^T T^\dagger(\underline{z}_{t-1} \mid \underline{z}_t, \underline{x})
}{
    q_\phi(\underline{z}_0 \mid \underline{x})
    \prod_{t=1}^T T(\underline{z}_t \mid \underline{z}_{t-1}, \underline{x})
}.
\tag{43}\]</span></span></p>
<p>Let’s unpack this expression. When performing MCMC, we build a Markov chain</p>
<p><span id="eq-mcmc-chain"><span class="math display">\[
\underline{z}_0 \to \underline{z}_1 \to \underline{z}_2 \to \cdots
\to \underline{z}_T,
\tag{44}\]</span></span></p>
<p>where <span class="math inline">\(\underline{z}_0 \sim q_\phi(\underline{z} \mid \underline{x})\)</span> is the initial position in the chain. At each step, we sample a new position from the transition kernel <span class="math inline">\(T(\underline{z}_{t} \mid \underline{z}_{t-1},
\underline{x})\)</span>, which is a function of the current position and the observed data. This way, we build a Markov chain that converges to the true posterior distribution. Therefore, the denominator of <a href="#eq-mcmc-approx" class="quarto-xref">Equation&nbsp;43</a> computes the probability of the initial position of the chain <span class="math inline">\(q_\phi(\underline{z}_0 \mid
\underline{x})\)</span> times the product of the transition probabilities for each of the <span class="math inline">\(\tau\)</span> steps, <span class="math inline">\(\prod_{t=1}^\tau T(\underline{z}_{t} \mid
\underline{z}_{t-1}, \underline{x})\)</span>.</p>
<p>The numerator of <a href="#eq-mcmc-approx" class="quarto-xref">Equation&nbsp;43</a> computes the equivalent probability, but in reverse order, i.e., starts by sampling the final position of the chain <span class="math inline">\(\underline{z}_T\)</span> from the joint distribution <span class="math inline">\(π_\theta(\underline{x},
\underline{z}_T)\)</span> and then samples the previous positions of the chain <span class="math inline">\(\underline{z}_{T-1}, \underline{z}_{T-2}, \ldots, \underline{z}_0\)</span> from the transition kernel run backwards, <span class="math inline">\(T^\dagger(\underline{z}_{t-1} \mid
\underline{z}_{t}, \underline{x})\)</span>. Including a Markov chain into the variational posterior approximation has the effect of trading off computational efficiency for better posterior approximation. In other words, to improve the quality of the posterior approximation we make use of arguably the most accurate family of inference methods known to date—Markov Chain Monte Carlo (MCMC)—at the cost of increased computational cost. Intuitively, our estimate of the unbiased gradient of the ELBO becomes more and more accurate the more steps we include in the Markov chain, but at the same time, the cost of computing this gradient becomes higher and higher.</p>
</section>
<section id="sec-normalizing-flows" class="level3">
<h3 class="anchored" data-anchor-id="sec-normalizing-flows">Normalizing Flows (Rezende &amp; Mohamed 2015)</h3>
<p>Another approach to imrpove the posterior approximation is to consider the composition of simple smooth invertible transformations <span class="citation" data-cites="rezende2016"><a href="#ref-rezende2016" role="doc-biblioref">[7]</a></span>, such that a variable <span class="math inline">\(\underline{z}_K\)</span> consists of <span class="math inline">\(K\)</span> of such transformations of a random variable <span class="math inline">\(\underline{z}_0\)</span>, sampled from a simple distribution, i.e., <span class="math inline">\(\underline{z}_K\)</span> is built from chaining <span class="math inline">\(K\)</span> simple transformations, <span class="math inline">\(f_x^k\)</span>, of <span class="math inline">\(\underline{z}_0\)</span>,</p>
<p><span id="eq-normalizing-flow"><span class="math display">\[
\underline{z}_K =
f_x^K \circ f_x^{K-1} \circ \cdots \circ f_x^1(\underline{z}_0),
\tag{45}\]</span></span></p>
<p>where <span class="math inline">\(f_i \circ f_j(x) = f_i(f_j(x))\)</span>. Because we define the transformations to be smooth and invertible, the change of variables formula tells us that the density of <span class="math inline">\(\underline{z}_K\)</span> writes</p>
<p><span id="eq-flow-density"><span class="math display">\[
q_\phi(\underline{z}_K \mid \underline{x}) =
q_\phi(\underline{z}_0 \mid \underline{x}) \prod_{k=1}^K |\det J_{f_x^k}|^{-1},
\tag{46}\]</span></span></p>
<p>where</p>
<p><span id="eq-jacobian"><span class="math display">\[
J_{f_x^k} = \frac{\partial f_x^k}{\partial \underline{z}},
\tag{47}\]</span></span></p>
<p>is the Jacobian matrix of the transformation <span class="math inline">\(f_x^k\)</span>. In other words, the transformation of the random variable upon a composition of simple transformations is a product of the Jacobians of each transformation. Thus, if we choose a sequence of simple transformations, with a simple Jacobian matrix, we can construct a more complex distribution over the latent variables. Again, this has the effect of trading off computational efficiency for better posterior approximation, this time in the form of smooth invertible transformations of the original variational distribution. In other words, say that approximating the posterior distribution using a simple distribution, such as a Gaussian, is too simple. We can use a sequence of smooth invertible transformations to transform this simple distribution into a more complex one that can better approximate the true posterior. This again, comes at the cost of increased computational cost.</p>
</section>
<section id="hamiltonian-markov-chain-monte-carlo-hmc" class="level3">
<h3 class="anchored" data-anchor-id="hamiltonian-markov-chain-monte-carlo-hmc">Hamiltonian Markov Chain Monte Carlo (HMC)</h3>
<p>Although seemingly unrelated to the previous two approaches, the Hamiltonian Markov Chain Monte Carlo (HMC) framework provides a way to construct a Markov chain that is informed by the target distribution we are trying to approximate. This property is exploited by the predecessor of the RHVAE framework, the Hamiltonian Variational Autoencoder that we will review in the next section. But first, let us give a brief overview of the main ideas of the HMC framework. We direct the interested reader to the excellent review <span class="citation" data-cites="betancourt2017"><a href="#ref-betancourt2017" role="doc-biblioref">[8]</a></span> for a more detailed treatment of the topic.</p>
<p>In the HMC framework, a random variable <span class="math inline">\(\underline{z}\)</span> is assumed to live in an Euclidean space with a target density <span class="math inline">\(π(\underline{z} \mid \underline{x})\)</span> derived from a potential <span class="math inline">\(U_{\underline{x}}(\underline{z})\)</span>, such that the distribution writes</p>
<p><span id="eq-hmc-dist"><span class="math display">\[
π(\underline{z} \mid \underline{x}) =
\frac{e^{-U_{\underline{x}}(\underline{z})}}
{\int d^d\underline{z} \, e^{-U_{\underline{x}}(\underline{z})}},
\tag{48}\]</span></span></p>
<p>where we define the potential energy as</p>
<p><span id="eq-potential-energy"><span class="math display">\[
U_{\underline{x}}(\underline{z}) \equiv -\log π(\underline{z} \mid \underline{x}).
\tag{49}\]</span></span></p>
<p>Notice that upon substitution of this definition into <a href="#eq-hmc-dist" class="quarto-xref">Equation&nbsp;48</a>, we get the trivial equality</p>
<p><span id="eq-trivial-equality"><span class="math display">\[
π(\underline{z} \mid \underline{x}) =
\frac{\exp[-(-\log π(\underline{z} \mid \underline{x}))]}
{\int d^d\underline{z} \, \exp[-(-\log π(\underline{z} \mid \underline{x}))]} =
\frac{
    π(\underline{z} \mid \underline{x})
}{
    \int d^d\underline{z} \, π(\underline{z} \mid \underline{x})
} =
π(\underline{z} \mid \underline{x}).
\tag{50}\]</span></span></p>
<p>Since it is most of the time impossible to sample directly from <span class="math inline">\(π(\underline{z}
\mid \underline{x})\)</span>, an independent auxiliary variable <span class="math inline">\(\underline{\rho} \in
\mathbb{R}^d\)</span> is introduced and used to sample <span class="math inline">\(\underline{z}\)</span> more efficiently. This variable—referred as the momentum—is such that:</p>
<p><span id="eq-momentum-dist"><span class="math display">\[
\underline{\rho} \sim \mathcal{N}(\underline{0}, \underline{\underline{M}}),
\tag{51}\]</span></span></p>
<p>where <span class="math inline">\(\underline{\underline{M}}\)</span> is the so-called mass matrix. The idea behind HMC is then to work with a distribution that extends the target distribution <span class="math inline">\(π(\underline{z} \mid \underline{x})\)</span> to a distribution over both position and momentum variables, i.e.,</p>
<p><span id="eq-extended-target"><span class="math display">\[
π(\underline{z}, \underline{\rho} \mid \underline{x}) =
π(
    \underline{z} \mid \underline{x}, \underline{\rho})π(\underline{\rho}
    \mid \underline{x}) =
π(\underline{z} \mid \underline{x})π(\underline{\rho}),
\tag{52}\]</span></span></p>
<p>where we assume the value of <span class="math inline">\(\underline{z}\)</span> does not directly depend on the momentum variable <span class="math inline">\(\underline{\rho}\)</span> but only on the observed data <span class="math inline">\(\underline{x}\)</span>, and that the momentum distribution <span class="math inline">\(π(\underline{\rho})\)</span> is independent of all other variables. Using <a href="#eq-hmc-dist" class="quarto-xref">Equation&nbsp;48</a>, the density of this extended distribution can be analogously written as</p>
<p><span id="eq-extended-density"><span class="math display">\[
π(\underline{z}, \underline{\rho} \mid \underline{x}) =
\frac{e^{-H_{\underline{x}}(\underline{z}, \underline{\rho})}}
{\displaystyle\iint
    d^d\underline{z} \,
    d^d\underline{\rho} \,
    e^{-H_{\underline{x}}(\underline{z}, \underline{\rho})}
}.
\tag{53}\]</span></span></p>
<p>where <span class="math inline">\(H_{\underline{x}}(\underline{z}, \underline{\rho})\)</span> is the so-called Hamiltonian, defined as</p>
<p><span id="eq-hamiltonian"><span class="math display">\[
H_{\underline{x}}(\underline{z}, \underline{\rho}) =
-\log π(\underline{z}, \underline{\rho} \mid \underline{x}) =
-\log π(\underline{z} \mid \underline{x}) - \log π(\underline{\rho}).
\tag{54}\]</span></span></p>
<p>Substituting <span class="math inline">\(π(\underline{\rho})\)</span> from <a href="#eq-momentum-dist" class="quarto-xref">Equation&nbsp;51</a> into <a href="#eq-hamiltonian" class="quarto-xref">Equation&nbsp;54</a> gives</p>
<p><span id="eq-hamiltonian-expanded"><span class="math display">\[
\begin{aligned}
H_{\underline{x}}(\underline{z}, \underline{\rho}) &amp;=
-\log π(\underline{z} \mid \underline{x}) +
\frac{1}{2}
\left[\log((2π)^d|\underline{\underline{M}}|)
+ \underline{\rho}^T \underline{\underline{M}}^{-1} \underline{\rho}\right] \\
&amp;= U_{\underline{x}}(\underline{z}) + \kappa(\underline{\rho}).
\end{aligned}
\tag{55}\]</span></span></p>
<p>In physical systems, the Hamiltonian gives the total energy of a system having a position <span class="math inline">\(\underline{z}\)</span> and a momentum <span class="math inline">\(\underline{\rho}\)</span>. <span class="math inline">\(U_{\underline{x}}(\underline{z})\)</span> is referred as the potential energy (since it is position-dependent) and <span class="math inline">\(\kappa(\underline{\rho})\)</span> is the kinetic energy, since it is momentum-dependent.</p>
<p>The point of writing the extended target distribution in this form is that we can take advantage of one specific desirable property of Hamiltonian dynamics: Hamiltonian flows are volume preserving <span class="citation" data-cites="betancourt2017"><a href="#ref-betancourt2017" role="doc-biblioref">[8]</a></span>. In other words, the volume in phase space defined by the position and momentum variables is invariant under Hamiltonian dynamics. What this means for our purposes of sampling from a posterior distribution <span class="math inline">\(\pi(\underline{z} \mid \underline{x})\)</span> is that a walker starting in some position <span class="math inline">\(\underline{z}\)</span> can “traverse” the volume of the target distribution very efficiently traveling through lines of equal probability. In this sense, the Hamiltonian dynamics can be seen as a way to explore the posterior distribution <span class="math inline">\(π(\underline{z} \mid \underline{x})\)</span> very efficiently. To do so, we compute the time evolution of a walker exploring the space of the position and momentum variables using Hamilton’s equations <span id="eq-hamilton-eqs"><span class="math display">\[
\begin{aligned} \frac{d\underline{z}}{dt} &amp;= \frac{\partial
H_{\underline{x}}}{\partial \underline{\rho}}, \\ \frac{d\underline{\rho}}{dt} &amp;=
-\frac{\partial H_{\underline{x}}}{\partial \underline{z}}. \end{aligned}
\tag{56}\]</span></span></p>
<p>One can show that for the particular choice of momentum distribution in <a href="#eq-momentum-dist" class="quarto-xref">Equation&nbsp;51</a>, these equations take the form <span id="eq-hamilton-eqs-expanded"><span class="math display">\[
\begin{aligned}
\frac{d\underline{z}}{dt} &amp;= \underline{\underline{M}}^{-1} \underline{\rho}, \\
\frac{d\underline{\rho}}{dt} &amp;=
-\nabla_{\underline{z}} \log \pi(\underline{z} \mid \underline{x}),
\end{aligned}
\tag{57}\]</span></span></p>
<p>where <span class="math inline">\(\nabla_{\underline{z}}\)</span> is the gradient with respect to the position variable <span class="math inline">\(\underline{z}\)</span>. Unfortunately, the resulting system of partial differential equations is almost always intractable analytically. Therefore, we must use specialized numerical integrators to solve it. The most popular choice is the so-called leapfrog integrator, which is a symplectic integrator that preserves the volume of the phase space. To update the position and momentum variables, the leapfrog integrator takes three steps:</p>
<ol type="1">
<li>A half step for the momentum:</li>
</ol>
<p><span id="eq-leapfrog-half-step"><span class="math display">\[
\underline{\rho}\left(t + \frac{\epsilon}{2}\right) =
\underline{\rho}(t) -
\frac{\epsilon}{2} \nabla_{\underline{z}} H_{\underline{x}}\left(
    \underline{z}(t), \underline{\rho}(t)
\right).
\tag{58}\]</span></span></p>
<ol start="2" type="1">
<li>A full step for the position:</li>
</ol>
<p><span id="eq-leapfrog-full-step"><span class="math display">\[
\underline{z}\left(t + \epsilon\right) =
\underline{z}(t) +
\epsilon \nabla_{\underline{\rho}} H_{\underline{x}}\left(
    \underline{z}(t), \underline{\rho}\left(t + \frac{\epsilon}{2}\right)
\right).
\tag{59}\]</span></span></p>
<ol start="3" type="1">
<li>A half step for the momentum:</li>
</ol>
<p><span id="eq-leapfrog-half-step-2"><span class="math display">\[
\underline{\rho}\left(t + \frac{\epsilon}{2}\right) =
\underline{\rho}\left(t + \frac{\epsilon}{2}\right) -
\frac{\epsilon}{2} \nabla_{\underline{z}} H_{\underline{x}}\left(
    \underline{z}(t + \epsilon), \underline{\rho}\left(t + \frac{\epsilon}{2}\right)
\right),
\tag{60}\]</span></span></p>
<p>where <span class="math inline">\(\epsilon\)</span> is the so-called leapfrog step size.</p>
<p>Without going further into the details of the HMC framework, we hope the reader can imagine how using this symplectic integrator, a walker aiming to explore the posterior distribution <span class="math inline">\(π(\underline{z} \mid \underline{x})\)</span> can traverse the volume of the target distribution very efficiently.</p>
<p>With this conceptual background, we are now ready to review the predecessor of the RHVAE framework, the Hamiltonian Variational Autoencoder framework.</p>
</section>
<section id="hamiltonian-variational-autoencoder-hvae" class="level3">
<h3 class="anchored" data-anchor-id="hamiltonian-variational-autoencoder-hvae">Hamiltonian Variational Autoencoder (HVAE)</h3>
<p>Given the dynamics defined in <a href="#eq-hamilton-eqs-expanded" class="quarto-xref">Equation&nbsp;57</a>, and the numerical integrator defined in <a href="#eq-leapfrog-half-step" class="quarto-xref">Equation&nbsp;58</a>, <a href="#eq-leapfrog-full-step" class="quarto-xref">Equation&nbsp;59</a>, and <a href="#eq-leapfrog-half-step-2" class="quarto-xref">Equation&nbsp;60</a>, we can now take <span class="math inline">\(K\)</span> iterations of the leapfrog integrator to sample a point <span class="math inline">\((\underline{z}_K, \underline{\rho}_K)\)</span> from the extended target distribution defined in <a href="#eq-extended-target" class="quarto-xref">Equation&nbsp;52</a>. This transition from <span class="math inline">\((\underline{z}_0, \underline{\rho}_0)\)</span> to <span class="math inline">\((\underline{z}_K,
\underline{\rho}_K)\)</span> via the symplectic integrator can be thought of as a transformation of the form</p>
<p><span id="eq-iterates"><span class="math display">\[
\{\phi_{\epsilon,\underline{x}}^{(K)} :
\mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}^d \times \mathbb{R}^d\},
\tag{61}\]</span></span></p>
<p>i.e., <span class="math inline">\(\phi_{\epsilon,\underline{x}}^{(K)}\)</span> is a function that takes some input value <span class="math inline">\((\underline{z}, \underline{\rho})\)</span> and advances it <span class="math inline">\(K\)</span> steps in phase space using the leapfrog integrator. The function includes <span class="math inline">\(\epsilon\)</span> to remind the step size of the integrator and <span class="math inline">\(\underline{x}\)</span> to remind its data dependence. To advance <span class="math inline">\(\ell\)</span> steps, we simply compose a one step iteration <span class="math inline">\((\ell=1)\)</span> with the <span class="math inline">\(\ell-1\)</span> function, i.e.,</p>
<p><span id="eq-composition"><span class="math display">\[
\phi_{\epsilon,\underline{x}}^{(\ell)} =
\phi_{\epsilon,\underline{x}}^{(\ell-1)} \circ \phi_{\epsilon,\underline{x}}^{(1)}
\tag{62}\]</span></span></p>
<p>By induction, we can see that <span class="math inline">\(\phi_{\epsilon,\underline{x}}^{(\ell)}\)</span> can be built by <span class="math inline">\(\ell\)</span> compositions of <span class="math inline">\(\phi_{\epsilon,\underline{x}}^{(1)}\)</span>, defining the entire transformation as</p>
<p><span id="eq-transformation"><span class="math display">\[
\phi_{\epsilon,\underline{x}}^{(K)} = \phi_{\epsilon,\underline{x}}^{(1)} \circ
\phi_{\epsilon,\underline{x}}^{(1)} \circ \cdots \circ \phi_{\epsilon,\underline{x}}^{(1)}.
\tag{63}\]</span></span></p>
<p>It is no coincidence that this resembles the composition of functions used in the normalizing flows framework described earlier in <a href="#eq-normalizing-flow" class="quarto-xref">Equation&nbsp;45</a>. We can then think of a <span class="math inline">\(K\)</span>-step leapfrog integrator trajectory that takes</p>
<p><span id="eq-trajectory"><span class="math display">\[
\left(\underline{z}_0, \underline{\rho}_0\right) \rightarrow
\left(\underline{z}_K, \underline{\rho}_K\right)
\tag{64}\]</span></span></p>
<p>as an invertible transformation formed by composing <span class="math inline">\(K\)</span> one-step leapfrog integrator trajectories. The original HVAE framework <span class="citation" data-cites="caterini2018"><a href="#ref-caterini2018" role="doc-biblioref">[9]</a></span> includes an extra step in between each leapfrog step that we now review. This step consisted of a <em>tempering</em> step where an initial temperature <span class="math inline">\(β_0\)</span> is proposed and the momentum is decreased by a factor</p>
<p><span id="eq-momentum-factor"><span class="math display">\[
α_k = \sqrt{\frac{β_{k-1}}{β_k}},
\tag{65}\]</span></span></p>
<p>after each leapfrog step <span class="math inline">\(k\)</span>. This simply means that for step <span class="math inline">\(k\)</span>, the momentum is computed as</p>
<p><span id="eq-momentum-update"><span class="math display">\[
\underline{\rho}_k = α_k \, \underline{\rho}_{k-1}.
\tag{66}\]</span></span></p>
<p>The temperature is then updated as</p>
<p><span id="eq-temp-update"><span class="math display">\[
\sqrt{β_k} = \left[
    \left(1-\frac{1}{\sqrt{β_0}}\right)\frac{k^2}{K^2} + \frac{1}{\sqrt{β_0}}
\right]^{-1}.
\tag{67}\]</span></span></p>
<p>This tempering step tries to produce an effect similar to that of Annealed Importance Sampling (AIS) <span class="citation" data-cites="neal"><a href="#ref-neal" role="doc-biblioref">[10]</a></span>, where the temperature is decreased gradually to produce a smoother distribution that can be better approximated by the variational distribution. Thus, the transformation <span class="math inline">\(\mathcal{H}_{\underline{x}}\)</span> used in <span class="citation" data-cites="caterini2018"><a href="#ref-caterini2018" role="doc-biblioref">[9]</a></span> takes the form</p>
<p><span id="eq-transformation-hvae"><span class="math display">\[
\mathcal{H}_{\underline{x}} = g^K \circ \phi_{\epsilon,\underline{x}}^{(1)}
\circ g^{K-1} \circ \cdots \circ \phi_{\epsilon,\underline{x}}^{(1)} \circ g^0
\circ \phi_{\epsilon,\underline{x}}^{(1)}.
\tag{68}\]</span></span></p>
<p>Since each transformation is smooth and differentiable, this entire transformation is amenable to the reparameterization trick used in the variational autoencoder framework. We can then use this transformation to access an unbiased estimator of the gradient of the ELBO with respect to the variational parameters <span class="math inline">\(\phi\)</span>.</p>
<p>As mentioned in <a href="#sec-normalizing-flows" class="quarto-xref">Section&nbsp;0.3.3</a>, for a series of smooth invertible transformations that define a normalizing flow, the change of variables formula tells us that the density of the transformed variable writes</p>
<p><span id="eq-flow-density-hvae"><span class="math display">\[
q_\phi(\underline{z}_K \mid \underline{x}) =
q_\phi(\underline{z}_0 \mid \underline{x})
|\det J_{\mathcal{H}_{\underline{x}}}|^{-1}.
\tag{69}\]</span></span></p>
<p>For our extended posterior distribution, we have that the initial point is sampled as</p>
<p><span id="eq-extended-posterior-init"><span class="math display">\[
(\underline{z}_0, \underline{\rho}_0) \sim
q_\phi(\underline{z}_0, \underline{\rho}_0 \mid \underline{x}) =
q_\phi(\underline{z}_0 \mid \underline{x}) \, q_\phi(\underline{\rho}_0),
\tag{70}\]</span></span></p>
<p>meaning that only the initial position is sampled from the variational distribution, while the momentum is sampled from a standard normal distribution. For the Jacobian, each step consists of the composition of two functions:</p>
<ol type="1">
<li>The leapfrog integrator step <span class="math inline">\(\phi_{\epsilon,\underline{x}}^{(1)}\)</span>.</li>
<li>The tempering step <span class="math inline">\(g^k\)</span>.</li>
</ol>
<p>Therefore, the determinant of the Jacobian of the transformation is given by the product of the determinants of each of the steps, i.e.,</p>
<p><span id="eq-hvae-jacobian"><span class="math display">\[
|\det J_{\mathcal{H}_{\underline{x}}}| =
\prod_{k=0}^K |\det J_{g^k}|
|\det J_{\phi_{\epsilon,\underline{x}}^{(1)}}|.
\tag{71}\]</span></span></p>
<p>The resulting variational distribution is then given by</p>
<p><span id="eq-hvae-variational-distribution"><span class="math display">\[
\begin{aligned}
q_{\phi}(\underline{z}_K, \underline{\rho}_K \mid \underline{x}) &amp;=
    q_{\phi}(\underline{z}_0 \mid \underline{x})
    q_{\phi}(\underline{\rho}_0)
    |\det J_{\mathcal{H}_{\underline{x}}}|^{-1} \\
    &amp;=
    q_{\phi}(\underline{z}_0 \mid \underline{x})
    q_{\phi}(\underline{\rho}_0)
    \left[
        \prod_{k=0}^K \left| \det J_{g^k} \right|
        \left| \det J_{\phi_{\epsilon,\underline{x}}^{(1)}} \right|
    \right]^{-1}
\end{aligned}
\tag{72}\]</span></span></p>
<p>However, recall that Hamiltonian flows are volume preserving. This means that the volume of the phase space is invariant under the transformation and thus the determinant of the Jacobian is equal to one, i.e.,</p>
<p><span id="eq-hvae-jacobian-det"><span class="math display">\[
|\det J_{\phi_{\epsilon,\underline{x}}^{(1)}}| = 1.
\tag{73}\]</span></span></p>
<p>For the tempering step, since <span class="math inline">\((\underline{z}, \underline{\rho}) \rightarrow
(\underline{z}, \alpha_k\underline{\rho})\)</span>, the determinant of the Jacobian is given by</p>
<p><span id="eq-hvae-tempering-step-det"><span class="math display">\[
|\det J_{g^k}| = \alpha_k^d = \left(\frac{β_{k-1}}{β_k}\right)^{\frac{d}{2}}.
\tag{74}\]</span></span></p>
<p>With this setting in hand <span class="citation" data-cites="caterini2018"><a href="#ref-caterini2018" role="doc-biblioref">[9]</a></span> proposes an algorithm to estimate the gradient of the ELBO with respect to the variational parameters that expands the the usual procedure to includes a series of leapfrog steps and a tempering step (see Algorithm 1 in <span class="citation" data-cites="caterini2018"><a href="#ref-caterini2018" role="doc-biblioref">[9]</a></span> for details).</p>
</section>
<section id="riemannian-hamiltonian-mcmc" class="level3">
<h3 class="anchored" data-anchor-id="riemannian-hamiltonian-mcmc">Riemannian Hamiltonian MCMC</h3>
<p>To improve the representational power of the variational posterior approximation, we can make the reasonable assumption that the latent variables <span class="math inline">\(\underline{z}\)</span> live in a Riemannian manifold <span class="math inline">\(\mathcal{M}\)</span> endowed with a Riemannian metric <span class="math inline">\(\underline{\underline{G}}(\underline{z})\)</span>. Although not entirely correct, one can think of this Riemannian metric as some sort of “position dependent scale bar” for a flat map representing a curved space. In the context of the HMC framework, this Riemannian metric can be included if we allow the momentum variable to be given by</p>
<p><span id="eq-momentum-dist-rhmc"><span class="math display">\[
\underline{\rho} \sim
\mathcal{N}\left(\underline{0}, \underline{\underline{G}}(\underline{z})\right),
\tag{75}\]</span></span></p>
<p>i.e., the auxiliary momentum variable is no longer independent of the position variable <span class="math inline">\(\underline{z}\)</span> but rather is distributed according to a normal distribution with mean zero and covariance matrix given by the position-dependent Riemannian metric <span class="math inline">\(\underline{\underline{G}}(\underline{z})\)</span>. The Hamiltonian governing the dynamics of the system then becomes</p>
<p><span id="eq-hamiltonian-rhmc"><span class="math display">\[
H_{\underline{x}}^{\mathcal{M}}(\underline{z}, \underline{\rho}) =
U_{\underline{x}}^{\mathcal{M}}(\underline{z}) +
\kappa_{\underline{x}}^{\mathcal{M}}(\underline{z}, \underline{\rho}),
\tag{76}\]</span></span></p>
<p>where the <span class="math inline">\(\mathcal{M}\)</span> superscript reminds us that we now consider the latent variables <span class="math inline">\(\underline{z}\)</span> living on a Riemannian manifold. Although the potential energy <span class="math inline">\(U_{\underline{x}}^{\mathcal{M}}(\underline{z})\)</span> is still given by <a href="#eq-potential-energy" class="quarto-xref">Equation&nbsp;49</a>, the kinetic energy term is now position-dependent, given by</p>
<p><span id="eq-kinetic-energy-rhmc"><span class="math display">\[
\kappa_{\underline{x}}^{\mathcal{M}}(\underline{z}, \underline{\rho}) =
\frac{1}{2}
\log((2π)^d|\underline{\underline{G}}(\underline{z})|) +
\frac{1}{2}
\underline{\rho}^T \underline{\underline{G}}(\underline{z})^{-1} \underline{\rho}
\tag{77}\]</span></span></p>
<p>Our target distribution <span class="math inline">\(\pi(\underline{z} \mid \underline{x})\)</span> is still given by <a href="#eq-extended-target" class="quarto-xref">Equation&nbsp;52</a>, but with the Hamiltonian given by <a href="#eq-hamiltonian-rhmc" class="quarto-xref">Equation&nbsp;76</a>. To show that we still recover the target distribution with this change in the Hamiltonian, let us substitute <a href="#eq-hamiltonian-rhmc" class="quarto-xref">Equation&nbsp;76</a> into <a href="#eq-extended-target" class="quarto-xref">Equation&nbsp;52</a>. This results in</p>
<p><span id="eq-target-rhmc"><span class="math display">\[
\pi(\underline{z} \mid \underline{x}) =
\frac{
    \displaystyle\int d^d\underline{\rho} \,
    \exp\left\{
        -U_{\underline{x}}^{\mathcal{M}}(\underline{z}) -
        \frac{1}{2} \log((2π)^d|\underline{\underline{G}}(\underline{z})|) -
        \frac{1}{2} \underline{\rho}^T \underline{\underline{G}}(\underline{z})^{-1} \underline{\rho}
    \right\}
}{
    \displaystyle\iint d^d\underline{z} \, d^d\underline{\rho} \,
    \exp\left\{
        -U_{\underline{x}}^{\mathcal{M}}(\underline{z}) -
        \frac{1}{2} \log((2π)^d|\underline{\underline{G}}(\underline{z})|) -
        \frac{1}{2} \underline{\rho}^T \underline{\underline{G}}(\underline{z})^{-1} \underline{\rho}
    \right\}
}.
\tag{78}\]</span></span></p>
<p>Rearranging terms, we can see that the target distribution is given by</p>
<p><span id="eq-target-rhmc-expanded"><span class="math display">\[
\pi(\underline{z} \mid \underline{x}) =
\frac{
    e^{-U_{\underline{x}}^{\mathcal{M}}(\underline{z})}
    \displaystyle\int d^d\underline{\rho} \,
    \left[
        \frac{
            e^{-\frac{1}{2}
            \underline{\rho}^T
            \underline{\underline{G}}(\underline{z})^{-1}
            \underline{\rho}}
        }{
            (2\pi)^d \sqrt{|\underline{\underline{G}}(\underline{z})|}
        }
    \right]
}
{
    \displaystyle\int d^d\underline{z} \,
    e^{-U_{\underline{x}}^{\mathcal{M}}(\underline{z})}
    \displaystyle\int d^d\underline{\rho} \,
    \left[
        \frac{
            e^{
                -\frac{1}{2}
                \underline{\rho}^T
                \underline{\underline{G}}(\underline{z})^{-1}
                \underline{\rho}
            }
        }{
            (2\pi)^d \sqrt{|\underline{\underline{G}}(\underline{z})|}
        }
    \right]
}.
\tag{79}\]</span></span></p>
<p>where we can recognize the terms in square brackets as the probability density function of a normal distribution. Thus, integrating out the momentum variable <span class="math inline">\(\underline{\rho}\)</span> from the target distribution, we obtain</p>
<p><span id="eq-target-rhmc-marginal"><span class="math display">\[
\pi(\underline{z} \mid \underline{x}) =
\frac{e^{-U_{\underline{x}}^{\mathcal{M}}(\underline{z})}}
{
    \displaystyle\int d^d\underline{z} \,
    e^{-U_{\underline{x}}^{\mathcal{M}}(\underline{z})}
},
\tag{80}\]</span></span></p>
<p>i.e., the correct target distribution as defined in <a href="#eq-hmc-dist" class="quarto-xref">Equation&nbsp;48</a>. Therefore, including the position-dependent Riemannian metric in the momentum variable does not change the target distribution. However, the same cannot be said for the Hamiltonian equations of motion, as we will see next.</p>
<p>Recall that the Hamiltonian equations in <a href="#eq-hamilton-eqs" class="quarto-xref">Equation&nbsp;56</a> require us to compute the gradient of the potential energy with respect to the position variable <span class="math inline">\(\underline{z}\)</span> and the gradient of the kinetic energy with respect to the momentum variable <span class="math inline">\(\underline{\rho}\)</span>. For the position variable, our result from <a href="#eq-hamilton-eqs-expanded" class="quarto-xref">Equation&nbsp;57</a> still holds, with the only difference that the mass matrix is now given by the Riemannian metric <span class="math inline">\(\underline{\underline{G}}(\underline{z})\)</span>. For a single entry of <span class="math inline">\(z_i\)</span>, dynamics are given by</p>
<p><span id="eq-gradient-hamiltonian-position-rhmc"><span class="math display">\[
\frac{d z_i}{d t} =
\frac{\partial H_{\underline{x}}^{\mathcal{M}}(\underline{z}, \underline{\rho})}
{
    \partial \rho_i
} =
\left(
    \nabla_{\underline{z}} H_{\underline{x}}^{\mathcal{M}}
    (\underline{z}, \underline{\rho})
\right)_i =
\left(
    \underline{\underline{G}}(\underline{z})^{-1} \underline{\rho}
\right)_i,
\tag{81}\]</span></span> where <span class="math inline">\(\left(\cdot\right)_i\)</span> denotes the <span class="math inline">\(i\)</span>-th entry of a vector.</p>
<p>For the momentum variable, we use three relatively standard results from matrix calculus:</p>
<ol type="1">
<li>The gradient of the inverse of a matrix function with respect to the argument is given by</li>
</ol>
<p><span id="eq-gradient-inverse-matrix"><span class="math display">\[
\frac{
    \partial \underline{\underline{G}}(\underline{z})^{-1}
}{
    \partial \underline{z}
} =
-\underline{\underline{G}}(\underline{z})^{-1}
\frac{d \underline{\underline{G}}(\underline{z})}{d \underline{z}}
\underline{\underline{G}}(\underline{z})^{-1}.
\tag{82}\]</span></span></p>
<ol start="2" type="1">
<li>The gradient of the determinant of a matrix function with respect to the argument is given by</li>
</ol>
<p><span id="eq-gradient-determinant-matrix"><span class="math display">\[
\frac{
    d
}{
    d \underline{z}
} \operatorname{det} \underline{\underline{G}}(\underline{z}) =
\operatorname{tr}\left(
    \operatorname{adj}(\underline{\underline{G}}(\underline{z}))
    \frac{d \underline{\underline{G}}(\underline{z})}{d \underline{z}}
\right) =
\operatorname{det} \underline{\underline{G}}(\underline{z})
\operatorname{tr}\left(
    \underline{\underline{G}}(\underline{z})^{-1}
    \frac{d \underline{\underline{G}}(\underline{z})}{d \underline{z}}
\right),
\tag{83}\]</span></span> where <span class="math inline">\(\operatorname{adj}\)</span> is the adjoint operator, <span class="math inline">\(\operatorname{tr}\)</span> is the trace operator, and <span class="math inline">\(\operatorname{det}\)</span> is the determinant operator.</p>
<ol start="3" type="1">
<li>The gradient of the logarithm of the determinant of a matrix function with respect to the argument is given by <span id="eq-gradient-log-determinant-matrix"><span class="math display">\[  
\frac{d}{d \underline{z}}
\log (\operatorname{det} \underline{\underline{G}}(\underline{z})) =
\operatorname{tr}\left(
\underline{\underline{G}}(\underline{z})^{-1}
\frac{d \underline{\underline{G}}(\underline{z})}{d \underline{z}}
\right).
\tag{84}\]</span></span></li>
</ol>
<p>Given these results, we can now compute the dynamics of the momentum variable. For a single entry of <span class="math inline">\(\underline{\rho}\)</span>, <span class="math inline">\(\rho_i\)</span>, the resulting expression is of the form</p>
<p><span id="eq-gradient-hamiltonian-momentum-rhmc"><span class="math display">\[
\frac{d \rho_i}{d t} =
\frac{
    \partial H_{\underline{x}}^{\mathcal{M}}(\underline{z}, \underline{\rho})
}{
    \partial z_i
} =
\frac{\partial \ln \pi(\underline{z} \mid \underline{x})}{\partial z_i} -
\frac{1}{2} \operatorname{tr}\left(
    \underline{\underline{G}}(\underline{z})
    \frac{\partial \underline{\underline{G}}(\underline{z})}{\partial z_i}
\right) +
\frac{1}{2}
\underline{\rho}^T
\underline{\underline{G}}(\underline{z})^{-1}
\frac{\partial \underline{\underline{G}}(\underline{z})}{\partial z_i}
\underline{\underline{G}}(\underline{z})^{-1} \underline{\rho}.
\tag{85}\]</span></span></p>
<p>As before, we can adapt the leapfrog integrator to this setting to produce an unbiased estimator of the gradient of the ELBO with respect to the variational parameters. However, the previously presented leapfrog integrator is no longer volume preserving in this setting due to the position-dependent Riemannian metric. Thus, we need to use a generalized version of the leapfrog integrator. This has been previously derived and <span class="citation" data-cites="chadebec2020"><a href="#ref-chadebec2020" role="doc-biblioref">[3]</a></span> lists the following steps for the generalized leapfrog integrator as</p>
<ol type="1">
<li>A half step for the momentum variable</li>
</ol>
<p><span id="eq-generalized-leapfrog-momentum-half-step"><span class="math display">\[
\underline{\rho}\left(t+\frac{\varepsilon}{2}\right) =
\underline{\rho}(t) -
\frac{\varepsilon}{2}
\nabla_{\underline{z}} H_{\underline{x}}^{\mathcal{M}}\left(
    \underline{z}(t),
    \underline{\rho}\left(t+\frac{\varepsilon}{2}\right)
\right).
\tag{86}\]</span></span></p>
<ol start="2" type="1">
<li>A full step for the position variable</li>
</ol>
<p><span id="eq-generalized-leapfrog-position-full-step"><span class="math display">\[
\underline{z}(t+\varepsilon) =
\underline{z}(t) +
\frac{\varepsilon}{2}
\left[
    \nabla_{\underline{z}} H_{\underline{x}}^{\mathcal{M}}\left(
        \underline{z}(t),
        \underline{\rho}\left(t+\frac{\varepsilon}{2}\right)
    \right) +
    \nabla_{\underline{\rho}} H_{\underline{x}}^{\mathcal{M}}\left(
        \underline{z}(t+\varepsilon),
        \underline{\rho}\left(t+\frac{\varepsilon}{2}\right)
    \right)
\right].
\tag{87}\]</span></span></p>
<ol start="3" type="1">
<li>A half step for the momentum variable</li>
</ol>
<p><span id="eq-generalized-leapfrog-momentum-full-step"><span class="math display">\[
\underline{\rho}(t+\varepsilon) =
\underline{\rho}\left(t+\frac{\varepsilon}{2}\right) -
\frac{\varepsilon}{2}
\nabla_{\underline{z}} H_{\underline{x}}^{\mathcal{M}}\left(
    \underline{z}(t+\varepsilon),
    \underline{\rho}\left(t+\frac{\varepsilon}{2}\right)
\right).
\tag{88}\]</span></span></p>
<p>Since the left hand side terms appear on the right hand side, these equations must be solved using fixed point iterations. This means that the numerical implementation of this generalized integrator iterates a step multiple times until it finds a “fixed point”, i.e., a point that, when fed to the right hand side of the equation, does not change the value of the left hand side. In practice, we set the number of iterations to a small number.</p>
</section>
<section id="riemannian-hamiltonian-variational-autoencoder" class="level3">
<h3 class="anchored" data-anchor-id="riemannian-hamiltonian-variational-autoencoder">Riemannian Hamiltonian Variational Autoencoder</h3>
<p>The Riemannian Hamiltonian Variational Autoencoder (RHVAE) method extends the HVAE idea to account for non-Euclidean geometry of the latent space. This means that the latent variables <span class="math inline">\(\underline{z}\)</span> live on a Riemannian manifold. This manifold is endowed with a Riemannian metric <span class="math inline">\(\underline{\underline{G}}(\underline{z})\)</span>, that is position-dependent. We navigate through this manifold using the Hamiltonian equations of motion, utilizing the geometric information encoded in the Riemannian metric.</p>
<p>As with the HAVE, using the generalized leapfrog integrator (<a href="#eq-generalized-leapfrog-momentum-half-step" class="quarto-xref">Equation&nbsp;86</a>, <a href="#eq-generalized-leapfrog-position-full-step" class="quarto-xref">Equation&nbsp;87</a>, <a href="#eq-generalized-leapfrog-momentum-full-step" class="quarto-xref">Equation&nbsp;88</a>) along with a tempering step creates a smooth mapping</p>
<p><span id="eq-rhvae-mapping"><span class="math display">\[
(\underline{z}_0, \underline{\rho}_0) \rightarrow
(\underline{z}_K, \underline{\rho}_K),
\tag{89}\]</span></span></p>
<p>that can be thought of as a kind of normalizing flow informed by the target distribution and the geometry of the latent space. Using the same procedure as the HVAE that includes a tempering step, the resulting variational distribution takes the form of <a href="#eq-hvae-variational-distribution" class="quarto-xref">Equation&nbsp;72</a>, with the only difference that the momentum variable is now position-dependent, i.e.,</p>
<p><span id="eq-rhvae-variational-distribution"><span class="math display">\[
q_{\phi}(\underline{z}_K, \underline{\rho}_K \mid \underline{x}) =
q_{\phi}(\underline{z}_0 \mid \underline{x})
q_{\phi}(\underline{\rho}_0 \mid \underline{z}_0)
\left[
    \prod_{k=1}^{K}
    \left| \det \underline{\underline{J}}_{g^k} \right|
    \left| \det \underline{\underline{J}}_{\phi_{\epsilon,\underline{x}}^{(1)}} \right|
\right]^{-1},
\tag{90}\]</span></span></p>
<p>where, as before, <span class="math inline">\(\underline{\underline{J}}_{\phi_{\epsilon,\underline{x}}^{(1)}}\)</span> is the Jacobian of the generalized leapfrog integrator with step size <span class="math inline">\(\epsilon\)</span>, and <span class="math inline">\(\underline{\underline{J}}_{g^k}\)</span> is the Jacobian of the tempering step. Since we established that Hamiltonian dynamics are volume preserving, the determinant of the Jacobian of the tempering step is one, i.e., <span class="math inline">\(\left| \det
\underline{\underline{J}}_{\phi_{\epsilon,\underline{x}}^{(1)}} \right| = 1\)</span>. Moreover, since we are using the same type of tempering step as in the HVAE, we have the same result for the determinant of the Jacobian of the tempering step as in <a href="#eq-hvae-tempering-step-det" class="quarto-xref">Equation&nbsp;74</a>. After substituting these results into <a href="#eq-rhvae-variational-distribution" class="quarto-xref">Equation&nbsp;90</a>, we obtain</p>
<p><span id="eq-rhvae-variational-distribution-expanded"><span class="math display">\[
q_{\phi}(\underline{z}_K, \underline{\rho}_K \mid \underline{x}) =
q_{\phi}(\underline{z}_0 \mid \underline{x})
q_{\phi}(\underline{\rho}_0 \mid \underline{z}_0)
\beta_0^{d / 2},
\tag{91}\]</span></span></p>
<p>where <span class="math inline">\(d\)</span> is the dimension of the latent space. The main difference from the HVAE is that the term <span class="math inline">\(q_{\phi}(\underline{\rho}_0 \mid \underline{z}_0)\)</span> depends on the position <span class="math inline">\(\underline{z}_0\)</span> of the latent variables. To establish this dependence, we must define the functional form for the metric tensor <span class="math inline">\(\underline{\underline{G}}(\underline{z})\)</span>.</p>
<p>In RHVAE, the metric is learned from the data. However, looking at the generalized leapfrog integrator, we see that the inverse and the determinant of the metric tensor are required. Thus, rather than defining the metric tensor directly, we define the inverse of the metric tensor <span class="math inline">\(\underline{\underline{G}}(\underline{z})^{-1}\)</span>, and use the fact that</p>
<p><span id="eq-metric-determinant-equivalence"><span class="math display">\[
\det \underline{\underline{G}}(\underline{z}) =
\det \underline{\underline{G}}(\underline{z})^{-1}.
\tag{92}\]</span></span></p>
<p>By doing so, we do not have to invert the metric tensor for every leapfrog step. <span class="citation" data-cites="chadebec2020"><a href="#ref-chadebec2020" role="doc-biblioref">[3]</a></span> proposes an inverse metric tensor of the form</p>
<p><span id="eq-metric-tensor-rhvae"><span class="math display">\[
\underline{\underline{G}}(\underline{z}) =
\sum_{i=1}^N
\underline{\underline{L}}_{\Psi_i}
\underline{\underline{L}}_{\Psi_i}^{\top}
\exp \left(
    -\frac{
        \left\| \underline{z} - \underline{\mu}{(\underline{x}_i)} \right\|_2^2
        }{
            T^2
        }
    \right) +
\lambda \underline{\underline{\mathbb{I}}}_l,
\tag{93}\]</span></span></p>
<p>where <span class="math inline">\(\underline{\underline{L}}_{\Psi_i}\)</span> is a lower triangular matrix with positive diagonal entries, <span class="math inline">\(\top\)</span> is the transpose operator, <span class="math inline">\(\underline{\mu}(\underline{x}_i)\)</span> is the mean of the <span class="math inline">\(i\)</span>-th component of the dataset, <span class="math inline">\(T\)</span> is a temperature parameter to smooth the metric tensor, <span class="math inline">\(\lambda\)</span> is a regularization parameter, and <span class="math inline">\(\underline{\underline{\mathbb{I}}}_l\)</span> is the <span class="math inline">\(l \times l\)</span> identity matrix. This last term with <span class="math inline">\(\lambda\)</span> is set for the metric tensor not to be zero. However, usually <span class="math inline">\(\lambda\)</span> is set to a small value, e.g., <span class="math inline">\(10^{-3}\)</span>. The terms <span class="math inline">\(\underline{\mu}{(\underline{x}_i)}\)</span> are referred to as the “<em>centroids</em>” and are given by the mean of the variational posterior, such that,</p>
<p><span id="eq-metric-tensor-rhvae-centroids"><span class="math display">\[
q_{\phi}(\underline{z}_i \mid \underline{x}_i) =
\mathcal{N}\left(
    \underline{\mu}{(\underline{x}_i)},
    \underline{\underline{\Sigma}}(\underline{x}_i)
\right).
\tag{94}\]</span></span></p>
<p>Intuitively, we can think of <span class="math inline">\(\underline{\underline{L}}_{\Psi_i}\)</span> as the triangular matrix in the Cholesky decomposition of <span class="math inline">\(\underline{\underline{G}}^{-1}(\underline{\mu}{(\underline{x}_i)})\)</span> up to a regularization factor. This matrix is learned using a neural network with parameters <span class="math inline">\(\Psi_i\)</span>, mapping</p>
<p><span id="eq-metric-tensor-rhvae-centroids-network"><span class="math display">\[
\underline{x}_i \rightarrow \underline{\underline{L}}_{\Psi_i}.
\tag{95}\]</span></span></p>
<p>The hyperparameters <span class="math inline">\(T\)</span> and <span class="math inline">\(\lambda\)</span> could be learned from the data as well, but, for simplicity, we set them to constants. We invite the reader to refer to <span class="citation" data-cites="chadebec2020"><a href="#ref-chadebec2020" role="doc-biblioref">[3]</a></span> for a more detailed discussion on the training procedure.</p>
<!-- notes on nonlinear manifolds -->
</section>
</section>
<section id="sec-nonlinear" class="level2">
<h2 class="anchored" data-anchor-id="sec-nonlinear">Advantages of Nonlinear Manifold Learning for Fitness Landscapes</h2>
<p>In this section, we provide a theoretical justification for why nonlinear dimensionality reduction methods may offer advantages over linear methods when modeling fitness landscapes. We build upon the concept of a causal manifold that captures the relationship between genotypes and their fitness across multiple environments.</p>
<section id="theoretical-framework-the-causal-manifold" class="level3">
<h3 class="anchored" data-anchor-id="theoretical-framework-the-causal-manifold">Theoretical Framework: The Causal Manifold</h3>
<p>The central concept in our analysis is what we call the causal manifold <span class="math inline">\(\mathcal{M}_c\)</span>—a low-dimensional space that encodes the information necessary to predict fitness across multiple environments. Another way to think about this manifold is in a data-generating process perspective: the causal manifold is the underlying space from which the fitness data is sampled. We define two key functions:</p>
<ol type="1">
<li><p>The mapping from genotype to manifold coordinates: <span id="eq-geno-to-manifold"><span class="math display">\[
\phi: g \in G \to \underline{z} \in \mathcal{M}_{c}.
\tag{96}\]</span></span></p></li>
<li><p>The mapping from manifold coordinates to fitness in environment <span class="math inline">\(E\)</span>: <span id="eq-manifold-to-fitness"><span class="math display">\[
F_{E}: \underline{z} \in \mathcal{M}_c \to f_{E} \in \mathbb{R}.
\tag{97}\]</span></span></p></li>
</ol>
<p>A critical assumption in linear approaches is that <span class="math inline">\(F_E\)</span> takes the form <span id="eq-linear-fitness"><span class="math display">\[
F_E(\underline{z}) = \underline{\beta} \cdot \underline{z} + b,
\tag{98}\]</span></span></p>
<p>where <span class="math inline">\(\underline{\beta}\)</span> is a vector of coefficients and <span class="math inline">\(b\)</span> is a scalar offset. However, this assumption may not hold in general. While local linearity is guaranteed by Taylor’s theorem for small changes in <span class="math inline">\(\underline{z}\)</span></p>
<p><span id="eq-taylor"><span class="math display">\[
F_E(\underline{z} + \Delta\underline{z}) \approx
F_E({\underline{z}}) +
\nabla F_E(\underline{z}) \cdot \Delta \underline{z} +
\mathcal{O}(\Delta\underline{z}^2),
\tag{99}\]</span></span></p>
<p>global linearity across the entire manifold is a much stronger constraint that may not reflect the true complexity of biological systems.</p>
</section>
<section id="limitations-of-linear-methods" class="level3">
<h3 class="anchored" data-anchor-id="limitations-of-linear-methods">Limitations of Linear Methods</h3>
<p>Applying linear dimensionality reduction techniques such as PCA/SVD constructs a flat manifold of dimension <span class="math inline">\(d\)</span> to approximate our fitness data. Through SVD, our <span class="math inline">\(N \times M\)</span> data matrix <span class="math inline">\(\underline{\underline{D}}\)</span> (<span class="math inline">\(N\)</span> being the number of environments and <span class="math inline">\(M\)</span> being the number of genotypes) containing fitness profiles is factorized as</p>
<p><span id="eq-svd"><span class="math display">\[
\underline{\underline{D}} =
\underline{\underline{U}}\,
\underline{\underline{\Sigma}}\,
\underline{\underline{V}}^T.
\tag{100}\]</span></span></p>
<p>The coordinates of genotype <span class="math inline">\(i\)</span> in this linear manifold are given by</p>
<p><span id="eq-linear-coords"><span class="math display">\[
\underline{z}_i = \left[\begin{matrix}
\sigma_1 v_{i1} \\
\sigma_2 v_{i2} \\
\vdots \\
\sigma_d v_{id}
\end{matrix}\right].
\tag{101}\]</span></span></p>
<p>The key property of this linear approach is that Euclidean distances in the manifold directly correspond to differences in fitness profiles in the truncated space</p>
<p><span id="eq-linear-distance"><span class="math display">\[
\|\underline{z}_1 - \underline{z}_2\| =
\|\underline{\hat{f}}_1 - \underline{\hat{f}}_2\|.
\tag{102}\]</span></span></p>
<p>While mathematically elegant and computationally tractable, this linear constraint may limit our ability to capture the true structure of the underlying phenotypic space efficiently.</p>
</section>
<section id="mathematical-advantages-of-nonlinear-manifolds" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-advantages-of-nonlinear-manifolds">Mathematical Advantages of Nonlinear Manifolds</h3>
<p>A critical insight comes from the Whitney Embedding Theorem, which states that any smooth <span class="math inline">\(d'\)</span>-dimensional manifold can be embedded in a Euclidean space of dimension <span class="math inline">\(2d' + 1\)</span>. This theorem explains the “unreasonable effectiveness of linear maps” while simultaneously highlighting their limitations. If the true causal manifold has intrinsic dimension <span class="math inline">\(d'\)</span> but is nonlinear, capturing its structure with a linear approximation generally requires <span class="math inline">\(d &gt; d'\)</span> dimensions.</p>
<p>To formalize this intuition, we consider the causal manifold as a Riemannian manifold with a position-dependent metric tensor <span class="math inline">\(\underline{\underline{G}}(\underline{z})\)</span>. On a nonlinear manifold, this tensor is not the identity matrix, meaning</p>
<p><span id="eq-nonlinear-distance"><span class="math display">\[
\|\underline{z}_1 - \underline{z}_2\| \neq
\|\underline{\hat{f}}_1 - \underline{\hat{f}}_2\|.
\tag{103}\]</span></span></p>
<p>Instead, the distance between points on a Riemannian manifold is given by <span id="eq-riemannian-distance"><span class="math display">\[
d_{\mathcal{M}_c}(\underline{z}_1, \underline{z}_2) =
\min_{\underline{\gamma}} \int_0^1 dt
\sqrt{
    \underline{\dot{\gamma}}(t)^T \,
    \underline{\underline{G}}(\underline{\gamma}(t)) \,
    \underline{\dot{\gamma}}(t)
},
\tag{104}\]</span></span></p>
<p>where <span class="math inline">\(\underline{\gamma}\)</span> is a parametric curve with <span class="math inline">\(\underline{\gamma}(0) =
\underline{z}_1\)</span> and <span class="math inline">\(\underline{\gamma}(1) = \underline{z}_2\)</span>. The curve that minimizes this distance is called a <strong>geodesic</strong>.</p>
<p>The key insight is that when the true causal manifold is nonlinear, approximating it with a linear method requires additional dimensions to compensate for the curvature. This is reflected in the singular value spectrum, where</p>
<p><span id="eq-svd-distance"><span class="math display">\[
\|\underline{\hat{f}}_1 - \underline{\hat{f}}_2\|^2 =
\sum_{j=1}^d \sigma_j^2(v_{1j} - v_{2j})^2.
\tag{105}\]</span></span></p>
<p>In this case, dimensions beyond the intrinsic dimensionality <span class="math inline">\(d'\)</span> (with smaller singular values) are effectively “compensating” for the curvature of the true manifold.</p>
</section>
<section id="nonlinear-methods-for-manifold-learning" class="level3">
<h3 class="anchored" data-anchor-id="nonlinear-methods-for-manifold-learning">Nonlinear Methods for Manifold Learning</h3>
<p>To learn nonlinear manifolds directly, we employ variational autoencoders (VAEs) that can approximate the joint distribution between fitness profiles <span class="math inline">\(\underline{f}\)</span> and latent variables <span class="math inline">\(\underline{z}\)</span> representing coordinates on the causal manifold</p>
<p><span id="eq-joint-prob"><span class="math display">\[
\pi(\underline{f}, \underline{z}) \approx
\pi(\underline{f} | \underline{z})\pi(\underline{z}).
\tag{106}\]</span></span></p>
<p>VAEs employ two key neural networks</p>
<ol type="1">
<li><p>An encoder function that approximates the posterior distribution: <span id="eq-encoder"><span class="math display">\[
\Phi^{(E)}: \underline{f} \in \mathbb{R}^N \rightarrow
\left[\begin{matrix}
\underline{\mu}^{(E)} \\ \\
\underline{\log \underline{\sigma}^{(E)}}
\end{matrix}\right] \in \mathbb{R}^{2d}.
\tag{107}\]</span></span></p></li>
<li><p>A decoder function that maps latent points to fitness profiles: <span id="eq-decoder"><span class="math display">\[
\Psi^{(D)}:
\underline{z} \in \mathcal{M}_c \rightarrow
\underline{\mu}^{(D)} \in \mathbb{R}^N.
\tag{108}\]</span></span></p></li>
</ol>
<p>For Riemannian Hamiltonian Variational Autoencoders (RHVAEs), used throughout this work, we add a third network that explicitly learns the metric tensor <span class="math inline">\(\underline{\underline{G}}(\underline{z})\)</span>, allowing the model to capture the geometric structure of the manifold directly.</p>
</section>
<section id="comparative-advantages-of-nonlinear-methods" class="level3">
<h3 class="anchored" data-anchor-id="comparative-advantages-of-nonlinear-methods">Comparative Advantages of Nonlinear Methods</h3>
<p>Nonlinear methods offer several theoretical advantages over linear approaches:</p>
<ol type="1">
<li><p><strong>Dimensionality Efficiency</strong>: Nonlinear methods can potentially represent the same information with fewer dimensions. If the true causal manifold has intrinsic dimension <span class="math inline">\(d'\)</span> but is nonlinear, linear methods may require <span class="math inline">\(d &gt; d'\)</span> dimensions to achieve comparable accuracy.</p></li>
<li><p><strong>Geometric Fidelity</strong>: By learning the metric tensor directly, nonlinear methods can capture the true geometric structure of the fitness landscape, including features like multiple peaks, valleys, and saddle points that may be difficult to represent in a linear space.</p></li>
<li><p><strong>Predictive Power</strong>: If the true relationship between genotype and fitness is nonlinear, nonlinear methods should provide better predictive accuracy, especially for genotypes distant from those in the training set.</p></li>
<li><p><strong>Information Compression</strong>: By capturing the curvature of the manifold explicitly rather than through additional dimensions, nonlinear methods can provide a more compact and interpretable representation of the fitness landscape.</p></li>
</ol>
</section>
<section id="empirical-evidence-and-caveats" class="level3">
<h3 class="anchored" data-anchor-id="empirical-evidence-and-caveats">Empirical Evidence and Caveats</h3>
<p>While the theoretical advantages of nonlinear methods are clear, empirical validation is essential. We observe this advantage in practice through several metrics:</p>
<ol type="1">
<li><p><strong>Reconstruction Error</strong>: Nonlinear methods achieve lower reconstruction error for the same latent dimension compared to linear methods.</p></li>
<li><p><strong>Singular Value Spectrum</strong>: The slow decay of singular values in the data matrix suggests inherent nonlinearity in the fitness landscape.</p></li>
<li><p><strong>Generalization Performance</strong>: Nonlinear methods show better performance when predicting fitness for held-out genotypes.</p></li>
</ol>
<p>However, it’s important to note that nonlinear methods come with their own challenges:</p>
<ol type="1">
<li><p><strong>Overfitting Risk</strong>: Nonlinear methods may introduce spurious complexity when the true structure is simple.</p></li>
<li><p><strong>Interpretation Challenges</strong>: The biological meaning of nonlinear latent dimensions can be less straightforward than those from linear methods.</p></li>
<li><p><strong>Training Complexity</strong>: Nonlinear methods typically require more data and computational resources for effective training.</p></li>
<li><p><strong>Validation Requirements</strong>: Demonstrating that a learned nonlinear structure reflects true biological relationships rather than mathematical artifacts requires careful validation.</p></li>
</ol>
<p>In this work, we address these challenges through rigorous cross-validation, comparison with linear baselines, and direct analysis of the learned metric structure to ensure biological relevance of our nonlinear representations.</p>
<!-- notes on cross-validation -->
</section>
</section>
<section id="cross-validation-methodology-for-evaluating-latent-space-predictive-power" class="level2">
<h2 class="anchored" data-anchor-id="cross-validation-methodology-for-evaluating-latent-space-predictive-power">Cross-Validation Methodology for Evaluating Latent Space Predictive Power</h2>
<p>To rigorously evaluate whether nonlinear latent space coordinates capture more information about the underlying phenotypic state than linear projections, we implemented a cross-validation framework that tests each model’s ability to predict responses to unseen antibiotics. This approach extends the bi-cross validation methodology described by <span class="citation" data-cites="kinsler2020"><a href="#ref-kinsler2020" role="doc-biblioref">[11]</a></span>, adapting it for both linear (PCA/SVD) and nonlinear (VAE and RHVAE) dimensionality reduction techniques.</p>
<section id="theoretical-background" class="level3">
<h3 class="anchored" data-anchor-id="theoretical-background">Theoretical Background</h3>
<p>The core insight of our cross-validation approach is that if latent space coordinates genuinely capture the underlying phenotypic state of the system, they should enable accurate prediction of cellular responses to antibiotics not used in training. We test this hypothesis by systematically holding out one antibiotic at a time, learning latent space coordinates without information from that antibiotic, and then evaluating how well these coordinates predict the response to the held-out antibiotic.</p>
</section>
<section id="linear-bi-cross-validation-svd" class="level3">
<h3 class="anchored" data-anchor-id="linear-bi-cross-validation-svd">Linear Bi-Cross Validation (SVD)</h3>
<p>For linear models, we implement the bi-cross validation method of <span class="citation" data-cites="kinsler2020"><a href="#ref-kinsler2020" role="doc-biblioref">[11]</a></span>. Given our <span class="math inline">\(IC_{50}\)</span> data matrix <span class="math inline">\(\underline{\underline{X}} \in \mathbb{R}^{m
\times n}\)</span> where rows represent antibiotics and columns represent genotypes, we partition the matrix into four quadrants</p>
<p><span id="eq-crossvalidation-svd"><span class="math display">\[
\underline{\underline{X}} =
\begin{bmatrix}
\underline{\underline{A}} &amp; \underline{\underline{B}} \\
\underline{\underline{C}} &amp; \underline{\underline{D}}
\end{bmatrix}
\tag{109}\]</span></span></p>
<p>Here, <span class="math inline">\(\underline{\underline{A}}\)</span> represents the target quadrant containing the held-out antibiotic-genotype combinations we aim to predict. Specifically, for a given held-out antibiotic <span class="math inline">\(i\)</span>:</p>
<ul>
<li><span class="math inline">\(\underline{\underline{A}} \in \mathbb{R}^{1 \times k}\)</span> contains <span class="math inline">\(IC_{50}\)</span> values for antibiotic <span class="math inline">\(i\)</span> and the validation set genotypes (<span class="math inline">\(k\)</span> genotypes)</li>
<li><span class="math inline">\(\underline{\underline{B}} \in \mathbb{R}^{1 \times (n-k)}\)</span> contains <span class="math inline">\(IC_{50}\)</span> values for antibiotic <span class="math inline">\(i\)</span> and the training set genotypes</li>
<li><span class="math inline">\(\underline{\underline{C}} \in \mathbb{R}^{(m-1) \times k}\)</span> contains <span class="math inline">\(IC_{50}\)</span> values for all other antibiotics and the validation set genotypes</li>
<li><span class="math inline">\(\underline{\underline{D}} \in \mathbb{R}^{(m-1) \times (n-k)}\)</span> contains <span class="math inline">\(IC_{50}\)</span> values for all other antibiotics and the training set genotypes</li>
</ul>
<p>The theoretical foundation of this approach rests on the observation that for a full-rank matrix, we can estimate <span class="math inline">\(\underline{\underline{A}}\)</span> as</p>
<p><span id="eq-crossvalidation-svd-estimate"><span class="math display">\[
\underline{\underline{A}} =
\underline{\underline{B}}\, \underline{\underline{D}}^+ \underline{\underline{C}}
\tag{110}\]</span></span></p>
<p>where <span class="math inline">\(\underline{\underline{D}}^+\)</span> is the Moore-Penrose pseudoinverse of <span class="math inline">\(\underline{\underline{D}}\)</span>. To evaluate how well a low-rank approximation performs, we compute rank-<span class="math inline">\(r\)</span> approximations of <span class="math inline">\(\underline{\underline{D}}\)</span> using SVD:</p>
<ol type="1">
<li>Perform SVD on <span class="math inline">\(\underline{\underline{D}}\)</span>:</li>
</ol>
<p><span id="eq-crossvalidation-svd-svd"><span class="math display">\[
\underline{\underline{D}} =
\underline{\underline{U}}\, \underline{\underline{\Sigma}} \,
\underline{\underline{V}}^T
\tag{111}\]</span></span></p>
<ol start="2" type="1">
<li>Create rank-<span class="math inline">\(r\)</span> approximation by retaining only the top <span class="math inline">\(r\)</span> singular values:</li>
</ol>
<p><span id="eq-crossvalidation-svd-svd-rank"><span class="math display">\[
\underline{\underline{D}}_r = \underline{\underline{U}}\,
\underline{\underline{\Sigma}}_r \underline{\underline{V}}^T
\tag{112}\]</span></span></p>
<ol start="3" type="1">
<li>Compute the predicted matrix:</li>
</ol>
<p><span id="eq-crossvalidation-svd-svd-pred"><span class="math display">\[
\underline{\underline{\hat{A}}}_r =
\underline{\underline{B}}\, \underline{\underline{D}}_r^+
\underline{\underline{C}}
\tag{113}\]</span></span></p>
<ol start="4" type="1">
<li>Calculate the mean squared error (MSE) between <span class="math inline">\(\underline{\underline{A}}\)</span> and <span class="math inline">\(\underline{\underline{\hat{A}}}_r\)</span>:</li>
</ol>
<p><span id="eq-crossvalidation-svd-svd-mse"><span class="math display">\[
\text{MSE}_r =
\frac{1}{|\underline{\underline{A}}|}\sum_{i,j}(\underline{\underline{A}}_{ij} -
\underline{\underline{\hat{A}}}_{r,ij})^2
\tag{114}\]</span></span></p>
<p>By examining how MSE varies with rank, we can determine the minimum dimensionality needed to accurately predict the held-out antibiotic data.</p>
</section>
<section id="non-linear-cross-validation-vae-and-rhvae" class="level3">
<h3 class="anchored" data-anchor-id="non-linear-cross-validation-vae-and-rhvae">Non-Linear Cross-Validation (VAE and RHVAE)</h3>
<p>For nonlinear models (VAE and RHVAE), we adapt the cross-validation approach to account for the different model architecture. The procedure for each held-out antibiotic <span class="math inline">\(i\)</span> consists of two phases:</p>
<section id="phase-1-training-the-encoder" class="level4">
<h4 class="anchored" data-anchor-id="phase-1-training-the-encoder">Phase 1: Training the Encoder</h4>
<p>First, we train a complete model (encoder + decoder) on <span class="math inline">\(IC_{50}\)</span> data from all antibiotics except the held-out antibiotic <span class="math inline">\(i\)</span>. This yields an 85%-15% train-validation split of genotypes for robust training. Denoting the set of all antibiotics as <span class="math inline">\(\mathcal{A}\)</span> and the held-out antibiotic as <span class="math inline">\(a_i\)</span>, we train on the data matrix <span class="math inline">\(\underline{\underline{X}}_{\mathcal{A} \setminus \{a_i\}}\)</span>.</p>
<p>For the VAE, we maximize the evidence lower bound (ELBO):</p>
<p><span id="eq-crossvalidation-vae-elbo"><span class="math display">\[
\mathcal{L}(\theta, \phi) =
\left\langle
    \log p_\theta(\underline{x}|\underline{z})
\right\rangle_{q_\phi(\underline{z}|\underline{x})} -
D_{KL}(q_\phi(\underline{z}|\underline{x}) || \pi(\underline{z}))
\tag{115}\]</span></span></p>
<p>where <span class="math inline">\(\underline{x}\)</span> represents the <span class="math inline">\(\log IC_{50}\)</span> values for a genotype across all antibiotics except <span class="math inline">\(a_i\)</span>, <span class="math inline">\(\underline{z}\)</span> is the latent representation, <span class="math inline">\(q_\phi(\underline{z}|\underline{x})\)</span> is the encoder, and <span class="math inline">\(p_\theta(\underline{x}|\underline{z})\)</span> is the decoder.</p>
<p>For the RHVAE, we similarly maximize the ELBO but with modifications to account for the Riemannian geometry of the latent space. The model learns a position-dependent metric tensor <span class="math inline">\(\underline{\underline{G}}(\underline{z})\)</span> along with the encoder and decoder parameters.</p>
<p>After training, the encoder maps inputs <span class="math inline">\(\underline{x}\)</span> to latent coordinates <span class="math inline">\(\underline{z}\)</span> that capture the underlying phenotypic state without information from antibiotic <span class="math inline">\(a_i\)</span>.</p>
</section>
<section id="phase-2-training-the-missing-antibiotic-decoder" class="level4">
<h4 class="anchored" data-anchor-id="phase-2-training-the-missing-antibiotic-decoder">Phase 2: Training the Missing Antibiotic Decoder</h4>
<p>Once the encoder is trained, we freeze its parameters—effectively fixing the latent space coordinates for each genotype. We then train a decoder-only model to predict the response to the held-out antibiotic <span class="math inline">\(a_i\)</span> using a 50%-50% train-validation split of genotypes</p>
<p><span id="eq-crossvalidation-vae-encoder"><span class="math display">\[
\underline{z} = \text{Encoder}_\phi(\underline{x})
\tag{116}\]</span></span></p>
<p><span id="eq-crossvalidation-vae-decoder"><span class="math display">\[
\hat{y}_i = \text{Decoder}_\theta(\underline{z})
\tag{117}\]</span></span></p>
<p>where <span class="math inline">\(\hat{y}_i\)</span> is the predicted <span class="math inline">\(IC_{50}\)</span> value for antibiotic <span class="math inline">\(a_i\)</span>.</p>
<p>This training maximizes the likelihood of the observed <span class="math inline">\(IC_{50}\)</span> values given the latent coordinates:</p>
<p><span id="eq-crossvalidation-vae-decoder-elbo"><span class="math display">\[
\mathcal{L}_{\text{decoder}} =
\left\langle
    \log p_\theta(y_i|\underline{z})
\right\rangle_{q_\phi(\underline{z}|\underline{x})}
\tag{118}\]</span></span></p>
<p>For both VAE and RHVAE, we use a 2-dimensional latent space to ensure fair comparison with linear methods. After training, we evaluate the model’s predictive performance on the validation set by computing the mean squared error between predicted and actual <span class="math inline">\(IC_{50}\)</span> values.</p>
</section>
</section>
<section id="implementation-details" class="level3">
<h3 class="anchored" data-anchor-id="implementation-details">Implementation Details</h3>
<p>The specific implementation involved several key components:</p>
<ol type="1">
<li><p><strong>Data Organization</strong>: The <span class="math inline">\(IC_{50}\)</span> values were organized in a 3D tensor with dimensions [antibiotics × genotypes × MCMC samples], where the MCMC samples represent posterior samples from the Bayesian inference procedure used to estimate <span class="math inline">\(IC_{50}\)</span> values in <span class="citation" data-cites="iwasawa2022"><a href="#ref-iwasawa2022" role="doc-biblioref">[1]</a></span>.</p></li>
<li><p><strong>Training Protocol</strong>: For each held-out antibiotic:</p>
<ul>
<li>The full model (encoder + decoder) was trained for the linear mapping from all antibiotics except the held-out one to the latent space.</li>
<li>The encoder parameters were frozen, and a new decoder was trained to map from the latent space to the held-out antibiotic’s <span class="math inline">\(IC_{50}\)</span> values.</li>
<li>A tempering parameter <span class="math inline">\(\beta_0\)</span> was set to 0.3 to improve stability during training.</li>
<li>Training used the Adam optimizer with a learning rate of <span class="math inline">\(10^{-3}\)</span>.</li>
<li>Models were trained for 50 epochs with a batch size of 256.</li>
</ul></li>
<li><p><strong>Model Architecture</strong>:</p>
<ul>
<li>The encoder consisted of a joint logarithmic encoder that maps inputs to latent space.</li>
<li>The decoder was a simple network mapping from latent space to antibiotic responses.</li>
<li>For RHVAE, the model incorporated a metric chain with position-dependent Riemannian metric.</li>
</ul></li>
<li><p><strong>Evaluation Metrics</strong>: We assessed predictive performance using mean squared error (MSE) between actual and predicted <span class="math inline">\(IC_{50}\)</span> values on the validation set.</p></li>
</ol>
</section>
<section id="comparative-analysis" class="level3">
<h3 class="anchored" data-anchor-id="comparative-analysis">Comparative Analysis</h3>
<p>To fairly compare the predictive power of linear and nonlinear dimensionality reduction techniques, we plotted the MSE for SVD at different ranks alongside the MSE for 2D VAE and 2D RHVAE models. This visualization, shown in Figure 7 in the main text, demonstrates that for all antibiotics, the 2D nonlinear latent space coordinates provide more accurate predictions than any number of linear dimensions.</p>
<p>The key finding is that the nonlinear latent spaces capture the underlying phenotypic structure of the system more effectively than linear projections, thus enabling better predictions of out-of-sample data. This validates our hypothesis that the phenotypic space of antibiotic resistance has an inherently nonlinear structure that is better represented by VAE and especially RHVAE models.</p>
</section>
<section id="cross-validation-comparison-of-2d-and-3d-latent-spaces" class="level3">
<h3 class="anchored" data-anchor-id="cross-validation-comparison-of-2d-and-3d-latent-spaces">Cross-Validation comparison of 2D and 3D latent spaces</h3>
<p>In Figure 7 in the main text, we tested the predictive power of 2D nonlinear latent space coordinates compared to linear latent space coordinates. There, we showed via a custom cross-validation scheme that the 2D latent space coordinates were more predictive of out-of-sample data than the linear latent space coordinates. Here, we repeat a similar analysis, but using 3D latent space coordinates.</p>
<p>Following the same cross-validation scheme as in the main text, we trained a full model on all but one antibiotic (85%-15% splits for training and validation data). This generates the latent space coordinates without using any information from the antibiotic that was left out. Next, we froze the encoder parameters —equivalent to fixing the latent space coordinates—and trained a decoder-only model on the missing antibiotic data (50%-50% splits for training and validation data). <a href="#fig-SI-data-2D-3D-crossval" class="quarto-xref">Figure&nbsp;5</a> shows the results of this analysis. We can see that other than a couple of exceptions (KM and NFLX), the 3D latent space coordinates are marginally more predictive of out-of-sample data than the 2D latent space coordinates. This suggests that the phenotypic changes associated with the experimental setup can be captured with two effective degrees of freedom.</p>
<div id="fig-SI-data-2D-3D-crossval" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-SI-data-2D-3D-crossval-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig/supplementary/figSI_data_2D-3D-crossval.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-SI-data-2D-3D-crossval-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: <strong>Comparison of 2D and 3D nonlinear latent space models for predicting out-of-sample antibiotic data</strong>. Reconstruction error for each missing antibiotic as a function of linear dimensions used in SVD cross-validation. Horizontal lines represent the accuracy of nonlinear models: 2D-VAE (dark blue, dashed), 2D-RHVAE (dark red, solid), 3D-VAE (dark green, dotted), and 3D-RHVAE (dark purple, dash-dotted). The 3D models show marginal improvement over their 2D counterparts for most antibiotics, suggesting that two effective degrees of freedom capture most of the relevant phenotypic variation.
</figcaption>
</figure>
</div>
<!-- notes on geodesics -->
</section>
</section>
<section id="geodesics-in-latent-space" class="level2">
<h2 class="anchored" data-anchor-id="geodesics-in-latent-space">Geodesics in Latent Space</h2>
<p>In this section, we explore an interesting and surprising property of some of the resulting evolutionary trajectories in latent space. As detailed in the main text, we trained an RHVAE on the <span class="math inline">\(IC_{50}\)</span> data from <span class="citation" data-cites="iwasawa2022"><a href="#ref-iwasawa2022" role="doc-biblioref">[1]</a></span>. The data used to train the RHVAE consists of a matrix with eight rows—one per each antibiotic—and <span class="math inline">\(\approx\)</span> 1300 columns representing one of the lineages at some time point in the experiment. However, at no point, the RHVAE has any knowledge of what genotype or time point corresponds to any particular column in the data matrix. All it sees at every epoch is a random subset of columns of this matrix. Nevertheless, it is interesting to ask whether there is some regularity in the resulting evolutionary trajectories in the learned latent space. In other words, once the RHVAE is trained, we can map the time series corresponding to a particular lineage to the corresponding latent space trajectory. A few of these trajectories are shown in <a href="#fig-data-geodesics" class="quarto-xref">Figure&nbsp;6</a> as gold connected points.</p>
<p>One of the unique features of the RHVAE model is the co-learning of the metric tensor in the latent space. A space endowed with such mathematical structure gives us the ability to compute the shortest path between any two points, commonly referred to as a geodesic. Geodesics are the generalization of the idea of a straight line in Euclidean space to curved spaces, and it is a fundamental concept in Riemannian geometry. Let us define this curve as <span class="math inline">\(\underline{\gamma}(t)\)</span> where <span class="math inline">\(t \in [0, 1]\)</span>. This means that the geodesic is a parametric curve <span class="math inline">\(\underline{\gamma}\)</span> for which we only need to define the initial and final points of the curve, <span class="math inline">\(\underline{\gamma}(0) = \underline{z}_0\)</span> and <span class="math inline">\(\underline{\gamma}(1) = \underline{z}_1\)</span>. What makes it a geodesic is that it minimizes the length of the curve connecting the initial and final points, i.e., it minimizes</p>
<p><span id="eq-geodesic"><span class="math display">\[
L(\underline{\gamma}) = \int_0^1
dt \,
\sqrt{
    \underline{\dot{\gamma}}(t)^T
    \underline{\underline{G}}(\underline{\gamma}(t))
    \underline{\dot{\gamma}}(t)
}
\tag{119}\]</span></span></p>
<p>where <span class="math inline">\(\underline{\dot{\gamma}}(t) = \frac{d}{dt} \underline{\gamma}(t)\)</span> is the velocity vector of the curve and <span class="math inline">\(\underline{\underline{G}}(\underline{\gamma}(t))\)</span> is the metric tensor at the point <span class="math inline">\(\underline{\gamma}(t)\)</span>. Computing this geodesic is a non-trivial task, especially for a data-derived metric tensor with no closed-form expression. Fortunately, we can leverage the generality of neural networks to parameterize the curve and compute the geodesic numerically <span class="citation" data-cites="chen2018a"><a href="#ref-chen2018a" role="doc-biblioref">[12]</a></span>. Again, all we need to do is define the initial and final points of the curve, provide the metric tensor learned by the RHVAE, and let the neural network approximate the shortest path between the two points. <a href="#fig-data-geodesics" class="quarto-xref">Figure&nbsp;6</a> shows the corresponding curves between the initial point (black crosses) and the final point (black triangles) as red curves for a few lineages. We highlight the fact that the curves are not straight lines in the latent space, but rather curved paths that follow the curvature of the latent space. For these few examples, we see that the geodesic curves are very similar to the gold curves, which are the actual trajectories of the lineages in the latent space. This rather surprising result suggests that the best spatial representation for the evolutionary trajectories coincides with the shortest path in the latent space.</p>
<div id="fig-data-geodesics" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-data-geodesics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig/supplementary/figSI_data_geodesics.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-geodesics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: <strong>Geodesic paths in latent space follow evolutionary trajectories</strong> The figure shows the latent space representation of evolutionary trajectories from the Iwasawa et al.&nbsp;dataset <span class="citation" data-cites="iwasawa2022"><a href="#ref-iwasawa2022" role="doc-biblioref">[1]</a></span>. Background coloring represents the metric volume (determinant of the metric tensor), with lighter regions indicating higher local curvature. Gold connected points show actual evolutionary trajectories of different lineages, while red curves represent the computed geodesics (shortest paths) between initial points (black crosses) and final points (black triangles). The striking similarity between geodesics and actual trajectories suggests that evolution in phenotype space tends to follow paths that minimize distance in the geometry-informed latent space.
</figcaption>
</figure>
</div>
<p>So far, these few examples suggest that the shortest path in the latent space coincides with the actual evolutionary trajectory. Since the latent space coordinates of any data point capture information about the resistance profile of the corresponding strain, we expect that the qualitative matching between the geodesic and the actual trajectory should mean that the resulting resistance profile predicted by the geodesic is very similar to the actual experimental resistance profile. To see if this is the case, we must decode the latent space trajectory back to the original space. Doing so projects back all of the points sampled along the geodesic curve to the original data space with eight antibiotic resistance values. However, there is a catch: the geodesic curve is a smooth continuous function of time, but the resistance profile of a strain is a discrete set of values with different step sizes. In other words, when constructing the geodesic curve, we define a continuous function connecting the initial and final point with no sudden jumps in the coordinates. But there is no <em>a priori</em> reason that the resulting experimental curve should be smooth. This statement is not necessarily a consequence of the discrete sampling of the data with a one-day time step that could be resolved by increasing the sampling frequency, but rather a consequence of the genotype-phenotype map. Although in the simulations presented in the main text we assume that all evolutionary steps follow certain distribution of step sizes as a convenient mathematical simplification, the complexity of the genotype-phenotype map might be such that this simplification is not valid. To emphasize this point even further, we can think of the geodesic curve as a prediction of what the cross-resistance values for the different antibiotics ought to be without information about when those values must evolve in time.</p>
<p>The consequence of this is that it is not clear how to align the continuous geodesic curve with the discrete experimental data. We can, however, make use of methodologies developed for similar purposes in the form of time-warping algorithms. In this case, we can use dynamic time warping (DTW) to align the continuous geodesic curve with the discrete experimental data. DTW is a well-known algorithm in the signal processing community for aligning two time series with different step sizes. Effectively, DTW finds the optimal pairing of points between the two time series, with the constraint that the alignment is monotonic, i.e., if point <span class="math inline">\(x_i\)</span> in one time series is aligned to point <span class="math inline">\(y_j\)</span> in the other time series, then <span class="math inline">\(x_{i+1}\)</span> is either aligned to <span class="math inline">\(y_{j}\)</span> or <span class="math inline">\(y_{j+1}\)</span>, never stepping backwards. We use this algorithm to align the points sampled along the geodesic curve in latent space with the resulting experimental curve also in latent space. This point, although subtle, is important: the alignment is done in latent space, not in the original data space. This means that the alignment is done based on the metric tensor learned by the RHVAE, and not on the original data.</p>
<p>Once we have the aligned latent space trajectory, we can decode it back to the original data space and plot the resulting experimental curve. <a href="#fig-data-geodesics-timewarp-1" class="quarto-xref">Figure&nbsp;7</a> and <a href="#fig-data-geodesics-timewarp-2" class="quarto-xref">Figure&nbsp;8</a> show the results of this procedure for the same few lineages shown in <a href="#fig-data-geodesics" class="quarto-xref">Figure&nbsp;6</a>. On the left, we show the same latent space trajectory as in <a href="#fig-data-geodesics" class="quarto-xref">Figure&nbsp;6</a>, on the right, we show the resulting <span class="math inline">\(IC_{50}\)</span> curves for each of the eight antibiotics. The gold curves show the original experimental curves, while the red curves show the geodesic-predicted curves after the dynamic time warping alignment. Although far from perfect, the alignment is remarkably good considering that the predictions were drawn from only knowing the initial and final points of the trajectory in latent space and then computing the shortest path between them. This surprising result begs for further experimental investigation along with extensive theoretical analysis of why this phenomenon occurs. One tantalizing, although highly speculative, possibility is that evolution proceeds in phenotype space following a least action principle, i.e., the path taken by the evolutionary trajectory is the one that minimizes the distance traveled in phenotype space subject to the constraints of phenotype accessibility, as modeled by our genotype-phenotype density function in the main text.</p>
<div id="fig-data-geodesics-timewarp-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-data-geodesics-timewarp-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig/supplementary/figSI_data_geodesics_timewarp_1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-geodesics-timewarp-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: <strong>Geodesic predictions of evolutionary trajectories match experimental data after time alignment.</strong> Left panel shows latent space trajectories (gold: experimental path; red: geodesic prediction) for selected lineages. Right panel shows the corresponding IC50 values for all eight antibiotics over time, comparing experimental measurements (gold) with geodesic predictions after dynamic time warping alignment (red).
</figcaption>
</figure>
</div>
<div id="fig-data-geodesics-timewarp-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-data-geodesics-timewarp-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig/supplementary/figSI_data_geodesics_timewarp_2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-geodesics-timewarp-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: <strong>Additional examples of geodesic-based predictions of antibiotic resistance evolution.</strong> Each panel pair shows another set of lineages where geodesic paths in latent space (left, red curves) were used to predict the temporal evolution of resistance profiles (right). Despite only using initial and final points as inputs, the geodesic predictions (red) capture many features of the actual evolutionary trajectories (gold) after time alignment.
</figcaption>
</figure>
</div>
<!-- notes on fitness landscapes -->
</section>
<section id="geometry-informed-latent-space-reconstruction-of-antibiotic-resistance-landscapes" class="level2">
<h2 class="anchored" data-anchor-id="geometry-informed-latent-space-reconstruction-of-antibiotic-resistance-landscapes">Geometry-Informed Latent Space Reconstruction of Antibiotic Resistance Landscapes</h2>
<p>In the main text, Figure 5 shows the ability of the RHVAE to reconstruct the underlying fitness landscapes’ topography from simulated data. There, we demonstrated that the relative position, number of peaks, and overall shape of the fitness landscapes are well-reproduced by the RHVAE latent space. It is therefore interesting to show the same reconstructions using the experimental data from <span class="citation" data-cites="iwasawa2022"><a href="#ref-iwasawa2022" role="doc-biblioref">[1]</a></span>.</p>
<p>After training an RHVAE model on the <span class="math inline">\(IC_{50}\)</span> data from <span class="citation" data-cites="iwasawa2022"><a href="#ref-iwasawa2022" role="doc-biblioref">[1]</a></span>—the same model used in the main text in Figure 6—we apply the same methodology to reconstruct the fitness landscapes for the eight antibiotics used in the experiment. <a href="#fig-iwasawa-landscapes" class="quarto-xref">Figure&nbsp;9</a> shows the reconstructed fitness landscapes for all eight antibiotics along with the latent space metric volume. One obvious difference between the landscapes reconstructed from the simulated and experimental data is the lack of clear peaks in the experimental landscapes. Although it is possible that such peaks do not exist, it is more likely that the lack of peaks is due to the limited resolution of the experimental data, where only a few initial genotypes were measured.</p>
<div id="fig-iwasawa-landscapes" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-iwasawa-landscapes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig/supplementary/figSI_iwasawa_landscapes.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-iwasawa-landscapes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: <strong>Reconstructed 2D antibiotic resistance landscapes from experimental data</strong>. The first eight panels show the antibiotic resistance landscape reconstructions using a 2D RHVAE trained on the <span class="math inline">\(IC_{50}\)</span> data from <span class="citation" data-cites="iwasawa2022"><a href="#ref-iwasawa2022" role="doc-biblioref">[1]</a></span>. Dotted lines show different level curves on the fitness landscape. The last panel shows the metric volume (the determinant of the metric tensor) of the latent space. The darker the color, the flatter the latent space.
</figcaption>
</figure>
</div>
<p>Given the lack of regularity in the resulting landscapes, we also show the three-dimensional representations of the resistance landscapes for different antibiotics using experimental data from <span class="citation" data-cites="iwasawa2022"><a href="#ref-iwasawa2022" role="doc-biblioref">[1]</a></span>. <a href="#fig-iwasawa-3D-landscapes" class="quarto-xref">Figure&nbsp;10</a> shows the same landscapes as in <a href="#fig-iwasawa-landscapes" class="quarto-xref">Figure&nbsp;9</a>, but using the z-axis to represent the fitness value. From this 3D perspective, the lack of clear peaks is more apparent.</p>
<div id="fig-iwasawa-3D-landscapes" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-iwasawa-3D-landscapes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig/supplementary/figSI_iwasawa_3Dlandscapes.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-iwasawa-3D-landscapes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: <strong>3D visualization of reconstructed antibiotic resistance landscapes</strong>. Three-dimensional representations of the same resistance landscapes shown in <a href="#fig-iwasawa-landscapes" class="quarto-xref">Figure&nbsp;9</a>, using experimental data from <span class="citation" data-cites="iwasawa2022"><a href="#ref-iwasawa2022" role="doc-biblioref">[1]</a></span>. The z-axis represents the fitness value (IC₅₀). This 3D perspective more clearly illustrates the lack of distinct peaks in the experimental data, which may be due to limited sampling of initial genotypes rather than the absence of such features in the actual fitness landscapes.
</figcaption>
</figure>
</div>
<section id="trajectories-in-experimentally-reconstructed-fitness-landscapes" class="level3">
<h3 class="anchored" data-anchor-id="trajectories-in-experimentally-reconstructed-fitness-landscapes">Trajectories in Experimentally-reconstructed Fitness Landscapes</h3>
<p>Given these reconstructed fitness landscapes, we can now explore the evolutionary trajectories of different lineages in the experimental data. In <span class="citation" data-cites="iwasawa2022"><a href="#ref-iwasawa2022" role="doc-biblioref">[1]</a></span>, the authors evolved genotypes in the presence of three different antibiotics: <em>Tetracycline</em> (TET), <em>Kanamycin</em> (KAN), and <em>Norfloxacin</em> (NFLX). Naively, if these fitness landscapes are representative of the underlying landscape available to the genotypes, we would expect the trajectories of these lineages to follow a noisy gradient ascent-like path in the latent space, following the fitness gradient predicted by the reconstructed landscapes. <a href="#fig-iwasawa-trajectories-KM" class="quarto-xref">Figure&nbsp;11</a>, <a href="#fig-iwasawa-trajectories-TET" class="quarto-xref">Figure&nbsp;12</a>, and <a href="#fig-iwasawa-trajectories-NFLX" class="quarto-xref">Figure&nbsp;13</a> show the evolutionary trajectories of the lineages evolved in the presence of <em>Kanamycin</em>, <em>Tetracycline</em>, and <em>Norfloxacin</em>, respectively. The left and right panels show the same trajectories in the 2D and 3D projection of the latent space, respectively. One of the problematic aspects of these trajectories is that in none of the cases do the trajectories follow a path towards the predicted highest fitness peak. We strongly suspect this is a problem with the experimental setup, rather than the entirety of our approach. Our suspicion is that the selection of initial genotypes completely biased the reconstruction of the fitness landscapes. Some of the lineages selected by the authors were previously evolved in the presence of one of these antibiotics, to then be re-evolved in the presence of the other antibiotics. In other words, there were some strains in the initial pool that were already highly resistant to, say, <em>Kanamycin</em>, and then these strains were re-evolved in the presence of <em>Tetracycline</em>. The presence of these strains explains the existence of those peaks that none of the trajectories reach, since these strains evolved for much longer in the corresponding antibiotic compared to this experimental setup. However, further investigation is needed to determine the cause of this behavior.</p>
<div id="fig-iwasawa-trajectories-KM" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-iwasawa-trajectories-KM-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig/supplementary/figSI_iwasawa_landscapes_trajectories_KM.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-iwasawa-trajectories-KM-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: <strong>Evolutionary trajectories of lineages evolved in Kanamycin</strong>. The left panel shows the 2D projection of evolutionary trajectories in the latent space overlaid on the Kanamycin resistance landscape. The right panel shows the same trajectories in a 3D projection. Different colors represent different lineages. Cross symbols indicate the initial point of the trajectory, while triangles indicate the final point. Note that trajectories do not follow paths toward the highest predicted fitness peaks, suggesting potential biases in the experimental setup or limitations in landscape reconstruction.
</figcaption>
</figure>
</div>
<div id="fig-iwasawa-trajectories-TET" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-iwasawa-trajectories-TET-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig/supplementary/figSI_iwasawa_landscapes_trajectories_TET.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-iwasawa-trajectories-TET-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: <strong>Evolutionary trajectories of lineages evolved in Tetracycline</strong>. The left panel shows the 2D projection of evolutionary trajectories in the latent space overlaid on the Tetracycline resistance landscape. The right panel shows the same trajectories in a 3D projection. Different colors represent different lineages. Cross symbols indicate the initial point of the trajectory, while triangles indicate the final point. Similar to the Kanamycin case, trajectories do not consistently follow gradient ascent paths toward fitness peaks.
</figcaption>
</figure>
</div>
<div id="fig-iwasawa-trajectories-NFLX" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-iwasawa-trajectories-NFLX-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig/supplementary/figSI_iwasawa_landscapes_trajectories_NFLX.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-iwasawa-trajectories-NFLX-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: <strong>Evolutionary trajectories of lineages evolved in Norfloxacin</strong>. The left panel shows the 2D projection of evolutionary trajectories in the latent space overlaid on the Norfloxacin resistance landscape. The right panel shows the same trajectories in a 3D projection. Different colors represent different lineages. Cross symbols indicate the initial point of the trajectory, while triangles indicate the final point. As with other antibiotics, the evolutionary paths do not align with the predicted fitness gradients, potentially due to pre-evolved strains in the initial pool biasing landscape reconstruction.
</figcaption>
</figure>
</div>
<!-- notes on Neural Geodesic -->
</section>
</section>
<section id="neural-network-architecture" class="level2">
<h2 class="anchored" data-anchor-id="neural-network-architecture">Neural Network Architecture</h2>
<section id="rhvae-overview" class="level3">
<h3 class="anchored" data-anchor-id="rhvae-overview">RHVAE Overview</h3>
<p>The neural network architecture implemented in this study is a Riemannian Hamiltonian Variational Autoencoder (RHVAE) <span class="citation" data-cites="chadebec2020"><a href="#ref-chadebec2020" role="doc-biblioref">[3]</a></span>. The network was implemented using the <code>Flux.jl</code> framework <span class="citation" data-cites="innes2018"><a href="#ref-innes2018" role="doc-biblioref">[13]</a></span> via the <code>AutoEncoderToolkit.jl</code> package <span class="citation" data-cites="razo-mejia2024"><a href="#ref-razo-mejia2024" role="doc-biblioref">[14]</a></span>, and consists of three main components: an encoder, a decoder, and a metric chain network for the Riemannian metric computations.</p>
</section>
<section id="model-parameters" class="level3">
<h3 class="anchored" data-anchor-id="model-parameters">Model Parameters</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 6%">
<col style="width: 67%">
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Latent Dimensions</td>
<td>2</td>
<td>Dimensionality of the latent space</td>
</tr>
<tr class="even">
<td>Hidden Layer Size</td>
<td>128</td>
<td>Number of neurons in hidden layers</td>
</tr>
<tr class="odd">
<td>Temperature (T)</td>
<td>0.8</td>
<td>Temperature parameter for RHVAE</td>
</tr>
<tr class="even">
<td>Regularization (λ)</td>
<td>0.01</td>
<td>Regularization parameter</td>
</tr>
<tr class="odd">
<td>Number of Centroids</td>
<td>256</td>
<td>Number of centroids for the manifold approximation</td>
</tr>
</tbody>
</table>
</section>
<section id="encoder-architecture" class="level3">
<h3 class="anchored" data-anchor-id="encoder-architecture">Encoder Architecture</h3>
<p>The encoder implements a joint Gaussian log-encoder with the following structure:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Layer</th>
<th>Output Dimensions</th>
<th>Activation Function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Input</td>
<td><code>n_env</code><span class="math inline">\(^*\)</span></td>
<td>-</td>
</tr>
<tr class="even">
<td>Dense 1</td>
<td>128</td>
<td>Identity</td>
</tr>
<tr class="odd">
<td>Dense 2</td>
<td>128</td>
<td>LeakyReLU</td>
</tr>
<tr class="even">
<td>Dense 3</td>
<td>128</td>
<td>LeakyReLU</td>
</tr>
<tr class="odd">
<td>Dense 4</td>
<td>128</td>
<td>LeakyReLU</td>
</tr>
<tr class="even">
<td>µ output</td>
<td>2</td>
<td>Identity</td>
</tr>
<tr class="odd">
<td>logσ output</td>
<td>2</td>
<td>Identity</td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(^*\)</span> <code>n_env</code> is the number of environments on which fitness is determined to train the model.</p>
</section>
<section id="decoder-architecture" class="level3">
<h3 class="anchored" data-anchor-id="decoder-architecture">Decoder Architecture</h3>
<p>The decoder implements a simple Gaussian decoder with the following structure:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Layer</th>
<th>Output Dimensions</th>
<th>Activation Function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Input</td>
<td>2</td>
<td>-</td>
</tr>
<tr class="even">
<td>Dense 1</td>
<td>128</td>
<td>Identity</td>
</tr>
<tr class="odd">
<td>Dense 2</td>
<td>128</td>
<td>LeakyReLU</td>
</tr>
<tr class="even">
<td>Dense 3</td>
<td>128</td>
<td>LeakyReLU</td>
</tr>
<tr class="odd">
<td>Dense 4</td>
<td>128</td>
<td>LeakyReLU</td>
</tr>
<tr class="even">
<td>Output</td>
<td><code>n_env</code></td>
<td>Identity</td>
</tr>
</tbody>
</table>
</section>
<section id="metric-chain-architecture" class="level3">
<h3 class="anchored" data-anchor-id="metric-chain-architecture">Metric Chain Architecture</h3>
<p>The metric chain computes the Riemannian metric tensor with the following structure:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Layer</th>
<th>Output Dimensions</th>
<th>Activation Function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Input</td>
<td><code>n_env</code></td>
<td>-</td>
</tr>
<tr class="even">
<td>Dense 1</td>
<td>128</td>
<td>Identity</td>
</tr>
<tr class="odd">
<td>Dense 2</td>
<td>128</td>
<td>LeakyReLU</td>
</tr>
<tr class="even">
<td>Dense 3</td>
<td>128</td>
<td>LeakyReLU</td>
</tr>
<tr class="odd">
<td>Dense 4</td>
<td>128</td>
<td>LeakyReLU</td>
</tr>
<tr class="even">
<td>Diagonal Output</td>
<td>2</td>
<td>Identity</td>
</tr>
<tr class="odd">
<td>Lower Triangular Output</td>
<td>1</td>
<td>Identity</td>
</tr>
</tbody>
</table>
</section>
<section id="data-preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="data-preprocessing">Data Preprocessing</h3>
<p>The input data underwent the following preprocessing steps:</p>
<ol type="1">
<li><p>Logarithmic transformation of fitness values.</p></li>
<li><p>Z-score standardization (mean = 0, std = 1) per environment.</p></li>
<li><p>K-medoids clustering to select centroids for the manifold approximation.</p></li>
</ol>
</section>
<section id="implementation-details-1" class="level3">
<h3 class="anchored" data-anchor-id="implementation-details-1">Implementation Details</h3>
<p>The model was implemented using:</p>
<ul>
<li><p><code>Julia</code> programming language.</p></li>
<li><p><code>Flux.jl</code> for neural network architecture.</p></li>
<li><p><code>AutoEncoderToolkit.jl</code> for RHVAE-specific components.</p></li>
<li><p>Random seed set to 42 for reproducibility.</p></li>
</ul>
<p>The complete model state and architecture were saved in <code>JLD2</code> format for reproducibility and future use.</p>
<p>All of the code used to implement the model is available in the GitHub repository for this project.</p>
</section>
</section>
<section id="neural-geodesic-architecture" class="level2">
<h2 class="anchored" data-anchor-id="neural-geodesic-architecture">Neural Geodesic Architecture</h2>
<p>The neural network architecture implemented in this study is a Neural Geodesic model designed to find geodesics on the latent space manifold of a Riemannian Hamiltonian Variational Autoencoder (RHVAE) <span class="citation" data-cites="chen2018a"><a href="#ref-chen2018a" role="doc-biblioref">[12]</a></span>. The network is implemented using the Flux.jl framework and learns to parameterize geodesic curves between points in the latent space.</p>
<section id="model-parameters-1" class="level3">
<h3 class="anchored" data-anchor-id="model-parameters-1">Model Parameters</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 32%">
<col style="width: 46%">
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Hidden Layer Size</td>
<td>32</td>
<td>Number of neurons in hidden layers</td>
</tr>
<tr class="even">
<td>Input Dimension</td>
<td>1</td>
<td>Time parameter t ∈ [0,1]</td>
</tr>
<tr class="odd">
<td>Output Dimension</td>
<td>2</td>
<td>Dimensionality of the latent space</td>
</tr>
<tr class="even">
<td>Endpoints</td>
<td>z_init=[0,0], z_end=[1,1]</td>
<td>Start and end points of the geodesic</td>
</tr>
</tbody>
</table>
</section>
<section id="neural-geodesic-architecture-1" class="level3">
<h3 class="anchored" data-anchor-id="neural-geodesic-architecture-1">Neural Geodesic Architecture</h3>
<p>The neural geodesic implements a feed-forward network that maps from the time domain to the latent space with the following structure:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Layer</th>
<th>Output Dimensions</th>
<th>Activation Function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Input</td>
<td>1</td>
<td>-</td>
</tr>
<tr class="even">
<td>Dense 1</td>
<td>32</td>
<td>Identity</td>
</tr>
<tr class="odd">
<td>Dense 2</td>
<td>32</td>
<td>TanH</td>
</tr>
<tr class="even">
<td>Dense 3</td>
<td>32</td>
<td>TanH</td>
</tr>
<tr class="odd">
<td>Dense 4</td>
<td>32</td>
<td>TanH</td>
</tr>
<tr class="even">
<td>Output</td>
<td>2</td>
<td>Identity</td>
</tr>
</tbody>
</table>
</section>
<section id="architectural-details" class="level3">
<h3 class="anchored" data-anchor-id="architectural-details">Architectural Details</h3>
<ol type="1">
<li><strong>Input Processing</strong>
<ul>
<li>Takes a single time parameter <span class="math inline">\(t \in [0,1]\)</span>.</li>
<li>Maps to the latent space dimension (2D in this implementation).</li>
<li>Designed to learn smooth geodesic curves.</li>
</ul></li>
<li><strong>Network Structure</strong>
<ul>
<li>Follows a deep architecture with 4 hidden layers.</li>
<li>Uses <code>tanh</code> activation for smooth curve generation.</li>
<li>Identity activation at input and output layers for unrestricted range.</li>
</ul></li>
<li><strong>Output Constraints</strong>
<ul>
<li>Network outputs must satisfy boundary conditions:
<ul>
<li><span class="math inline">\(\gamma(0) = \underline{z}_{\text{init}}\)</span>.</li>
<li><span class="math inline">\(\gamma(1) = \underline{z}_{\text{end}}\)</span>.</li>
</ul></li>
<li>Generated curve represents path in latent space.</li>
</ul></li>
</ol>
</section>
<section id="implementation-details-2" class="level3">
<h3 class="anchored" data-anchor-id="implementation-details-2">Implementation Details</h3>
<p>The model is implemented with:</p>
<ul>
<li><p><code>Julia</code> programming language.</p></li>
<li><p><code>Flux.jl</code> for neural network architecture.</p></li>
<li><p><code>AutoEncode.diffgeo</code> package for geodesic-specific components.</p></li>
<li><p>Random seed set to 42 for reproducibility.</p></li>
</ul>
<p>The model is designed to work in conjunction with a pre-trained RHVAE model, using its latent space structure to inform the geodesic computation. The complete model state and architecture are saved in <code>JLD2</code> format for reproducibility and future use.</p>
</section>
<section id="integration-with-rhvae" class="level3">
<h3 class="anchored" data-anchor-id="integration-with-rhvae">Integration with RHVAE</h3>
<p>This neural geodesic model complements the RHVAE architecture by:</p>
<ol type="1">
<li><p>Using the same latent space dimensionality.</p></li>
<li><p>Learning geodesics that respect the Riemannian metric of the RHVAE</p></li>
<li><p>Providing a parameterized way to interpolate between latent points</p></li>
</ol>
<!-- notes on latent space alignment -->
</section>
</section>
<section id="latent-space-alignment-via-procrustes-analysis" class="level2">
<h2 class="anchored" data-anchor-id="latent-space-alignment-via-procrustes-analysis">Latent Space Alignment via Procrustes Analysis</h2>
<p>When working with different dimensionality reduction techniques such as PCA, VAE, and RHVAE, the resulting latent spaces often have arbitrary orientations that make direct comparisons challenging. To facilitate meaningful comparisons between these latent representations and the ground truth phenotype space, we employed Procrustes analysis. This section details the mathematical foundations and implementation of our alignment procedure.</p>
<section id="mathematical-formulation" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-formulation">Mathematical Formulation</h3>
<p>Procrustes analysis finds an optimal rigid transformation (rotation, scaling, and potentially translation) that aligns one set of points with another while minimizing the sum of squared differences. Given two sets of points <span class="math inline">\(\underline{\underline{X}}\)</span> and <span class="math inline">\(\underline{\underline{Y}}\)</span>, where each column represents a point in <span class="math inline">\(\mathbb{R}^d\)</span>, the objective is to find a transformation of <span class="math inline">\(\underline{\underline{X}}\)</span> that best aligns with <span class="math inline">\(\underline{\underline{Y}}\)</span>.</p>
<p>The Procrustes problem can be formulated as:</p>
<p><span id="eq-procrustes"><span class="math display">\[
\min_{R, s} \|
    \underline{\underline{Y}} -
    s\underline{\underline{X}}\,\underline{\underline{R}}
\|_F^2
\tag{120}\]</span></span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\underline{\underline{R}}\)</span> is a <span class="math inline">\(d \times d\)</span> orthogonal rotation matrix (<span class="math inline">\(\underline{\underline{R}}^T\underline{\underline{R}} =
\underline{\underline{I}}\)</span>)</li>
<li><span class="math inline">\(s\)</span> is a scalar scaling factor</li>
<li><span class="math inline">\(\|\cdot\|_F\)</span> denotes the Frobenius norm</li>
</ul>
<p>If the data is centered (mean-subtracted), the transformation involves only rotation and scaling. The solution to this optimization problem is obtained through Singular Value Decomposition (SVD).</p>
</section>
<section id="algorithmic-procedure" class="level3">
<h3 class="anchored" data-anchor-id="algorithmic-procedure">Algorithmic Procedure</h3>
<p>Our implementation of Procrustes analysis follows these steps:</p>
<ol type="1">
<li><strong>Data Standardization</strong>: Each latent representation (PCA, VAE, RHVAE) and the ground truth phenotype data are standardized to have zero mean and unit standard deviation along each dimension:</li>
</ol>
<p><span id="eq-standardization"><span class="math display">\[
\hat{\underline{\underline{X}}} =
\frac{
    \underline{\underline{X}} -
    \underline{\mu}_X
}{
    \underline{\sigma}_X
}
\tag{121}\]</span></span></p>
<p>where <span class="math inline">\(\underline{\mu}_X\)</span> and <span class="math inline">\(\underline{\sigma}_X\)</span> are the mean and standard deviation vectors computed across all points in the respective space.</p>
<ol start="2" type="1">
<li><strong>Inner Product Computation</strong>: We compute the inner product matrix between the standardized ground truth phenotype data <span class="math inline">\(\hat{\underline{\underline{Y}}}\)</span> and each standardized latent representation <span class="math inline">\(\hat{\underline{\underline{X}}}\)</span>:</li>
</ol>
<p><span id="eq-inner-product"><span class="math display">\[
\underline{\underline{A}} =
\hat{\underline{\underline{Y}}}\,
\hat{\underline{\underline{X}}}^T
\tag{122}\]</span></span></p>
<ol start="3" type="1">
<li><strong>Singular Value Decomposition</strong>: We perform SVD on the inner product matrix:</li>
</ol>
<p><span id="eq-svd"><span class="math display">\[
\underline{\underline{A}} =
\underline{\underline{U}}\,
\underline{\underline{\Sigma}}\,
\underline{\underline{V}}^T
\tag{123}\]</span></span></p>
<p>where <span class="math inline">\(\underline{\underline{U}}\)</span> and <span class="math inline">\(\underline{\underline{V}}\)</span> are orthogonal matrices, and <span class="math inline">\(\underline{\underline{\Sigma}}\)</span> is a diagonal matrix of singular values.</p>
<ol start="4" type="1">
<li><strong>Rotation Matrix Computation</strong>: The optimal rotation matrix is given by:</li>
</ol>
<p><span id="eq-rotation"><span class="math display">\[
\underline{\underline{R}} =
\underline{\underline{U}}\,
\underline{\underline{V}}^T
\tag{124}\]</span></span></p>
<ol start="5" type="1">
<li><strong>Scaling Factor Computation</strong>: The optimal scaling factor is calculated as:</li>
</ol>
<p><span id="eq-scaling"><span class="math display">\[
s = \frac{
        \text{tr}(\underline{\underline{\Sigma}})
    }{
        \|\hat{\underline{\underline{X}}}\|_F^2
    }
\tag{125}\]</span></span></p>
<p>where <span class="math inline">\(\text{tr}(\underline{\underline{\Sigma}})\)</span> is the trace of <span class="math inline">\(\underline{\underline{\Sigma}}\)</span> (sum of singular values) and <span class="math inline">\(\|\hat{\underline{\underline{X}}}\|_F^2\)</span> is the squared Frobenius norm of <span class="math inline">\(\hat{\underline{\underline{X}}}\)</span>.</p>
<ol start="6" type="1">
<li><strong>Transformation Application</strong>: Each latent representation is transformed using the corresponding rotation matrix:</li>
</ol>
<p><span id="eq-alignment"><span class="math display">\[
\underline{\underline{X}}_{\text{aligned}} =
\underline{\underline{R}}\,\hat{\underline{\underline{X}}}
\tag{126}\]</span></span></p>
<ol start="7" type="1">
<li><strong>Similarity Metric Calculation</strong>: We compute a correlation-like measure to quantify the goodness-of-fit between the aligned spaces:</li>
</ol>
<p><span id="eq-correlation"><span class="math display">\[
\rho = \frac{
    \text{tr}(\underline{\underline{\Sigma}})
}{
    \|\hat{\underline{\underline{X}}}\|_F\|\hat{\underline{\underline{Y}}}\|_F
}
\tag{127}\]</span></span></p>
<p>This metric ranges from 0 (no similarity) to 1 (perfect alignment).</p>
</section>
<section id="significance-for-comparative-analysis" class="level3">
<h3 class="anchored" data-anchor-id="significance-for-comparative-analysis">Significance for Comparative Analysis</h3>
<p>The Procrustes alignment enables several critical aspects of our analysis:</p>
<ol type="1">
<li><p><strong>Direct Visual Comparison</strong>: By aligning all latent spaces to the same orientation as the ground truth phenotype space, we can directly visualize and compare the structural preservation properties of each dimensionality reduction technique.</p></li>
<li><p><strong>Quantitative Assessment</strong>: The correlation metric <span class="math inline">\(\rho\)</span> provides a quantitative measure of how well each latent representation preserves the geometric structure of the ground truth phenotype space.</p></li>
<li><p><strong>Geometric Structure Preservation</strong>: The alignment allows us to assess how features such as local neighborhoods, relative distances, and global structure are preserved in each latent representation.</p></li>
<li><p><strong>Trajectory Analysis</strong>: For evolutionary trajectories, the alignment enables comparison of path lengths, directional changes, and convergence patterns across different representations.</p></li>
</ol>
<p>This alignment procedure forms the foundation for the comparative analysis presented in the main text, where we evaluate the effectiveness of different dimensionality reduction techniques in capturing the underlying structure of the phenotype space.</p>
<!-- notes on Metropolis-Kimura -->
</section>
</section>
<section id="sec-metropolis-kimura" class="level2">
<h2 class="anchored" data-anchor-id="sec-metropolis-kimura">Alternative Metropolis-Kimura Evolutionary Dynamics</h2>
<p>In the main text, we described a simple algorithm for evolving a population as a biased random walk on a phenotypic space controlled by both a fitness function and a genotype-phenotype density function. Here, we propose an alternative algorithm that combines elements of the Metropolis-Hastings algorithm with Kimura’s classical population genetics theory.</p>
<section id="metropolis-kimura-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="metropolis-kimura-algorithm">Metropolis-Kimura Algorithm</h3>
<p>In <a href="#sec-metropolis" class="quarto-xref">Section&nbsp;0.2</a>, we described a simple framework with a fitness function and a genotype-phenotype density function. Here, we use the same framework but introduce a biologically motivated algorithm for the evolutionary dynamics that combines elements of the Metropolis-Hastings algorithm with Kimura’s classical population genetics theory. This approach implements a two-step process that separately models:</p>
<ol type="1">
<li>The probability of mutation occurring (mutation accessibility) as governed by the genotype-phenotype density.</li>
<li>The probability of fixation within a population (selection) determined by the fitness effect of such mutation and the effective population size.</li>
</ol>
<section id="biological-motivation" class="level4">
<h4 class="anchored" data-anchor-id="biological-motivation">Biological Motivation</h4>
<p>In natural populations, evolution proceeds through two distinct processes:</p>
<ol type="1">
<li><p><strong>Mutation</strong>: New variants arise through random genetic changes. The probability of specific mutations depends on molecular mechanisms and constraints of the genotype-phenotype map.</p></li>
<li><p><strong>Fixation</strong>: Once a mutation occurs, it must spread through the population to become fixed. The probability of fixation depends on the selection coefficient (fitness advantage) and the effective population size.</p></li>
</ol>
</section>
<section id="mathematical-framework" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-framework">Mathematical Framework</h4>
<section id="mutation-probability" class="level5">
<h5 class="anchored" data-anchor-id="mutation-probability">Mutation Probability</h5>
<p>The probability of a mutation from phenotype <span class="math inline">\(\underline{x}\)</span> to <span class="math inline">\(\underline{x}'\)</span> is modeled using a Metropolis-like criterion based on the genotype-phenotype density:</p>
<p><span id="eq-mutation-probability"><span class="math display">\[
\pi_{\text{mut}}(\underline{x} \to \underline{x}') =
\min\left(1, \left(\frac{GP(\underline{x}')}{GP(\underline{x})}\right)^{\beta}\right)
\tag{128}\]</span></span></p>
<p>Here, <span class="math inline">\(GP(\underline{x})\)</span> represents the genotype-phenotype density at phenotype <span class="math inline">\(\underline{x}\)</span>, and <span class="math inline">\(\beta\)</span> is a parameter controlling the strength of mutational constraints. The higher <span class="math inline">\(\beta\)</span> is, the less likely a mutation going downhill in genotype-phenotype density is to be accepted.</p>
</section>
<section id="kimuras-fixation-probability" class="level5">
<h5 class="anchored" data-anchor-id="kimuras-fixation-probability">Kimura’s Fixation Probability</h5>
<p>Once a mutation occurs, its probability of fixation in a population of effective size <span class="math inline">\(N\)</span> is given by Kimura’s formula</p>
<p><span id="eq-kimura-fixation-probability"><span class="math display">\[
\pi_{\text{fix}}(\underline{x} \to \underline{x}') =
\frac{1 - e^{-2s}}{1 - e^{-2Ns}}
\tag{129}\]</span></span></p>
<p>where <span class="math inline">\(s\)</span> is the selection coefficient. We compute this selection coefficient as the difference in fitness between the new and old phenotype divided by the fitness of the old phenotype:</p>
<p><span id="eq-selection-coefficient"><span class="math display">\[
s = \frac{F_E(\underline{x}') - F_E(\underline{x})}{F_E(\underline{x})}
\tag{130}\]</span></span></p>
<p>This equation captures a fundamental result from population genetics: beneficial mutations (<span class="math inline">\(s &gt; 0\)</span>) have a higher probability of fixation, with the probability approaching <span class="math inline">\(2s\)</span> for small positive selection coefficients and approaching 1 for large positive selection coefficients. Deleterious mutations (<span class="math inline">\(s &lt; 0\)</span>) have an exponentially decreasing probability of fixation as population size increases.</p>
</section>
</section>
<section id="overall-acceptance-probability" class="level4">
<h4 class="anchored" data-anchor-id="overall-acceptance-probability">Overall Acceptance Probability</h4>
<p>The overall acceptance probability—defined as the probability of a proposed step <span class="math inline">\(\underline{x} \rightarrow \underline{x}'\)</span>—is the product of the mutation probability and the fixation probability:</p>
<p><span id="eq-overall-acceptance-probability"><span class="math display">\[
\pi_{\text{accept}}(\underline{x} \to \underline{x}') =
\pi_{\text{mut}}(\underline{x} \to \underline{x}') \cdot
\pi_{\text{fix}}(\underline{x} \to \underline{x}')
\tag{131}\]</span></span></p>
<p>This two-step process better reflects the biological reality of evolution, where both mutational accessibility and selection contribute to evolutionary trajectories.</p>
</section>
</section>
<section id="effect-of-population-size" class="level3">
<h3 class="anchored" data-anchor-id="effect-of-population-size">Effect of Population Size</h3>
<p><a href="#fig-metropolis-population-effect" class="quarto-xref">Figure&nbsp;14</a> shows the effect of population size parameter <span class="math inline">\(N\)</span> influences evolutionary trajectories in our Metropolis-Kimura framework. Through a series of panels showing both phenotypic trajectories (upper) and fitness dynamics (lower), we can observe how different population sizes affect the balance between exploration and exploitation in phenotypic space.</p>
<p>At small population sizes (left panels), the evolutionary trajectories exhibit highly stochastic behavior, with populations taking meandering paths through the phenotypic landscape. This represents a regime where genetic drift plays a dominant role. As population size increases (moving right), the trajectories become increasingly deterministic, with populations taking more direct paths toward fitness peaks, while avoiding genotype-to-phenotype density valleys. This transition reflects a shift from drift-dominated to selection-dominated dynamics, where populations more strictly follow the local fitness gradient. The fitness trajectories in the lower panels further reinforce this pattern, showing more erratic fitness fluctuations at small population sizes and more monotonic increases at large population sizes, consistent with stronger selective pressures.</p>
<div id="fig-metropolis-population-effect" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-metropolis-population-effect-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig/supplementary/figSI_sim_population_effect.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-metropolis-population-effect-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: <strong>Effect of population size on adaptive dynamics</strong>. Upper panels: Population trajectories on a landscape with a single fitness peak and four genotype-phenotype density peaks. Black dashed lines indicate fitness landscape contours, while white solid lines show genotype-phenotype density contours. Colored lines represent independent population trajectories, with initial positions (crosses) and final positions (triangles). Lower panels: Temporal evolution of population fitness. Panels from left to right show increasing values of population size (<span class="math inline">\(N\)</span>), demonstrating increasingly deterministic adaptive trajectories.
</figcaption>
</figure>
</div>
</section>
<section id="main-text-results" class="level3">
<h3 class="anchored" data-anchor-id="main-text-results">Main Text Results</h3>
<p>Having developed this alternative algorithm for the population dynamics, we reproduce the results from the main text using this new algorithm. In particular, Figure 2, 4, and 5 in the main text used synthetic data generated by the algorithm described in <a href="#sec-metropolis" class="quarto-xref">Section&nbsp;0.2</a>. We now show that the same results can be obtained using the Metropolis-Kimura algorithm.</p>
<section id="figure-2-main-text" class="level4">
<h4 class="anchored" data-anchor-id="figure-2-main-text">Figure 2 Main Text</h4>
<div id="fig-02-kimura" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-02-kimura-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig/supplementary/figSI_02_kimura.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-02-kimura-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: <strong>Evolutionary dynamics on phenotype space with Metropolis-Kimura dynamics</strong>. (A) Top: Metropolis-like evolutionary dynamics on phenotype space. Each line represents the trajectory of a lineage as it evolves over time, with crosses and triangles denoting the initial phenotypic coordinate of a few selected lineages. Note that trajectories tend to move towards higher fitness values, avoiding low genotype-to-phenotype density regions. Bottom: Fitness over time of the same trajectories shown above. (B) The same trajectories shown in (A) overlaid on different fitness maps determined by different environments. Although the phenotypic coordinates of the genotypes remain the same (top panels), the resulting fitness readouts change as the topography of the environment-dependent fitness landscape changes (bottom panels).
</figcaption>
</figure>
</div>
</section>
<section id="figure-4-main-text" class="level4">
<h4 class="anchored" data-anchor-id="figure-4-main-text">Figure 4 Main Text</h4>
<div id="fig-04-kimura" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-04-kimura-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig/supplementary/figSI_04_kimura.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-04-kimura-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: <strong>Geometry-informed latent space captures phenotypic structure and improves reconstruction accuracy</strong>. (A) Latent space coordinates of simulated fitness profiles colored by lineage, after alignment with the ground truth phenotype space using Procrustes analysis. The RHVAE better preserves phenotypic relationships than PCA or vanilla VAE, as evidenced by the highlighted points (diamond markers). (B) Comparison of all pairwise Euclidean distances between genotypes in the ground truth phenotypic space versus the different latent spaces, showing RHVAE better preserves the original distances. Given the large number of data points and the even larger number of pairwise comparisons, the data are shown as a smear rather than single points. (C) Reconstruction error (MSE) comparison showing 2D-RHVAE achieves accuracy comparable to a 10-dimensional PCA model. (D) Metric volume visualization of the RHVAE latent space (left) and with projected data points (right), revealing regions of high curvature that correspond to areas of low genotype-phenotype density in the simulation.
</figcaption>
</figure>
</div>
</section>
<section id="figure-5-main-text" class="level4">
<h4 class="anchored" data-anchor-id="figure-5-main-text">Figure 5 Main Text</h4>
<div id="fig-05-kimura" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-05-kimura-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig/supplementary/figSI_05_kimura.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-05-kimura-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: <strong>Geometry-informed latent space enables accurate reconstruction of complex fitness landscapes</strong>. Comparison of ground truth and reconstructed fitness landscapes across multiple environments. The first row shows examples of the original simulated fitness landscapes used to generate the data. Subsequent rows show reconstructions using the RHVAE (second row), VAE (third row), and PCA (fourth row) models. The RHVAE accurately captures the underlying topography, including the number and relative positions of fitness peaks. While the VAE captures general landscape shapes, it lacks a geometric metric to define prediction reliability regions. PCA fails to represent the nonlinear structure of the landscapes, demonstrating the limitations of linear dimensionality reduction for complex fitness landscape reconstruction.
</figcaption>
</figure>
</div>



</section>
</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list">
<div id="ref-iwasawa2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">J. Iwasawa, T. Maeda, A. Shibai, H. Kotani, M. Kawada, and C. Furusawa, <span>“Analysis of the evolution of resistance to multiple antibiotics enables prediction of the <span>Escherichia</span> coli phenotype-based fitness landscape,”</span> <em>PLOS Biology</em>, vol. 20, no. 12, p. e3001920, Dec. 2022, doi: <a href="https://doi.org/10.1371/journal.pbio.3001920">10.1371/journal.pbio.3001920</a>.</div>
</div>
<div id="ref-ge2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">H. Ge, K. Xu, and Z. Ghahramani, <span>“Turing: <span>A Language</span> for <span>Flexible Probabilistic Inference</span>,”</span> in <em>Proceedings of the <span>Twenty-First International Conference</span> on <span>Artificial Intelligence</span> and <span>Statistics</span></em>, PMLR, Mar. 2018, pp. 1682–1690.</div>
</div>
<div id="ref-chadebec2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">C. Chadebec, C. Mantoux, and S. Allassonnière, <span>“Geometry-<span>Aware Hamiltonian Variational Auto-Encoder</span>.”</span> arXiv, Oct. 2020. doi: <a href="https://doi.org/10.48550/arXiv.2010.11518">10.48550/arXiv.2010.11518</a>.</div>
</div>
<div id="ref-kingma2014a" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">D. P. Kingma and M. Welling, <span>“Auto-<span>Encoding Variational Bayes</span>.”</span> arXiv, May 2014. doi: <a href="https://doi.org/10.48550/arXiv.1312.6114">10.48550/arXiv.1312.6114</a>.</div>
</div>
<div id="ref-kingma2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">D. P. Kingma and M. Welling, <span>“An <span>Introduction</span> to <span>Variational Autoencoders</span>,”</span> <em>Foundations and Trends<span></span> in Machine Learning</em>, vol. 12, no. 4, pp. 307–392, 2019, doi: <a href="https://doi.org/10.1561/2200000056">10.1561/2200000056</a>.</div>
</div>
<div id="ref-salimans2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">T. Salimans, D. P. Kingma, and M. Welling, <span>“Markov <span>Chain Monte Carlo</span> and <span>Variational Inference</span>: <span>Bridging</span> the <span>Gap</span>.”</span> arXiv, May 2015. Accessed: Nov. 28, 2023. [Online]. Available: <a href="https://arxiv.org/abs/1410.6460">https://arxiv.org/abs/1410.6460</a></div>
</div>
<div id="ref-rezende2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">D. J. Rezende and S. Mohamed, <span>“Variational <span>Inference</span> with <span>Normalizing Flows</span>.”</span> arXiv, Jun. 2016. Accessed: Nov. 28, 2023. [Online]. Available: <a href="https://arxiv.org/abs/1505.05770">https://arxiv.org/abs/1505.05770</a></div>
</div>
<div id="ref-betancourt2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">M. Betancourt, <span>“A <span>Conceptual Introduction</span> to <span>Hamiltonian Monte Carlo</span>,”</span> <em>ArXiv</em>, 2017, Available: <a href="https://arxiv.org/abs/1701.02434">https://arxiv.org/abs/1701.02434</a></div>
</div>
<div id="ref-caterini2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">A. L. Caterini, A. Doucet, and D. Sejdinovic, <span>“Hamiltonian <span>Variational Auto-Encoder</span>,”</span> p. 11, 2018, doi: <a href="https://doi.org/10.48550/arXiv.1805.11328">10.48550/arXiv.1805.11328</a>.</div>
</div>
<div id="ref-neal" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">R. M. Neal, <span>“Annealed importance sampling.”</span></div>
</div>
<div id="ref-kinsler2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">G. Kinsler, K. Geiler-Samerotte, and D. A. Petrov, <span>“Fitness variation across subtle environmental perturbations reveals local modularity and global pleiotropy of adaptation,”</span> <em>eLife</em>, vol. 9, pp. 1–52, Dec. 2020, doi: <a href="https://doi.org/10.7554/eLife.61271">10.7554/eLife.61271</a>.</div>
</div>
<div id="ref-chen2018a" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">N. Chen, A. Klushyn, R. Kurle, X. Jiang, J. Bayer, and P. Smagt, <span>“Metrics for <span>Deep Generative Models</span>,”</span> in <em>Proceedings of the <span>Twenty-First International Conference</span> on <span>Artificial Intelligence</span> and <span>Statistics</span></em>, PMLR, Mar. 2018, pp. 1540–1550.</div>
</div>
<div id="ref-innes2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">M. Innes, <span>“Flux: <span>Elegant</span> machine learning with <span>Julia</span>,”</span> <em>Journal of Open Source Software</em>, vol. 3, no. 25, p. 602, May 2018, doi: <a href="https://doi.org/10.21105/joss.00602">10.21105/joss.00602</a>.</div>
</div>
<div id="ref-razo-mejia2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">M. Razo-Mejia, <span>“<span>AutoEncoderToolkit</span>.jl: <span>A Julia</span> package for training (<span>Variational</span>) <span>Autoencoders</span>,”</span> Jul. 2024, doi: <a href="https://doi.org/10.21105/joss.06794">10.21105/joss.06794</a>.</div>
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>