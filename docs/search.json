[
  {
    "objectID": "code/metropolis_kimura_evolution.html",
    "href": "code/metropolis_kimura_evolution.html",
    "title": "Metropolis-Kimura Evolutionary Dynamics",
    "section": "",
    "text": "# Import project package\nimport Antibiotic\n\n# Import basic math libraries\nimport StatsBase\nimport LinearAlgebra\nimport Random\nimport Distributions\n\n# Load CairoMakie for plotting\nusing CairoMakie\nimport ColorSchemes\n\n# Activate backend\nCairoMakie.activate!()\n\n# Set custom plotting style\nAntibiotic.viz.theme_makie!()",
    "crumbs": [
      "Notebooks",
      "Metropolis-Kimura Evolutionary Dynamics"
    ]
  },
  {
    "objectID": "code/metropolis_kimura_evolution.html#evolutionary-dynamics",
    "href": "code/metropolis_kimura_evolution.html#evolutionary-dynamics",
    "title": "Metropolis-Kimura Evolutionary Dynamics",
    "section": "Evolutionary dynamics",
    "text": "Evolutionary dynamics\n\nMetropolis-Kimura Algorithm in Evolutionary Dynamics\nWe now introduce a biologically motivated algorithm that combines elements of the Metropolis-Hastings algorithm with Motoo Kimura’s population genetics theory. This approach implements a two-step process that separately models:\n\nThe probability of mutation occurring (mutation accessibility) as governed by the genotype-phenotype density.\nThe probability of fixation within a population (selection) determined by the fitness effect of such mutation and the effective population size.\n\n\n\nBiological Motivation\nIn natural populations, evolution proceeds through two distinct processes:\n\nMutation: New variants arise through random genetic changes. The probability of specific mutations depends on molecular mechanisms and constraints of the genotype-phenotype map.\nFixation: Once a mutation occurs, it must spread through the population to become fixed. The probability of fixation depends on the selection coefficient (fitness advantage) and the effective population size.\n\n\n\nMathematical Framework\n\nMutation Probability\nThe probability of a mutation from phenotype \\(\\underline{x}\\) to \\(\\underline{x}'\\) is modeled using a Metropolis-like criterion based on the genotype-phenotype density:\n\\[\n\\pi_{\\text{mut}}(\\underline{x} \\to \\underline{x}') =\n\\min\\left(1, \\frac{GP(\\underline{x}')}{GP(\\underline{x})}\\right)^{\\beta}\n\\tag{5}\\]\nHere, \\(GP(\\underline{x})\\) represents the genotype-phenotype density at phenotype \\(\\underline{x}\\), and \\(\\beta\\) is a parameter controlling the strength of mutational constraints. The higher \\(\\beta\\) is, the less likely a mutation going downhill in genotype-phenotype density is to be accepted.\n\n\nKimura’s Fixation Probability\nOnce a mutation occurs, its probability of fixation in a population of effective size \\(N\\) is given by Kimura’s formula\n\\[\n\\pi_{\\text{fix}}(\\underline{x} \\to \\underline{x}') =\n\\frac{1 - e^{-2s}}{1 - e^{-2Ns}}\n\\tag{6}\\]\nwhere \\(s\\) is the selection coefficient. We compute this selection coefficient as the difference in fitness between the new and old phenotype divided by the fitness of the old phenotype, i.e.,\n\\[\ns = \\frac{F_E(\\underline{x}') - F_E(\\underline{x})}{F_E(\\underline{x})}\n\\tag{7}\\]\nThis equation captures a fundamental result from population genetics: beneficial mutations (\\(s &gt; 0\\)) have a higher probability of fixation, with the probability approaching \\(2s\\) for small positive selection coefficients and approaching 1 for large positive selection coefficients. Deleterious mutations (\\(s &lt; 0\\)) have an exponentially decreasing probability of fixation as population size increases.\n\n\nOverall Acceptance Probability of Mutation\nThe overall acceptance probability—defined as the probability of a proposed step \\(x \\rightarrow x'\\)—is the product of the mutation probability and the fixation probability\n\\[\n\\pi_{\\text{accept}}(\\underline{x} \\to \\underline{x}') =\n\\pi_{\\text{mut}}(\\underline{x} \\to \\underline{x}') \\cdot\n\\pi_{\\text{fix}}(\\underline{x} \\to \\underline{x}')\n\\tag{8}\\]\nThis two-step process better reflects the biological reality of evolution, where both mutational accessibility and selection contribute to evolutionary trajectories.\n\n\n\nNumerical Implementation\nLet’s now implement the Metropolis-Kimura algorithm in code.\n\n@doc raw\"\"\"\n    evo_metropolis_kimura(x0, fitness_peaks, gen_peaks, N, β, µ, n_steps)\n\nPerform evolutionary algorithm to simulate phenotypic evolution using a\nMetropolis-Hastings-like mutation probability and Kimura's fixation probability.\n\n# Arguments\n- `x0::AbstractVector`: Initial phenotype vector.\n- `fitness_peaks::Union{GaussianPeak, Vector{GaussianPeak}}`: Fitness landscape\n  defined by one or more Gaussian peaks.\n- `gen_peaks::Union{GaussianPeak, Vector{GaussianPeak}}`: Genotype-phenotype\n  density landscape defined by one or more Gaussian peaks.\n- `N::Real`: Effective population size.\n- `β::Real`: Parameter controlling the strength of mutational constraints.\n- `µ::Real`: Mutation step size standard deviation.\n- `n_steps::Int`: Number of steps to simulate.\n\n# Returns\n- `Matrix{Float64}`: Matrix of phenotypes, where each column represents a step\n  in the simulation.\n\n# Description\nThis function implements a two-step evolutionary algorithm to simulate\nphenotypic evolution in a landscape defined by fitness and genotype-phenotype\ndensity. It uses the following steps:\n\n1. Initialize the phenotype trajectory with the given starting point.\n2. For each step: \n   a. Propose a new phenotype by adding Gaussian noise. \n   b. Calculate the fitness and genotype-phenotype density values for the new \n      phenotype.\n   c. Compute the mutation probability P_mut = min(1, (GP(x_new)/GP(x))^β)\n   d. Compute the selection coefficient s = (F_new - F)/F\n   e. Calculate fixation probability using Kimura's equation:\n      P_fix = (1 - exp(-2s))/(1 - exp(-2Ns))\n   f. Accept or reject based on P_accept = P_mut * P_fix\n3. Return the complete phenotype trajectory.\n\"\"\"\nfunction evo_metropolis_kimura(\n    x0::AbstractVector,\n    fitness_peaks::Union{GaussianPeak,Vector{GaussianPeak}},\n    gen_peaks::Union{GaussianPeak,Vector{GaussianPeak}},\n    N::Real,\n    β::Real,\n    µ::Real,\n    n_steps::Int,\n)\n    # Initialize array to hold phenotypes\n    x = Matrix{Float64}(undef, length(x0), n_steps + 1)\n    # Set initial phenotype\n    x[:, 1] = x0\n\n    # Compute fitness and genotype-phenotype density at initial phenotype\n    fitness_val = fitness(fitness_peaks, x0)\n    gen_val = genotype_phenotype_density(gen_peaks, x0)\n\n    # Loop over steps\n    for t in 1:n_steps\n        # Propose new phenotype\n        x_new = x[:, t] + µ * randn(length(x0))\n\n        # Calculate fitness and genotype-phenotype density\n        fitness_val_new = fitness(fitness_peaks, x_new)\n        gen_val_new = genotype_phenotype_density(gen_peaks, x_new)\n\n        # Compute selection coefficient\n        s = (fitness_val_new - fitness_val) /\n            (fitness_val + eps(eltype(fitness_val)))\n\n        # Compute mutation probability\n        P_mut = min(1, (gen_val_new / gen_val)^β)\n\n        # Compute fixation probability using Kimura's equation\n        # Use approximation when s is very small for numerical stability\n        if abs(s) &lt; 1e-10\n            P_fix = 1 / N\n        elseif abs(s) &lt; 0.1\n            # For small s, use approximation\n            P_fix = (1 - exp(-2 * s)) / (1 - exp(-2 * N * s))\n            # Check for potential numerical issues\n            if !isfinite(P_fix) || P_fix &lt; 0\n                P_fix = s &gt; 0 ? 2 * s : 1 / N\n            end\n        else\n            # Otherwise use full Kimura equation\n            P_fix = (1 - exp(-2 * s)) / (1 - exp(-2 * N * s))\n        end\n\n        # Compute acceptance probability\n        P_accept = P_mut * P_fix\n\n        # Accept or reject proposal\n        if rand() &lt; P_accept\n            # Accept proposal\n            x[:, t+1] = x_new\n            # Update fitness and genotype-phenotype density\n            fitness_val = fitness_val_new\n            gen_val = gen_val_new\n        else\n            # Reject proposal\n            x[:, t+1] = x[:, t]\n        end\n    end # for\n    return x\nend # function\n\nLet’s test this function with the previously defined fitness and genotype-phenotype density functions.\n\nRandom.seed!(42)\n\n# Define peak parameters\namplitude = 5.0\nmean = [0.0, 0.0]\ncovariance = [3.0 0.0; 0.0 3.0]\n# Create peak\nfit_peak = GaussianPeak(amplitude, mean, covariance)\n# Define peak parameters\namplitude = 1.0\nmeans = [\n    [-1.5, -1.5],\n    [1.5, -1.5],\n    [1.5, 1.5],\n    [-1.5, 1.5],\n]\ncovariance = [0.45 0.0; 0.0 0.45]\n\n# Create peak\nmut_peaks = GaussianPeak.(Ref(amplitude), means, Ref(covariance))\n\n# Set initial phenotype\nx0 = [-2.5, -2.5]\n\n# Set parameters\nN = 10 # Effective population size \nβ = 10.0    # Mutational constraint parameter\nµ = 0.1    # Mutation step size\nn_steps = 3000\n\n# Run Metropolis-Kimura algorithm\nx_traj = evo_metropolis_kimura(x0, fit_peak, mut_peaks, N, β, µ, n_steps)\n\nLet’s plot the trajectory of the phenotypes in phenotype space.\n\nRandom.seed!(42)\n\n# Define range of phenotypes to evaluate\nx = range(-4, 4, length=100)\ny = range(-4, 4, length=100)\n\n# Create meshgrid\nF = fitness.(Ref(fit_peak), [[x, y] for x in x, y in y])\nM = genotype_phenotype_density.(Ref(mut_peaks), [[x, y] for x in x, y in y])\n\n# Initialize figure\nfig = Figure(size=(600, 300))\n\n# Add axis for trajectory in fitness landscape\nax1 = Axis(\n    fig[1, 1],\n    xlabel=\"phenotype 1\",\n    ylabel=\"phenotype 2\",\n    aspect=AxisAspect(1),\n    title=\"Fitness landscape\",\n    yticklabelsvisible=false,\n    xticklabelsvisible=false,\n)\n# Add axis for trajectory in genotype-phenotype density\nax2 = Axis(\n    fig[1, 2],\n    xlabel=\"phenotype 1\",\n    ylabel=\"phenotype 2\",\n    aspect=AxisAspect(1),\n    title=\"Genotype-phenotype\\nDensity\",\n    yticklabelsvisible=false,\n    xticklabelsvisible=false,\n)\n\n# Plot a heatmap of the fitness landscape\nheatmap!(ax1, x, y, F, colormap=:algae)\n# Plot heatmap of genotype-phenotype density\nheatmap!(ax2, x, y, M, colormap=Reverse(ColorSchemes.Purples_9))\n\n# Plot contour plot\ncontour!(ax1, x, y, F, color=:white)\ncontour!(ax2, x, y, M, color=:white)\n\n# Plot trajectory\nscatterlines!.(\n    [ax1, ax2],\n    Ref(x_traj[1, :]),\n    Ref(x_traj[2, :]),\n    color=ColorSchemes.seaborn_colorblind[4],\n    markersize=3\n)\n\nfig\n\n\n\n\nWe can see that the trajectory climbs the fitness peak, but it does so by avoiding regions of low genotype-phenotype density.\nLet’s plot the fitness and the genotype-phenotype density for the phenotypes in the trajectory over time.\n\n# Initialize arrays to hold fitness and genotype-phenotype density\nF_traj = Vector{Float64}(undef, n_steps + 1)\nM_traj = Vector{Float64}(undef, n_steps + 1)\n\n# Loop over steps\nfor (i, x) in enumerate(eachcol(x_traj))\n    F_traj[i] = fitness(fit_peak, x)\n    M_traj[i] = genotype_phenotype_density(mut_peaks, x)\nend\n\n# Initialize figure\nfig = Figure(size=(600, 300))\n\n# Add axis for fitness\nax1 = Axis(\n    fig[1, 1],\n    xlabel=\"step\",\n    ylabel=\"fitness\",\n    yticklabelsvisible=false,\n)\n\n# Add axis for genotype-phenotype density\nax2 = Axis(\n    fig[1, 2],\n    xlabel=\"step\",\n    ylabel=\"genotype-phenotype density\",\n    yticklabelsvisible=false,\n)\n\n# Plot fitness\nlines!(ax1, F_traj, color=ColorSchemes.seaborn_colorblind[1])\n\n# Plot genotype-phenotype density\nlines!(ax2, M_traj, color=ColorSchemes.seaborn_colorblind[2])\n\nfig\n\n\n\n\nLet’s now repeat the simulation multiple times and plot all different trajectories.\n\nRandom.seed!(42)\n\n# Define number of simulations\nn_sim = 5\n\n# Run simulations\nx_traj_list = [\n    evo_metropolis_kimura(x0, fit_peak, mut_peaks, N, β, µ, n_steps)\n    for _ in 1:n_sim\n]\n\n# Initialize figure\nfig = Figure(size=(600, 300))\n\n# Add axis for fitness landscape\nax1 = Axis(\n    fig[1, 1],\n    xlabel=\"phenotype 1\",\n    ylabel=\"phenotype 2\",\n    aspect=AxisAspect(1),\n    title=\"Fitness\",\n    yticklabelsvisible=false,\n    xticklabelsvisible=false,\n)\n# Add axis for genotype-phenotype density\nax2 = Axis(\n    fig[1, 2],\n    xlabel=\"phenotype 1\",\n    ylabel=\"phenotype 2\",\n    aspect=AxisAspect(1),\n    title=\"Genotype-phenotype\\nDensity\",\n    yticklabelsvisible=false,\n    xticklabelsvisible=false,\n)\n\n# Plot fitness landscape\nheatmap!(ax1, x, y, F, colormap=:algae)\n# Plot heatmap of genotype-phenotype density\nheatmap!(ax2, x, y, M, colormap=Reverse(ColorSchemes.Purples_9))\n\n# Plot contour plot\ncontour!(ax1, x, y, F, color=:white)\ncontour!(ax2, x, y, M, color=:white)\n\n# Loop over simulations\nfor (i, x_traj) in enumerate(x_traj_list)\n    # Plot trajectory\n    scatterlines!.(\n        [ax1, ax2],\n        Ref(x_traj[1, :]),\n        Ref(x_traj[2, :]),\n        color=ColorSchemes.seaborn_colorblind[i],\n        markersize=3\n    )\nend\n\n# Set limits\nxlims!(ax1, -4, 4)\nylims!(ax1, -4, 4)\nxlims!(ax2, -4, 4)\nylims!(ax2, -4, 4)\n\nfig",
    "crumbs": [
      "Notebooks",
      "Metropolis-Kimura Evolutionary Dynamics"
    ]
  },
  {
    "objectID": "code/metropolis_kimura_evolution.html#conclusion",
    "href": "code/metropolis_kimura_evolution.html#conclusion",
    "title": "Metropolis-Kimura Evolutionary Dynamics",
    "section": "Conclusion",
    "text": "Conclusion\nIn this notebook, we developed a biologically-motivated model for evolutionary dynamics that combines elements of the Metropolis-Hastings algorithm with classic population genetics theory. This model implements a two-step process that separately accounts for:\n\nThe probability of mutation occurring (based on genotype-phenotype density)\nThe probability of fixation within a population (based on Kimura’s fixation probability)\n\nBy separating these processes, our Metropolis-Kimura algorithm provides a plausible model of evolutionary dynamics, where mutational constraints and selection pressures interact to shape evolutionary trajectories. This approach allows us to explore how both the accessibility of phenotypic space and the fitness landscape influence the paths taken by evolving populations.",
    "crumbs": [
      "Notebooks",
      "Metropolis-Kimura Evolutionary Dynamics"
    ]
  },
  {
    "objectID": "code/rhvae_sim.html",
    "href": "code/rhvae_sim.html",
    "title": "Training an RHVAE on synthetic data",
    "section": "",
    "text": "(c) This work is licensed under a Creative Commons Attribution License CC-BY 4.0. All code contained herein is licensed under an MIT license.",
    "crumbs": [
      "Notebooks",
      "Training an RHVAE on synthetic data"
    ]
  },
  {
    "objectID": "code/rhvae_sim.html#introduction-to-rhvaes",
    "href": "code/rhvae_sim.html#introduction-to-rhvaes",
    "title": "Training an RHVAE on synthetic data",
    "section": "Introduction to RHVAEs",
    "text": "Introduction to RHVAEs\nVariational Autoencoders (VAEs) are generative models that learn to encode high-dimensional data into a lower-dimensional latent space and then decode it back. A standard VAE consists of an encoder network that maps input data to a distribution in latent space, and a decoder network that reconstructs the input from samples from this distribution.\nThe Riemannian Hamiltonian VAE (RHVAE) extends this framework by modeling the latent space as a Riemannian manifold, learning not only the encoding/decoding functions but also the geometric structure of the latent space. This is achieved by:\n\nLearning a position-dependent metric tensor that captures how distances in latent space relate to distances in data space.\nUsing Hamiltonian dynamics to move in latent space in a geometry-aware manner.\n\nThe RHVAE is particularly useful for our application because: - It captures nonlinear relationships between fitness profiles and underlying phenotypes. - It provides meaningful distances in latent space via the learned metric tensor. - It offers a principled way to interpolate between points in latent space.",
    "crumbs": [
      "Notebooks",
      "Training an RHVAE on synthetic data"
    ]
  },
  {
    "objectID": "code/rhvae_sim.html#setup-environment",
    "href": "code/rhvae_sim.html#setup-environment",
    "title": "Training an RHVAE on synthetic data",
    "section": "Setup Environment",
    "text": "Setup Environment\nFirst, let’s import the necessary packages for our implementation.\n\n# Import project package\nimport Antibiotic\nimport Antibiotic.mh as mh # Metropolis-Hastings dynamics module\n\n# Import AutoEncoderToolkit to train VAEs\nimport AutoEncoderToolkit as AET\n\n# Import packages for manipulating results\nimport DimensionalData as DD\nimport DataFrames as DF\nimport Glob\n\n# Import ML libraries\nimport Flux\n\n# Import CUDA (if available) to train using GPU\n# import CUDA\n\n# Import library to save models\nimport JLD2\n\n# Import libraries for data handling\nimport IterTools\nimport StatsBase\nimport Random\n\n# Load Plotting packages\nusing CairoMakie\nimport ColorSchemes\nimport Colors\n\n# Activate backend\nCairoMakie.activate!()\n\n# Set plotting style\nAntibiotic.viz.theme_makie!()\n\n# Set random seed for reproducibility\nRandom.seed!(42)",
    "crumbs": [
      "Notebooks",
      "Training an RHVAE on synthetic data"
    ]
  },
  {
    "objectID": "code/rhvae_sim.html#defining-directories",
    "href": "code/rhvae_sim.html#defining-directories",
    "title": "Training an RHVAE on synthetic data",
    "section": "Defining Directories",
    "text": "Defining Directories\nWe’ll set up the directories where we’ll load data from and save our model and results to.\n\n# Defining directories...\n\n# Define output directory\nout_dir = \"./sim_metropolis_dynamics\"\n\n# Define model state directory\nstate_dir = \"$(out_dir)/rhvae_model_state\"",
    "crumbs": [
      "Notebooks",
      "Training an RHVAE on synthetic data"
    ]
  },
  {
    "objectID": "code/rhvae_sim.html#model-architecture-and-hyperparameters",
    "href": "code/rhvae_sim.html#model-architecture-and-hyperparameters",
    "title": "Training an RHVAE on synthetic data",
    "section": "Model Architecture and Hyperparameters",
    "text": "Model Architecture and Hyperparameters\nThe RHVAE architecture consists of three main components: 1. Encoder: Maps fitness profiles to a distribution in latent space 2. Decoder: Reconstructs fitness profiles from latent space coordinates 3. Metric Network: Learns the Riemannian metric tensor of the latent space\nLet’s define the hyperparameters and architecture of our model\n\n# Defining hyperparameters...\n\n# Define dimensionality of latent space\nn_latent = 2\n# Define number of neurons in hidden layers\nn_neuron = 128\n\n# Define RHVAE hyper-parameters\nT = 0.8f0 # Temperature\nλ = 1.0f-2 # Regularization parameter\nn_centroids = 256 # Number of centroids\n\n# Define loss function hyper-parameters\nϵ = Float32(1E-3) # Leapfrog step size\nK = 10 # Number of leapfrog steps\nβₒ = 0.3f0 # Initial temperature for tempering\n\n# Define RHVAE hyper-parameters in a named tuple\nrhvae_kwargs = (K=K, ϵ=ϵ, βₒ=βₒ)\n\n# Define training hyperparameters\nn_epoch = 75 # Number of epochs\nn_batch = 512 # Batch size\nn_batch_loss = 512 # Batch size for loss computation\nη = 10^-3 # Learning rate\nsplit_frac = 0.85 # Train/val split\n\n# Define ELBO prefactors\nlogp_prefactor = [10.0f0, 0.1f0, 0.1f0]\nlogq_prefactor = [0.1f0, 0.1f0, 0.1f0]\n\n# Define loss function kwargs in a NamedTuple\nloss_kwargs = (\n    K=K,\n    ϵ=ϵ,\n    βₒ=βₒ,\n    logp_prefactor=logp_prefactor,\n    logq_prefactor=logq_prefactor,\n)\n\n# Define by how much to subsample the time series\nn_sub = 10",
    "crumbs": [
      "Notebooks",
      "Training an RHVAE on synthetic data"
    ]
  },
  {
    "objectID": "code/rhvae_sim.html#loading-and-preprocessing-data",
    "href": "code/rhvae_sim.html#loading-and-preprocessing-data",
    "title": "Training an RHVAE on synthetic data",
    "section": "Loading and Preprocessing Data",
    "text": "Loading and Preprocessing Data\nNow let’s load the fitness profiles from our evolutionary simulations and preprocess them for training. Since the simulations are very densly sampled over time, we will train the RHVAE on a subsampled time series, taking every n_sub time point.\n\n# Loading data into memory...\n\n# Load fitnotype profiles\nfitnotype_profiles = JLD2.load(\"$(out_dir)/sim_evo.jld2\")[\"fitnotype_profiles\"]\n\n# Extract initial and final time points\nt_init, t_final = collect(DD.dims(fitnotype_profiles, :time)[[1, end]])\n# Subsample time series\nfitnotype_profiles = fitnotype_profiles[time=DD.At(t_init:n_sub:t_final)]\n\n# Define number of environments\nn_env = length(DD.dims(fitnotype_profiles, :landscape))\n\n# Extract fitness data bringing the fitness dimension to the first dimension\nfit_data = permutedims(fitnotype_profiles.fitness.data, (5, 1, 2, 3, 4, 6))\n# Reshape the array to a Matrix\nfit_data = reshape(fit_data, size(fit_data, 1), :)\n\n# Reshape the array to stack the 3rd dimension\nfit_mat = log.(fit_data)\n\n# Fit model to standardize data to mean zero and standard deviation 1 on each\n# environment \ndt = StatsBase.fit(StatsBase.ZScoreTransform, fit_mat, dims=2)\n\n# Standardize the data to have mean 0 and standard deviation 1\nfit_std = StatsBase.transform(dt, fit_mat)\n\n# Split indexes of data into training and validation\ntrain_idx, val_idx = Flux.splitobs(\n    1:size(fit_std, 2), at=split_frac, shuffle=true\n)\n\n# Extract train and validation data\ntrain_data = fit_std[:, train_idx]\nval_data = fit_std[:, val_idx]\n\nOur data preprocessing involves several key steps: 1. Log transformation: We apply a logarithmic transformation to the fitness values to make their distribution more amenable to the model. 2. Standardization: We standardize the log-fitness values per environment to have mean 0 and standard deviation 1. This is common practice when training neural networks as it helps with numerical stability and convergence. 3. Train-validation split: We split the data into training (85%) and validation (15%) sets. The validation set is used to monitor the model’s performance during training.",
    "crumbs": [
      "Notebooks",
      "Training an RHVAE on synthetic data"
    ]
  },
  {
    "objectID": "code/rhvae_sim.html#defining-the-rhvae-model-architecture",
    "href": "code/rhvae_sim.html#defining-the-rhvae-model-architecture",
    "title": "Training an RHVAE on synthetic data",
    "section": "Defining the RHVAE Model Architecture",
    "text": "Defining the RHVAE Model Architecture\nWe now define the RHVAE model architecture with its three core components: encoder, decoder, and metric network. For this, we use the AutoEncoderToolkit.jl package (Razo-Mejia 2024).\nThe first step is to select the centroids used to “anchor” the metric tensor learned by the RHVAE. We do this by using the centroids_kmedoids function from the AutoEncoderToolkit.jl package. These centroids will be part of the definition of the RHVAE model itself.\n\n# Selecting centroids via k-means...\n\n# Select centroids via k-medoids\ncentroids_data = AET.utils.centroids_kmedoids(fit_std, n_centroids)\n\nNow, we can define the RHVAE architecture. We start by defining the encoder, which maps the fitness profiles to a distribution in latent space. Since AutoEncoderToolkit.jl is based on Flux.jl, we will build all of the components of the RHVAE using Flux.jl components.\n\n# Define JointGaussianLogEncoder...\n\n# Define encoder chain\nencoder_chain = Flux.Chain(\n    # First layer\n    Flux.Dense(n_env =&gt; n_neuron, Flux.identity),\n    # Second layer\n    Flux.Dense(n_neuron =&gt; n_neuron, Flux.leakyrelu),\n    # Third layer\n    Flux.Dense(n_neuron =&gt; n_neuron, Flux.leakyrelu),\n    # Fourth layer\n    Flux.Dense(n_neuron =&gt; n_neuron, Flux.leakyrelu),\n)\n\n# Define layers for µ and log(σ)\nµ_layer = Flux.Dense(n_neuron =&gt; n_latent, Flux.identity)\nlogσ_layer = Flux.Dense(n_neuron =&gt; n_latent, Flux.identity)\n\n# build encoder\nencoder = AET.JointGaussianLogEncoder(encoder_chain, µ_layer, logσ_layer)\n\nNow, we define the decoder, which maps the latent space coordinates to the fitness profiles.\n\n# Define SimpleGaussianDecoder...\n\n# Initialize decoder\ndecoder = AET.SimpleGaussianDecoder(\n    Flux.Chain(\n        # First layer\n        Flux.Dense(n_latent =&gt; n_neuron, Flux.identity),\n        # Second Layer\n        Flux.Dense(n_neuron =&gt; n_neuron, Flux.leakyrelu),\n        # Third layer\n        Flux.Dense(n_neuron =&gt; n_neuron, Flux.leakyrelu),\n        # Fourth layer\n        Flux.Dense(n_neuron =&gt; n_neuron, Flux.leakyrelu),\n        # Output layer\n        Flux.Dense(n_neuron =&gt; n_env, Flux.identity)\n    )\n)\n\nFinally, we define the metric network, which learns the Riemannian metric tensor of the latent space.\n\n# Define MetricChain...\n\n# Define mlp chain\nmlp_chain = Flux.Chain(\n    # First layer\n    Flux.Dense(n_env =&gt; n_neuron, Flux.identity),\n    # Second layer\n    Flux.Dense(n_neuron =&gt; n_neuron, Flux.leakyrelu),\n    # Third layer\n    Flux.Dense(n_neuron =&gt; n_neuron, Flux.leakyrelu),\n    # Fourth layer\n    Flux.Dense(n_neuron =&gt; n_neuron, Flux.leakyrelu),\n)\n\n# Define layers for the diagonal and lower triangular part of the covariance\n# matrix\ndiag = Flux.Dense(n_neuron =&gt; n_latent, Flux.identity)\nlower = Flux.Dense(\n    n_neuron =&gt; n_latent * (n_latent - 1) ÷ 2, Flux.identity\n)\n\n# Build metric chain\nmetric_chain = AET.RHVAEs.MetricChain(mlp_chain, diag, lower)\n\nFor the final step, we bring all the components together to define the RHVAE model itself.\n\n# Initialize rhvae\nrhvae = AET.RHVAEs.RHVAE(\n    encoder * decoder,\n    metric_chain,\n    centroids_data,\n    T,\n    λ\n)\n\nThe RHVAE architecture comprises:\n\nEncoder:\n\nTakes fitness profiles (dimension = number of environments) as input\nProcesses through 4 hidden layers with 128 neurons each\nOutputs mean (μ) and log-variance (log σ) parameters of a diagonal Gaussian distribution in latent space\n\nDecoder:\n\nTakes latent space coordinates (dimension = 2) as input\nProcesses through 4 hidden layers with 128 neurons each\nOutputs reconstructed fitness profiles\n\nMetric Network:\n\nLearns the parameters of the Riemannian metric tensor\nTakes decoded fitness profiles as input\nOutputs parameters for the diagonal and lower triangular parts of the metric tensor\n\n\nWe also select centroids using k-medoids clustering to approximate the data manifold, which helps in computing the metric tensor more efficiently.",
    "crumbs": [
      "Notebooks",
      "Training an RHVAE on synthetic data"
    ]
  },
  {
    "objectID": "code/rhvae_sim.html#training-the-rhvae",
    "href": "code/rhvae_sim.html#training-the-rhvae",
    "title": "Training an RHVAE on synthetic data",
    "section": "Training the RHVAE",
    "text": "Training the RHVAE\nNow, let’s train our RHVAE model on the fitness profiles:\n\nprintln(\"Checking previous model states...\")\n\n# List previous model parameters\nmodel_states = sort(Glob.glob(\"$(state_dir)/beta-rhvae_epoch*.jld2\"[2:end], \"/\"))\n\n# Check if model states exist\nif length(model_states) &gt; 0\n    # Load model state\n    model_state = JLD2.load(model_states[end])[\"model_state\"]\n    # Input parameters to model\n    Flux.loadmodel!(rhvae, model_state)\n    # Update metric parameters\n    AET.RHVAEs.update_metric!(rhvae)\n    # Extract epoch number\n    epoch_init = parse(\n        Int, match(r\"epoch(\\d+)\", model_states[end]).captures[1]\n    ) + 1\nelse\n    epoch_init = 1\nend # if\n\nprintln(\"Initial epoch: $epoch_init\")\n\nprintln(\"Uploading model to GPU...\")\n\n# Check if CUDA is available\nif CUDA.functional()\n    # Upload model to GPU\n    rhvae = Flux.gpu(rhvae)\n    # Upload data to GPU\n    train_data = Flux.gpu(train_data)\n    val_data = Flux.gpu(val_data)\nend\n\n# Explicit setup of optimizer\nopt_rhvae = Flux.Train.setup(\n    Flux.Optimisers.Adam(η),\n    rhvae\n)\n\nprintln(\"\\nTraining RHVAE...\\n\")\n\n# Loop through number of epochs\nfor epoch in epoch_init:n_epoch\n    # Define number of batches\n    num_batches = size(train_data, 2) ÷ n_batch\n    # Shuffle data indexes\n    idx_shuffle = Random.shuffle(1:size(train_data, 2))\n    # Split indexes into batches\n    idx_batches = IterTools.partition(idx_shuffle, n_batch)\n    # Loop through batches\n    for (i, idx_tuple) in enumerate(idx_batches)\n        println(\"Epoch: $(epoch) | Batch: $(i) / $(length(idx_batches))\")\n        # Extract indexes\n        idx_batch = collect(idx_tuple)\n        # Train RHVAE\n        loss_epoch = AET.RHVAEs.train!(\n            rhvae, train_data[:, idx_batch], opt_rhvae;\n            loss_kwargs=loss_kwargs, verbose=false, loss_return=true\n        )\n        println(\"Loss: $(loss_epoch)\")\n    end # for train_loader\n\n    # Sample train data\n    train_sample = train_data[\n        :,\n        StatsBase.sample(1:size(train_data, 2), n_batch_loss, replace=false)\n    ]\n    # Sample val data\n    val_sample = val_data\n\n    println(\"Computing loss in training and validation data...\")\n    loss_train = AET.RHVAEs.loss(rhvae, train_sample; loss_kwargs...)\n    loss_val = AET.RHVAEs.loss(rhvae, val_sample; loss_kwargs...)\n\n    # Forward pass sample through model\n    println(\"Computing MSE in training and validation data...\")\n    out_train = rhvae(train_sample; rhvae_kwargs...).μ\n    mse_train = Flux.mse(train_sample, out_train)\n    out_val = rhvae(val_sample; rhvae_kwargs...).μ\n    mse_val = Flux.mse(val_sample, out_val)\n\n    println(\n        \"\\n Epoch: $(epoch) / $(n_epoch)\\n \" *\n        \"   - loss_train: $(loss_train)\\n\" *\n        \"   - loss_val: $(loss_val)\\n\" *\n        \"   - mse_train: $(mse_train)\\n\" *\n        \"   - mse_val: $(mse_val)\\n\"\n    )\n\n    # Save checkpoint\n    JLD2.jldsave(\n        \"$(state_dir)/beta-rhvae_epoch$(lpad(epoch, 5, \"0\")).jld2\",\n        model_state=Flux.state(rhvae) |&gt; Flux.cpu,\n        loss_train=loss_train,\n        loss_val=loss_val,\n        mse_train=mse_train,\n        mse_val=mse_val,\n        train_idx=train_idx,\n        val_idx=val_idx,\n    )\nend # for n_epoch\n\nThe training process involves:\n\nBatched Training: We process the data in batches of size 512.\nLoss Computation: We use a custom loss function for RHVAEs that combines:\n\nReconstruction loss (how well the model reconstructs the input)\nKL divergence (a regularization term)\nHamiltonian loss (related to the metric tensor)\n\nOptimization: We use the Adam optimizer with a learning rate of 0.001.\nCheckpointing: We save the model state after each epoch, allowing us to resume training if needed.\n\nThe loss includes several components weighted by the logp_prefactor and logq_prefactor parameters, which balance reconstruction quality against regularization.",
    "crumbs": [
      "Notebooks",
      "Training an RHVAE on synthetic data"
    ]
  },
  {
    "objectID": "code/rhvae_sim.html#analyzing-training-results",
    "href": "code/rhvae_sim.html#analyzing-training-results",
    "title": "Training an RHVAE on synthetic data",
    "section": "Analyzing Training Results",
    "text": "Analyzing Training Results\nAfter training, we can analyze the training results to understand how well our model is learning. First, let’s load the loss and mean squared error (MSE) training curves into a dataframe.\n\n# Loading trained model...\n\n# Find model file\nmodel_file = first(Glob.glob(\"$(out_dir)/rhvae_model*.jld2\"))\n# List epoch parameters\nmodel_states = Glob.glob(\"$(state_dir)/*.jld2\")\n\n# Initialize dataframe to store files metadata\ndf_meta = DF.DataFrame()\n\n# Loop over files\nfor f in model_states\n    # Extract epoch number from file name using regular expression\n    epoch = parse(Int, match(r\"epoch(\\d+)\", f).captures[1])\n    # Load model_state file\n    f_load = JLD2.load(f)\n    # Extract values\n    loss_train = f_load[\"loss_train\"]\n    loss_val = f_load[\"loss_val\"]\n    mse_train = f_load[\"mse_train\"]\n    mse_val = f_load[\"mse_val\"]\n    # Generate temporary dataframe to store metadata\n    df_tmp = DF.DataFrame(\n        :epoch =&gt; epoch,\n        :loss_train =&gt; loss_train,\n        :loss_val =&gt; loss_val,\n        :mse_train =&gt; mse_train,\n        :mse_val =&gt; mse_val,\n        :model_file =&gt; model_file,\n        :model_state =&gt; f,\n    )\n    # Append temporary dataframe to main dataframe\n    global df_meta = DF.vcat(df_meta, df_tmp)\nend # for f in model_states\n\nNow, we can plot the training loss and mean squared error (MSE) training curves.\n\n# Plotting training loss...\n\n# Initialize figure\nfig = Figure(size=(600, 300))\n\n# Add axis\nax = Axis(\n    fig[1, 1],\n    xlabel=\"epoch\",\n    ylabel=\"loss\",\n)\n\n# Plot training loss\nlines!(\n    ax,\n    df_meta.epoch,\n    df_meta.loss_train,\n    label=\"train\",\n)\n# Plot validation loss\nlines!(\n    ax,\n    df_meta.epoch,\n    df_meta.loss_val,\n    label=\"validation\",\n)\n\n# Add legend\naxislegend(ax, position=:rt)\n\n# Add axis\nax = Axis(\n    fig[1, 2],\n    xlabel=\"epoch\",\n    ylabel=\"mean squared error\",\n)\n\n# Plot training loss\nlines!(\n    ax,\n    df_meta.epoch,\n    df_meta.mse_train,\n    label=\"train\",\n)\n# Plot validation loss\nlines!(\n    ax,\n    df_meta.epoch,\n    df_meta.mse_val,\n    label=\"validation\",\n)\n\n# Add legend\naxislegend(ax, position=:rt)\n\nfig\n\n\n\n\nThis code loads all the training checkpoints and plots the training curves, showing: 1. The total loss over time for both training and validation sets 2. The Mean Squared Error (MSE) over time for both sets\nThese plots help us assess if the model is learning effectively and if it’s overfitting or underfitting the data. Since both curves perfectly track each other, we can conclude that the model is learning effectively without overfitting the training data.",
    "crumbs": [
      "Notebooks",
      "Training an RHVAE on synthetic data"
    ]
  },
  {
    "objectID": "code/rhvae_sim.html#mapping-data-to-latent-space",
    "href": "code/rhvae_sim.html#mapping-data-to-latent-space",
    "title": "Training an RHVAE on synthetic data",
    "section": "Mapping Data to Latent Space",
    "text": "Mapping Data to Latent Space\nNow that we have a trained model, we can use it to map our fitness profiles to the latent space. Again, we will take advantage of the DimArray type to preserve the metadata of the fitness profiles when mapping them to the latent space.\n\n# Mapping data to latent space...\n\n# Standardize the data to have mean 0 and standard deviation 1\nlog_fitnotype_std = DD.DimArray(\n    mapslices(slice -&gt; StatsBase.transform(dt, slice),\n        log.(fitnotype_profiles.fitness.data),\n        dims=[5]),\n    fitnotype_profiles.fitness.dims,\n)\n\n# Load model\nrhvae = JLD2.load(model_file)[\"model\"]\n# Load latest model state\nFlux.loadmodel!(rhvae, JLD2.load(df_meta.model_state[end])[\"model_state\"])\n# Update metric parameters\nAET.RHVAEs.update_metric!(rhvae)\n\n# Define latent space dimensions\nlatent = DD.Dim{:latent}([:latent1, :latent2])\n\n# Map data to latent space\ndd_latent = DD.DimArray(\n    dropdims(\n        mapslices(slice -&gt; rhvae.vae.encoder(slice).μ,\n            log_fitnotype_std.data,\n            dims=[5]);\n        dims=1\n    ),\n    (log_fitnotype_std.dims[2:4]..., latent, log_fitnotype_std.dims[6]),\n)\n\nThis process involves: 1. Loading the fitness data again and applying the same preprocessing steps 2. Loading our trained RHVAE model 3. Using the encoder part of the RHVAE to map each fitness profile to a point in the 2D latent space 4. Storing the results in a dimensional array that preserves the metadata (lineage, time, etc.)\nWe now have a low-dimensional representation of our fitness profiles that we can visualize and analyze.",
    "crumbs": [
      "Notebooks",
      "Training an RHVAE on synthetic data"
    ]
  },
  {
    "objectID": "code/rhvae_sim.html#visualizing-the-latent-space",
    "href": "code/rhvae_sim.html#visualizing-the-latent-space",
    "title": "Training an RHVAE on synthetic data",
    "section": "Visualizing the Latent Space",
    "text": "Visualizing the Latent Space\nLet’s visualize the latent space to see how our model has organized the fitness profiles:\n\n# Plotting latent space coordinates...\n\n# Initialize figure\nfig = Figure(size=(300, 300))\n\n# Add axis\nax = Axis(\n    fig[1, 1],\n    xlabel=\"latent dimension 1\",\n    ylabel=\"latent dimension 2\",\n    aspect=AxisAspect(1)\n)\n\n# Plot latent space\nscatter!(\n    ax,\n    vec(dd_latent[latent=DD.At(:latent1)]),\n    vec(dd_latent[latent=DD.At(:latent2)]),\n    markersize=5,\n)\n\nfig\n\n\n\n\nThis basic plot shows all points in the latent space. Each point represents the encoding of a fitness profile. Next, let’s colorize the points by lineage to gain more insight:\n\n# Plotting latent space coordinates colored by lineage...\n\n# Initialize figure\nfig = Figure(size=(300, 300))\n\n# Add axis\nax = Axis(\n    fig[1, 1],\n    xlabel=\"latent dimension 1\",\n    ylabel=\"latent dimension 2\",\n    aspect=AxisAspect(1)\n)\n\n# Loop over lineages\nfor (i, lin) in enumerate(DD.dims(dd_latent, :lineage))\n    # Plot latent space\n    scatter!(\n        ax,\n        vec(dd_latent[latent=DD.At(:latent1), lineage=lin]),\n        vec(dd_latent[latent=DD.At(:latent2), lineage=lin]),\n        markersize=5,\n        color=(ColorSchemes.seaborn_colorblind[i], 0.25),\n    )\nend # for \n\nfig\n\n\n\n\nThis plot shows the latent space colored by lineage. Each lineage is assigned a distinct color, allowing us to see how different lineages are distributed in the latent space.\n\nVisualizing the latent space curvature\nOne of the main advantages of the RHVAE is that it learns a Riemannian metric on the latent space, which allows us to relate distances in the latent space with distances in the original data space, despite the nonlinear transformations.\nThe metric tensor, \\(\\underline{\\underline{G}}(\\underline{z})\\), is a position-dependent positive-definite matrix that we can evaluate at any point \\(\\underline{z}\\) in the latent space. For at 2D latent space, this results in a 2x2 matrix. A way to visualize the local curvature of the latent space is to compute the so-called metric volume, defined as the square root of the determinant of the metric tensor, \\(\\sqrt{\\det(\\underline{\\underline{G}}(\\underline{z}))}\\). AutoEncoderToolkit.jl provides a function to compute the inverse metric tensor, \\(G^{-1}(\\underline{z})\\), from the model. We can use this directly to compute the metric volume at each point in the latent space.\nLet’s define the ranges of the latent space dimensions and evaluate the metric tensor at each point in the latent space.\n\n# Computing metric tensor...\n\n# Define number of points per axis\nn_points = 250\n\n# Extract latent space ranges\nlatent1_range = range(\n    minimum(dd_latent[latent=DD.At(:latent1)]) - 1.5,\n    maximum(dd_latent[latent=DD.At(:latent1)]) + 1.5,\n    length=n_points\n)\nlatent2_range = range(\n    minimum(dd_latent[latent=DD.At(:latent2)]) - 1.5,\n    maximum(dd_latent[latent=DD.At(:latent2)]) + 1.5,\n    length=n_points\n)\n# Define latent points to evaluate\nz_mat = reduce(hcat, [[x, y] for x in latent1_range, y in latent2_range])\n\n# Compute inverse metric tensor\nGinv = AET.RHVAEs.G_inv(z_mat, rhvae)\n\n# Compute metric \nlogdetG = reshape(\n    -1 / 2 * AET.utils.slogdet(Ginv), n_points, n_points\n)\n\nNow, let’s plot the heatmap of the metric volume and the surface of the latent space.\n\n# Plotting latent space metric...\n\n# Initialize figure\nfig = Figure(size=(700, 300))\n\n# Add axis\nax1 = Axis(\n    fig[1, 1],\n    xlabel=\"latent dimension 1\",\n    ylabel=\"latent dimension 2\",\n    aspect=AxisAspect(1)\n)\nax2 = Axis(\n    fig[1, 2],\n    xlabel=\"latent dimension 1\",\n    ylabel=\"latent dimension 2\",\n    aspect=AxisAspect(1)\n)\n\n# Plot heatmpat of log determinant of metric tensor\nhm = heatmap!(\n    ax1,\n    latent1_range,\n    latent2_range,\n    logdetG,\n    colormap=Reverse(to_colormap(ColorSchemes.PuBu)),\n)\n\nsurface!(\n    ax2,\n    latent1_range,\n    latent2_range,\n    logdetG,\n    colormap=Reverse(to_colormap(ColorSchemes.PuBu)),\n    shading=NoShading,\n    rasterize=true,\n)\n\n# Plot latent space\nscatter!(\n    ax2,\n    vec(dd_latent[latent=DD.At(:latent1)]),\n    vec(dd_latent[latent=DD.At(:latent2)]),\n    markersize=4,\n    color=(:white, 0.3),\n    rasterize=true,\n)\n\n# Find axis limits from minimum and maximum of latent points\nxlims!.(\n    [ax2, ax1],\n    minimum(dd_latent[latent=DD.At(:latent1)]) - 1.5,\n    maximum(dd_latent[latent=DD.At(:latent1)]) + 1.5\n)\nylims!.(\n    [ax2, ax1],\n    minimum(dd_latent[latent=DD.At(:latent2)]) - 1.5,\n    maximum(dd_latent[latent=DD.At(:latent2)]) + 1.5\n)\n\n# Add colorbar\nColorbar(fig[1, 3], hm, label=\"√log[det(G̲̲)]\")\n\nfig\n\n\n\n\nThis plot shows the heatmap of the metric volume and the surface of the latent space. The colorbar shows the metric volume, which is a measure of the local curvature of the latent space. The darker the region, the flatter the latent space. One neat feature of the RHVAE is that the learned metric tensor “cages” the data cloud, clearly defining the regions in which the model is able to make meaningful predictions (since neural networks are famously terrible at extrapolation). We also see that within the data cloud, regions of high curvature coincide with the four regions of low genotype-phenotype fitness we originally defined in the Evolutionary Dynamics, suggesting that the model is able to capture the underlying phenotypic space.",
    "crumbs": [
      "Notebooks",
      "Training an RHVAE on synthetic data"
    ]
  },
  {
    "objectID": "code/rhvae_sim.html#conclusion",
    "href": "code/rhvae_sim.html#conclusion",
    "title": "Training an RHVAE on synthetic data",
    "section": "Conclusion",
    "text": "Conclusion\nIn this notebook, we’ve trained a Riemannian Hamiltonian Variational Autoencoder (RHVAE) on synthetic data generated from evolutionary simulations. We’ve used the RHVAE to learn a low-dimensional representation of the fitness profiles and visualized the latent space to gain insights into the model’s organization of the fitness profiles.",
    "crumbs": [
      "Notebooks",
      "Training an RHVAE on synthetic data"
    ]
  },
  {
    "objectID": "code/metropolis_hastings_evolution.html",
    "href": "code/metropolis_hastings_evolution.html",
    "title": "Energy-based evolutionary dynamics",
    "section": "",
    "text": "(c) This work is licensed under a Creative Commons Attribution License CC-BY 4.0. All code contained herein is licensed under an MIT license.\n# Import project package\nimport Antibiotic\n\n# Import basic math libraries\nimport StatsBase\nimport LinearAlgebra\nimport Random\nimport Distributions\n\n# Load CairoMakie for plotting\nusing CairoMakie\nimport ColorSchemes\n\n# Activate backend\nCairoMakie.activate!()\n\n# Set custom plotting style\nAntibiotic.viz.theme_makie!()",
    "crumbs": [
      "Notebooks",
      "Energy-based evolutionary dynamics"
    ]
  },
  {
    "objectID": "code/metropolis_hastings_evolution.html#evolutionary-dynamics",
    "href": "code/metropolis_hastings_evolution.html#evolutionary-dynamics",
    "title": "Energy-based evolutionary dynamics",
    "section": "Evolutionary dynamics",
    "text": "Evolutionary dynamics\n\nMetropolis-Hastings Algorithm in Evolutionary Dynamics\nThe Metropolis-Hastings algorithm is a Markov Chain Monte Carlo (MCMC) method used to sample from a target probability distribution \\(\\pi(\\underline{x})\\). In our evolutionary context, \\(\\pi(\\underline{x})\\) can be thought of as the steady-state distribution of phenotypes under the combined influence of selection and mutation. We can define this distribution as:\n\\[\n\\pi(\\underline{x}) \\propto \\exp\\left(\n    -\\beta U(\\underline{x})\n\\right),\n\\tag{5}\\]\nhere, \\(\\beta\\) is an inverse temperature parameter (from statistical mechanics) that controls the level of stochasticity in the system. A higher \\(\\beta\\) means the system is more likely to move towards higher fitness and mutational accessibility.\n\n\nProposal Distribution\nAt each step, we propose a new phenotype \\(\\underline{x}{\\prime}\\) from a proposal distribution \\(q(\\underline{x}{\\prime} | \\underline{x})\\). For simplicity, we can use a symmetric proposal distribution, such as a Gaussian centered at the current phenotype. Furthere, we constrain the proposal distribution to be symmetric, i.e., \\[\nq(x | x') = q(x' | x).\n\\tag{6}\\]\nThis symmetry simplifies the acceptance probability later on.\n\n\nAcceptance Probability\nThe acceptance probability \\(P_{\\text{accept}}\\) for moving from \\(\\underline{x}\\) to \\(\\underline{x}{\\prime}\\) is given by:\n\\[\nP_{\\text{accept}} =\n\\min\\left(1,\n    \\frac{\\pi(\\underline{x}{\\prime}) q(\\underline{x} | \\underline{x}{\\prime})}\n    {\\pi(\\underline{x}) q(\\underline{x}{\\prime} | \\underline{x})}\n\\right).\n\\tag{7}\\]\nGiven the symmetry of the proposal distribution, \\(q(\\underline{x}{\\prime} |\n\\underline{x}) = q(\\underline{x} | \\underline{x}{\\prime})\\) , so the acceptance probability simplifies to:\n\\[\nP_{\\text{accept}} = \\min\\left(\n    1,\n    \\frac{\\pi(\\underline{x}{\\prime})}{\\pi(\\underline{x})}\n    \\right) = \\min\\left(\n    1,\n    e^{\\beta [\\ln F_E(\\underline{x}{\\prime}) + \\ln M(\\underline{x}{\\prime})] -\n    \\beta [\\ln F_E(\\underline{x}) + \\ln M(\\underline{x})]}\n    \\right).\n\\tag{8}\\]\nThis can be rewritten as \\[\nP_{\\text{accept}} = \\min\\left(\n    1,\n    \\frac{F_E(\\underline{x}{\\prime})^β M(\\underline{x}{\\prime})^β}\n    {F_E(\\underline{x})^β M(\\underline{x})^β}\n\\right),\n\\tag{9}\\]\nThis expression shows that the acceptance probability depends on the difference in fitness and mutational accessibility between the current and proposed phenotypes.\nLet’s now implement the Metropolis-Hastings algorithm in code.\n\n@doc raw\"\"\"\n    evo_metropolis_hastings(x0, fitness_peaks, mut_peaks, β, µ, n_steps)\n\nPerform evolutionary Metropolis-Hastings algorithm to simulate phenotypic\nevolution.\n\n# Arguments\n- `x0::AbstractVecOrMat`: Initial phenotype vector or matrix.\n- `fitness_peaks::Union{GaussianPeak, Vector{GaussianPeak}}`: Fitness landscape\n  defined by one or more Gaussian peaks.\n- `mut_peaks::Union{GaussianPeak, Vector{GaussianPeak}}`: genotype-phenotype\n  density function defined by one or more Gaussian peaks.\n- `β::AbstractFloat`: Inverse temperature parameter controlling selection\n  strength.\n- `µ::AbstractFloat`: Mutation step size standard deviation.\n- `n_steps::Int`: Number of steps to simulate.\n\n# Returns\n- `Matrix{Float64}`: Matrix of phenotypes, where each column represents a step\n  in the simulation.\n\n# Description\nThis function implements the Metropolis-Hastings algorithm to simulate\nphenotypic evolution in a landscape defined by fitness and mutational\naccessibility. It uses the following steps:\n\n1. Initialize the phenotype trajectory with the given starting point.\n2. For each step: \n   a. Propose a new phenotype by adding Gaussian noise. \n   b. Calculate the fitness and genotype-phenotype density values for the new \n      phenotype.\n   c. Compute the acceptance probability based on the ratio of new and current \n      values. \n   d. Accept or reject the proposed phenotype based on the acceptance \n      probability.\n3. Return the complete phenotype trajectory.\n\nThe acceptance probability is calculated using the simplified form: \nP_accept = min(1, (F_E(x_new) * GP(x_new)) / (F_E(x) * GP(x)))\n\nwhere F_E is the fitness function and GP is the genotype-phenotype density\nfunction.\n\"\"\"\nfunction evo_metropolis_hastings(\n    x0::AbstractVector,\n    fitness_peaks::Union{GaussianPeak,Vector{GaussianPeak}},\n    mut_peaks::Union{GaussianPeak,Vector{GaussianPeak}},\n    β::Real,\n    µ::Real,\n    n_steps::Int,\n)\n    # Initialize array to hold phenotypes\n    x = Matrix{Float64}(undef, length(x0), n_steps + 1)\n    # Set initial phenotype\n    x[:, 1] = x0\n\n    # Compute fitness and genotype-phenotype density at initial phenotype\n    fitness_val = fitness(fitness_peaks, x0)\n    mut_val = genotype_phenotype_density(mut_peaks, x0)\n\n    # Loop over steps\n    for t in 1:n_steps\n        # Propose new phenotype\n        x_new = x[:, t] + µ * randn(length(x0))\n\n        # Calculate fitness and genotype-phenotype density\n        fitness_val_new = fitness(fitness_peaks, x_new)\n        mut_val_new = genotype_phenotype_density(mut_peaks, x_new)\n\n        # Compute acceptance probability\n        P_accept = min(\n            1, (fitness_val_new * mut_val_new / (fitness_val * mut_val))^β\n        )\n\n        # Accept or reject proposal\n        if rand() &lt; P_accept\n            # Accept proposal\n            x[:, t+1] = x_new\n            # Update fitness and genotype-phenotype density\n            fitness_val = fitness_val_new\n            mut_val = mut_val_new\n        else\n            # Reject proposal\n            x[:, t+1] = x[:, t]\n        end\n    end # for\n    return x\nend # function\n\nLet’s test this function with the previously defined fitness and genotype-phenotype density functions.\n\nRandom.seed!(42)\n\n# Define peak parameters\namplitude = 5.0\nmean = [0.0, 0.0]\ncovariance = [3.0 0.0; 0.0 3.0]\n# Create peak\nfit_peak = GaussianPeak(amplitude, mean, covariance)\n# Define peak parameters\namplitude = 1.0\nmeans = [\n    [-1.5, -1.5],\n    [1.5, -1.5],\n    [1.5, 1.5],\n    [-1.5, 1.5],\n]\ncovariance = [0.45 0.0; 0.0 0.45]\n\n# Create peak\nmut_peaks = GaussianPeak.(Ref(amplitude), means, Ref(covariance))\n\n# Set initial phenotype\nx0 = [-2.5, -2.5]\n\n# Set parameters\nβ = 10.0\nµ = 0.1\nn_steps = 300\n\n# Run Metropolis-Hastings algorithm\nx_traj = evo_metropolis_hastings(x0, fit_peak, mut_peaks, β, µ, n_steps)\n\nLet’s plot the trajectory of the phenotypes in phenotype space.\n\nRandom.seed!(42)\n\n# Define range of phenotypes to evaluate\nx = range(-4, 4, length=100)\ny = range(-4, 4, length=100)\n\n# Create meshgrid\nF = fitness.(Ref(fit_peak), [[x, y] for x in x, y in y])\nM = genotype_phenotype_density.(Ref(mut_peaks), [[x, y] for x in x, y in y])\n\n# Initialize figure\nfig = Figure(size=(600, 300))\n\n# Add axis for trajectory in fitness landscape\nax1 = Axis(\n    fig[1, 1],\n    xlabel=\"phenotype 1\",\n    ylabel=\"phenotype 2\",\n    aspect=AxisAspect(1),\n    title=\"Fitness landscape\",\n    yticklabelsvisible=false,\n    xticklabelsvisible=false,\n)\n# Add axis for trajectory in genotype-phenotype density\nax2 = Axis(\n    fig[1, 2],\n    xlabel=\"phenotype 1\",\n    ylabel=\"phenotype 2\",\n    aspect=AxisAspect(1),\n    title=\"Genotype-phenotype\\nDensity\",\n    yticklabelsvisible=false,\n    xticklabelsvisible=false,\n)\n\n# Plot a heatmap of the fitness landscape\nheatmap!(ax1, x, y, F, colormap=:algae)\n# Plot heatmap of genotype-phenotype density\nheatmap!(ax2, x, y, M, colormap=Reverse(ColorSchemes.Purples_9))\n\n# Plot contour plot\ncontour!(ax1, x, y, F, color=:white)\ncontour!(ax2, x, y, M, color=:white)\n\n# Plot trajectory\nscatterlines!.(\n    [ax1, ax2],\n    Ref(x_traj[1, :]),\n    Ref(x_traj[2, :]),\n    color=ColorSchemes.seaborn_colorblind[4],\n    markersize=5\n)\n\nfig\n\n\n\n\nLet’s plot the energy for the phenotypes in the trajectory.\n\n# Initialize array to hold energy\nU = Vector{Float64}(undef, n_steps + 1)\n\n# Loop over steps\nfor (i, x) in enumerate(eachcol(x_traj))\n    U[i] = -log(fitness(fit_peak, x)) - log(genotype_phenotype_density(mut_peaks, x))\nend\n\n# Initialize figure\nfig = Figure(size=(350, 300))\n\n# Add axis for energy\nax = Axis(\n    fig[1, 1],\n    xlabel=\"step\",\n    ylabel=\"energy\",\n    yticklabelsvisible=false,\n)\n\n# Plot energy\nlines!(ax, U)\n\nfig\n\n\n\n\nLet’s now repeat the simulation multiple times and plot all different trajectories.\n\nRandom.seed!(42)\n\n# Define number of simulations\nn_sim = 5\n\n# Run simulations\nx_traj_list = [\n    evo_metropolis_hastings(x0, fit_peak, mut_peaks, β, µ, n_steps)\n    for _ in 1:n_sim\n]\n\n# Initialize figure\nfig = Figure(size=(600, 300))\n\n# Add axis for fitness landscape\nax1 = Axis(\n    fig[1, 1],\n    xlabel=\"phenotype 1\",\n    ylabel=\"phenotype 2\",\n    aspect=AxisAspect(1),\n    title=\"Fitness\",\n    yticklabelsvisible=false,\n    xticklabelsvisible=false,\n)\n# Add axis for genotype-phenotype density\nax2 = Axis(\n    fig[1, 2],\n    xlabel=\"phenotype 1\",\n    ylabel=\"phenotype 2\",\n    aspect=AxisAspect(1),\n    title=\"Genotype-phenotype\\nDensity\",\n    yticklabelsvisible=false,\n    xticklabelsvisible=false,\n)\n\n# Plot fitness landscape\nheatmap!(ax1, x, y, F, colormap=:algae)\n# Plot heatmap of genotype-phenotype density\nheatmap!(ax2, x, y, M, colormap=Reverse(ColorSchemes.Purples_9))\n\n# Plot contour plot\ncontour!(ax1, x, y, F, color=:white)\ncontour!(ax2, x, y, M, color=:white)\n\n# Loop over simulations\nfor (i, x_traj) in enumerate(x_traj_list)\n    # Plot trajectory\n    scatterlines!.(\n        [ax1, ax2],\n        Ref(x_traj[1, :]),\n        Ref(x_traj[2, :]),\n        color=ColorSchemes.seaborn_colorblind[i],\n        markersize=3\n    )\nend\n\n# Set limits\nxlims!(ax1, -4, 4)\nylims!(ax1, -4, 4)\nxlims!(ax2, -4, 4)\nylims!(ax2, -4, 4)\n\nfig\n\n\n\n\nAs desired, all trajectories climb up the fitness peak avoiding the low genotype-phenotype density regions.",
    "crumbs": [
      "Notebooks",
      "Energy-based evolutionary dynamics"
    ]
  },
  {
    "objectID": "code/metropolis_hastings_evolution.html#conclusion",
    "href": "code/metropolis_hastings_evolution.html#conclusion",
    "title": "Energy-based evolutionary dynamics",
    "section": "Conclusion",
    "text": "Conclusion\nIn this notebook we proposed a simple model for evolutionary dynamics inspired on the classic Metropolis-Hastings algorithm. We used this model to simulate the evolution of a phenotype in a fitness landscape with an explicit genotype-phenotype density function. The simple energy-based model allows us to integrate both fitness and genotype-phenotype density in a single framework. This model is the basis for the simulations presented in the main text of the paper.",
    "crumbs": [
      "Notebooks",
      "Energy-based evolutionary dynamics"
    ]
  },
  {
    "objectID": "supplementary.html",
    "href": "supplementary.html",
    "title": "Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps",
    "section": "",
    "text": "The experimental data used throughout this work was kindly provided by the first author of the excellent paper [1]. In there, the authors evolved E. coli strains on different antibiotics via an automated liquid handling platform. Here, we will detail our analysis of the raw data to infer the \\(IC_{50}\\) values of the strains on a panel of antibiotics.\n\n\nTo justify the analysis, we first need to understand the experimental design. To determine the antibiotic resistance on multiple antibiotics, the authors adopted a 96-well plate design depicted in Figure 1. On a 96-well plate, each row contained a titration of one of eight antibiotics. This format allowed measuring the optical density of the bacterial culture in each well as a function of each of the antibiotic concentrations. To evolve the strains on a particular antibiotic (e.g., tetracycline as in Figure 1), the authors propagated the strains every day by inoculating a fresh plate with the same panel of antibiotic titrations solely from the well with the highest tetracycline concentration in which there was still measurable growth the day before. In this way, the authors selected for the strains with the highest antibiotic resistance while still measuring the effects of this adaptation on the other antibiotics.\n\n\n\n\n\n\nFigure 1: Iwasawa et al. experimental design. Schematic of the experimental design used by Iwasawa et al. [1]. On a 96-well plate, each of the eight rows contained a titration of one of eight antibiotics. For strains evolved in tetracycline (TET), the plate for each day \\(t+1\\) was inoculated by propagating from the well with the highest tetracycline concentration in which there was still measurable growth on day \\(t\\) to all 96 wells.\n\n\n\nFigure 2 shows a typical progression of the optical density curves over the course of the experimental evolution. When adapted to a particular antibiotic, the optical density curves with their sigmoidal shape shifted to the right, indicating that the strains were able to grow at higher concentrations of the antibiotic.\n\n\n\n\n\n\nFigure 2: Iwasawa et al. typical experimental data. The optical density curves of the bacterial culture in the evolution antibiotic over the course of the experiment. The curves are shifted to the right, indicating that the strains were able to grow at higher concentrations of the antibiotic.\n\n\n\n\n\n\nGiven the natural sigmoidal shape of the optical density curves, we can model the optical density \\(OD(x)\\) of the bacterial culture as a function of the antibiotic concentration \\(x\\). Following the approach of [1], we model the optical density as a function of the antibiotic concentration \\(x\\) as\n\\[\nf(x) = \\frac{a}\n{1+\\exp \\left[b\\left(\\log _2 x-\\log _2 \\mathrm{IC}_{50}\\right)\\right]} + c\n\\tag{1}\\]\nwhere \\(a\\), \\(b\\), and \\(c\\) are nuisance parameters of the model, \\(\\mathrm{IC}_{50}\\) is the parameter of interest, and \\(x\\) is the antibiotic concentration. We can define a function to compute this model.\nGiven the model presented in Equation 1, and the data, our objective is to infer the value of all parameters (including the nuisance parameters). By Bayes theorem, we write \\[\n\\pi(\\mathrm{IC}_{50}, a, b, c \\mid \\text{data}) =\n\\frac{\\pi(\\text{data} \\mid \\mathrm{IC}_{50}, a, b, c)\n\\pi(\\mathrm{IC}_{50}, a, b, c)}\n{\\pi(\\text{data})},\n\\tag{2}\\]\nwhere \\(\\text{data}\\) consists of the pairs of antibiotic concentration and optical density. Let’s begin by defining the likelihood function. For simplicity, we assume each datum is independent and identically distributed (i.i.d.) and write\n\\[\n\\pi(\\text{data} \\mid \\mathrm{IC}_{50}, a, b, c) =\n\\prod_{i=1}^n \\pi(d_i \\mid \\mathrm{IC}_{50}, a, b, c),\n\\tag{3}\\]\nwhere \\(d_i = (x_i, y_i)\\) is the \\(i\\)-th pair of antibiotic concentration and optical density, respectively, and \\(n\\) is the total number of data points. We assume that our experimental measurements can be expressed as\n\\[\ny_i = f(x_i, \\mathrm{IC}_{50}, a, b, c) + \\epsilon_i,\n\\tag{4}\\]\nwhere \\(\\epsilon_i\\) is the experimental error. Furthermore, we assume that the experimental error is normally distributed, i.e.,\n\\[\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2),\n\\tag{5}\\]\nwhere \\(\\sigma^2\\) is an unknown variance parameter that must be included in our inference. Notice that we assume the same variance parameter for all data points since \\(\\sigma^2\\) is not indexed by \\(i\\).\nGiven this likelihood function, we must update our inference on the parameters as\n\\[\n\\pi(\\mathrm{IC}_{50}, a, b, c, \\sigma^2 \\mid \\text{data}) =\n\\frac{\\pi(\\text{data} \\mid \\mathrm{IC}_{50}, a, b, c, \\sigma^2)\n\\pi(\\mathrm{IC}_{50}, a, b, c, \\sigma^2)}\n{\\pi(\\text{data})},\n\\tag{6}\\]\nto include the new parameter \\(\\sigma^2\\). Our likelihood function is then of the form\n\\[\ny_i \\mid \\mathrm{IC}_{50}, a, b, c, \\sigma^2 \\sim\n\\mathcal{N}(f(x_i, \\mathrm{IC}_{50}, a, b, c), \\sigma^2).\n\\tag{7}\\]\nFor the prior, we assume that all parameters are independent and write \\[\n\\pi(\\mathrm{IC}_{50}, a, b, c, \\sigma^2) =\n\\pi(\\mathrm{IC}_{50}) \\pi(a) \\pi(b) \\pi(c) \\pi(\\sigma^2).\n\\tag{8}\\]\nLet’s detail each prior.\n\n\\(\\mathrm{IC}_{50}\\): The \\(IC_{50}\\) is a strictly positive parameter. However, we will fit for \\(\\log_2(\\mathrm{IC}_{50})\\). Thus, we will use a normal prior for \\(\\log_2(\\mathrm{IC}_{50})\\). This means we have \\[\n\\log_2(\\mathrm{IC}_{50}) \\sim\n\\mathcal{N}(\n\\mu_{\\log_2(\\mathrm{IC}_{50})}, \\sigma_{\\log_2(\\mathrm{IC}_{50})}^2\n).\n\\tag{9}\\]\n\\(a\\): This nuisance parameter scales the logistic function. Again, the natural scale for this parameter is a strictly positive real number. Thus, we will use a lognormal prior for \\(a\\). This means we have \\[\na \\sim \\text{LogNormal}(\\mu_a, \\sigma_a^2).\n\\tag{10}\\]\n\\(b\\): This parameter controls the steepness of the logistic function. Again, the natural scale for this parameter is a strictly positive real number. Thus, we will use a lognormal prior for \\(b\\). This means we have \\[\nb \\sim \\text{LogNormal}(\\mu_b, \\sigma_b^2).\n\\tag{11}\\]\n\\(c\\): This parameter controls the minimum value of the logistic function. Since this is a strictly positive real number that does not necessarily scale with the data, we will use a half-normal prior for \\(c\\). This means we have \\[\nc \\sim \\text{Half-}\\mathcal{N}(0, \\sigma_c^2).\n\\tag{12}\\]\n\\(\\sigma^2\\): This parameter controls the variance of the experimental error. Since this is a strictly positive real number that does not necessarily scale with the data, we will use a half-normal prior for \\(\\sigma^2\\). This means we have \\[\n\\sigma^2 \\sim \\text{Half-}\\mathcal{N}(0, \\sigma_{\\sigma^2}^2).\n\\tag{13}\\]\n\n\n\nTo identify and remove outliers from our optical density measurements, we implemented a residual-based outlier detection method. First, we fit the logistic model (Equation 1) to the data using a deterministic least-squares approach. We then computed the residuals between the fitted model and the experimental measurements. Points with residuals exceeding a threshold of two standard deviations from the mean residual were classified as outliers and removed from subsequent analysis. This approach helped eliminate experimental artifacts while preserving the underlying sigmoidal relationship between antibiotic concentration and optical density.\n\n\n\n\nWith this setup, we can sample the posterior distribution using Markov Chain Monte Carlo (MCMC) methods. In particular, we used the Turing.jl package [2] implementation of the No-U-Turn Sampler (NUTS). All code is available in the GitHub repository for this work. Figure 3 shows the posterior predictive checks of the posterior distribution for the \\(IC_{50}\\) parameter. Briefly, the posterior predictive checks are a visual method to assess the fit of the model to the data. The idea is to generate samples from the posterior distribution and compare them to the data. If the model is a good fit, the samples should be able to reproduce the data.\nThe effectiveness of the residual-based outlier detection method is demonstrated in Figure 3, where the posterior predictive checks show significantly tighter credible intervals after outlier removal, particularly in regions of high antibiotic concentration where experimental noise tends to be more pronounced.\n\n\n\n\n\n\nFigure 3: Posterior predictive check. The posterior predictive check of the posterior distribution for the \\(IC_{50}\\) parameter inference. The blue line is the mean of the posterior distribution, the different shades of blue represent the 95%, 68%, and 50% credible regions. The black dots are the raw optical density measurements as provided by Iwasawa et al. [1].\n\n\n\n\n\n\n\n\nIn the main text, we describe a simple algorithm for evolving a population as a biased random walk on a phenotypic space controlled by both a fitness function and a genotype-phenotype density function. Here, we provide a more detailed description of the algorithm and its implementation.\n\n\nWe assume that phenotypes driving adaptation can be described by real-valued numbers. Let \\(\\underline{x}(t) = (x_1(t), x_2(t), \\cdots, x_N(t))\\) be an \\(N\\) dimensional vector describing the phenotype of a population at time \\(t\\). The fitness for a given environment \\(E\\) is a scalar function \\(F_E(\\underline{x})\\) such that\n\\[\nF_E: \\mathbb{R}^N \\to \\mathbb{R},\n\\tag{14}\\]\ni.e, the coordinates in phenotype space map to a fitness value. Throughout this work, we consider a series of Gaussian fitness peaks, i.e.,\n\\[\nF_E(\\underline{x}) =\n\\sum_{p=1}^P A_p\n\\exp\\left(\n    -\\frac{1}{2}\n    (\\underline{x} - \\underline{\\hat{x}}^{(p)})^T\n    \\Sigma_p^{-1}\n    (\\underline{x} - \\underline{\\hat{x}}^{(p)})\n\\right),\n\\tag{15}\\]\nwhere \\(P\\) is the number of peaks, \\(A_p\\) is the amplitude of the \\(p\\)-th peak, \\(\\underline{\\hat{x}}^{(p)}\\) is the \\(N\\)-dimensional vector of optimal phenotypes for the \\(p\\)-th peak, and \\(\\Sigma_p\\) is the \\(N \\times N\\) covariance matrix for the \\(p\\)-th peak. This formulation allows for multiple peaks in the fitness landscape, each potentially having different heights, widths, and covariance structures between dimensions. The covariance matrix \\(\\Sigma_p\\) captures potential interactions between different phenotypic dimensions, allowing for more complex and realistic fitness landscapes.\nIn the same vein, we define a genotype-phenotype density function \\(GP(\\underline{x})\\) as a scalar function that maps a phenotype to a mutational effect, i.e.,\n\\[\nGP: \\mathbb{R}^N \\to \\mathbb{R},\n\\tag{16}\\]\nwhere \\(GP(\\underline{x})\\) is related to the probability of a genotype mapping to a phenotype \\(\\underline{x}\\). As a particular density function we will consider a set of negative Gaussian peaks that will serve as “mutational barriers” that limit the range of phenotypes that can be reached. The deeper the peak, the less probable it is that a genotype will map to a phenotype within the peak. This idea captured by the following mutational landscape\n\\[\nGP(\\underline{x}) =\n\\sum_{p=1}^P -B_p\n\\exp\\left(\n    -\\frac{1}{2}\n    (\\underline{x} - \\underline{\\hat{x}}^{(p)})^T\n    \\Sigma_p^{-1}\n    (\\underline{x} - \\underline{\\hat{x}}^{(p)})\n\\right),\n\\tag{17}\\]\nwhere \\(B_p\\) is the depth of the \\(p\\)-th peak, and \\(\\underline{\\hat{x}}^{(p)}\\) is the \\(N\\)-dimensional vector of optimal phenotypes for the \\(p\\)-th peak.\n\n\n\nThe Metropolis-Hastings algorithm is a Markov Chain Monte Carlo (MCMC) method used to sample from a target probability distribution \\(\\pi(\\underline{x})\\). In our evolutionary context, \\(\\pi(\\underline{x})\\) can be thought of as the steady-state distribution of phenotypes under the combined influence of selection and mutation. We can define this distribution as\n\\[\n\\pi(\\underline{x}) \\propto \\exp\\left(\n    -\\beta U(\\underline{x})\n\\right),\n\\tag{18}\\]\nwhere \\(U(\\underline{x})\\) is the potential function defined as\n\\[\nU(\\underline{x}) = \\ln F_E(\\underline{x}) + \\ln GP(\\underline{x}).\n\\tag{19}\\]\nThis functional form of the potential function is motivated by the desired property of the potential function being a sum of the fitness and genotype-phenotype density functions. \\(\\beta\\) in Equation 18 is an inverse temperature parameter (from statistical mechanics) that controls the level of stochasticity in the system. A higher \\(\\beta\\) means the system is more likely to move towards higher fitness and genotype-phenotype density. In a sense, \\(\\beta\\) can be related to the effective population size, controlling the directness of the evolutionary dynamics.\nAt each step, we propose a new phenotype \\(\\underline{x}{\\prime}\\) from a proposal distribution \\(q(\\underline{x}{\\prime} | \\underline{x})\\). For simplicity, we can use a symmetric proposal distribution, such as a Gaussian centered at the current phenotype. Further, we constrain the proposal distribution to be symmetric, i.e.,\n\\[\nq(\\underline{x}{\\prime} | \\underline{x}) =\nq(\\underline{x} | \\underline{x}{\\prime}).\n\\tag{20}\\]\nThis symmetry simplifies the acceptance probability later on.\nThe acceptance probability \\(P_{\\text{accept}}\\) for moving from \\(\\underline{x}\\) to \\(\\underline{x}{\\prime}\\) is given by\n\\[\nP_{\\text{accept}} =\n\\min\\left(1,\n    \\frac{\\pi(\\underline{x}{\\prime}) q(\\underline{x} | \\underline{x}{\\prime})}\n    {\\pi(\\underline{x}) q(\\underline{x}{\\prime} | \\underline{x})}\n\\right).\n\\tag{21}\\]\nGiven the symmetry of the proposal distribution, \\(q(\\underline{x}{\\prime} |\n\\underline{x}) = q(\\underline{x} | \\underline{x}{\\prime})\\) , so the acceptance probability simplifies to\n\\[\nP_{\\text{accept}} = \\min\\left(\n    1,\n    \\frac{\\pi(\\underline{x}{\\prime})}{\\pi(\\underline{x})}\n    \\right).\n\\tag{22}\\]\nSubstituting Equation 18 and Equation 19 into Equation 22, we get:\n\\[\nP_{\\text{accept}} = \\min\\left(\n    1,\n    e^{\\beta [\\ln F_E(\\underline{x}{\\prime}) + \\ln M(\\underline{x}{\\prime})] -\n    \\beta [\\ln F_E(\\underline{x}) + \\ln M(\\underline{x})]}\n    \\right).\n\\tag{23}\\]\nThis can be rewritten as\n\\[\nP_{\\text{accept}} = \\min\\left(\n    1,\n    \\frac{F_E(\\underline{x}{\\prime})^β GP(\\underline{x}{\\prime})^β}\n    {F_E(\\underline{x})^β GP(\\underline{x})^β}\n\\right),\n\\tag{24}\\]\nThis expression shows that the acceptance probability depends on the difference in fitness and genotype-phenotype density between the current and proposed phenotypes.\n\n\n\nFigure 4 shows the effect of the inverse temperature parameter \\(\\beta\\) influences evolutionary trajectories in our Metropolis-Hastings framework. Through a series of panels showing both phenotypic trajectories (upper) and fitness dynamics (lower), we can observe how different \\(\\beta\\) values affect the balance between exploration and exploitation in phenotypic space.\nAt low \\(\\beta\\) values (left panels), the evolutionary trajectories exhibit highly stochastic behavior, with populations taking meandering paths through the phenotypic landscape. This represents a regime where genetic drift plays a dominant role. As \\(\\beta\\) increases (moving right), the trajectories become increasingly deterministic, with populations taking more direct paths toward fitness peaks, while avoiding genotype-to-phenotype density valleys. This transition reflects a shift from drift-dominated to selection-dominated dynamics, where populations more strictly follow the local fitness gradient. The fitness trajectories in the lower panels further reinforce this pattern, showing more erratic fitness fluctuations at low \\(\\beta\\) and more monotonic increases at high \\(\\beta\\), consistent with stronger selective pressures.\n\n\n\n\n\n\nFigure 4: Effect of inverse temperature on adaptive dynamics. Upper panels: Population trajectories on a landscape with a single fitness peak and four genotype-phenotype density peaks. Black dashed lines indicate fitness landscape contours, while white solid lines show genotype-phenotype density contours. Colored lines represent independent population trajectories, with initial positions (crosses) and final positions (triangles). Lower panels: Temporal evolution of population fitness. Panels from left to right show increasing values of inverse temperature (\\(\\beta\\)), demonstrating increasingly deterministic adaptive trajectories.\n\n\n\n\n\n\n\n\nThe bulk of this work makes use of the variational autoencoder variation developed by [3] known as Riemannian Hamiltonian Variational Autoencoder (RHVAE). Understanding this model requires some background that we provide in the following sections. It is not necessary to follow every mathematical derivation in detail for these sections, as they don’t directly pertain to the main contribution of this work. However, we believe that some intuition behind some of the concepts that inspired the RHVAE model can go a long way to understand the model. We invite the reader to refer to the original paper [3] and the references therein for additional details.\nLet us start by reviewing the variational autoencoder model setting and the variational inference framework.\n\n\nGiven a data set \\(\\underline{x}^{1:N} \\in \\mathcal{X} \\subseteq \\mathbb{R}^D\\), where the superscript denotes a set of \\(N\\) observations, i.e., \\[\n\\underline{x}^{1:N} =\n\\left\\{\n    \\underline{x}_1, \\underline{x}_2, \\ldots, \\underline{x}_N\n\\right\\},\n\\tag{25}\\] where \\(\\underline{x}_i \\in \\mathbb{R}^D\\) for \\(i = 1, 2, \\ldots, N\\), the variational autoencoder aims to fit a joint distribution \\(π_\\theta(\\underline{x}, \\underline{z})\\) where the data generative process is assumed to involve a set of latent—unobserved—continuous variables \\(\\underline{z} \\in \\mathcal{Z} \\subseteq \\mathbb{R}^d\\), where usually \\(d \\ll D\\). In other words, the VAE assumes that the data we get to observe is a consequence of a set of hidden variables \\(\\underline{z}\\) that we cannot measure directly. We assume that these variables live in a much lower-dimensional space than the one we observe and we are trying to learn this joint distribution such that, if desired, we can generate new data points by sampling from this distribution. Since the data we observe is a “consequence” of the latent variables, we express this joint distribution as\n\\[\nπ_\\theta(\\underline{x}, \\underline{z}) =\nπ_\\theta(\\underline{x}|\\underline{z}) \\, π_\\text{prior}(\\underline{z}),\n\\tag{26}\\]\nwhere \\(π_\\text{prior}(\\underline{z})\\) is the prior distribution over latent variables and \\(π_\\theta(\\underline{x}|\\underline{z})\\) is the likelihood of the data given the latent variables. This looks exactly as the problem one confronts when performing Bayesian inference. What distinguishes the VAE from other Bayesian inference problems, as we will see, is that in many cases, we do not know the functional form of both the prior and the likelihood function that generated our data, but we have enough data samples that we hope to be able to learn these function using the neural networks function approximation capabilities. Thus, the objective of the VAE is to parameterize this likelihood function via a neural network with parameters \\(\\theta\\) given some prior distribution over the latent variables. For simplicity, it is common to choose a standard normal prior distribution, i.e., \\(π_\\text{prior}(\\underline{z}) =\n\\mathcal{N}(\\underline{0}, \\underline{\\underline{I}}_d)\\), where \\(\\underline{\\underline{I}}_d\\) is the \\(d\\)-dimensional identity matrix [4], [5]. This way, once we have learned the parameters of the neural network, we can generate new data points by sampling from this prior distribution and running the resulting samples through the likelihood function encoded by this neural network.\nOur objective then becomes to find the parameters \\(\\theta\\) that maximize the probability of observing the data we actually observed. This is equivalent to maximizing the so-called marginal likelihood of the data, i.e.,\n\\[\n\\max_\\theta \\, π_\\theta(\\underline{x}) =\n\\max_\\theta \\int d^d\\underline{z} \\, π_\\theta(\\underline{x}, \\underline{z}).\n\\tag{27}\\]\nBut there are two problems with this objective:\n\nThe marginalization over the latent variables is intractable for most interesting distributions.\nThe distribution \\(π_\\theta(\\underline{x})\\) is in general unknown and we need to somehow approximate it.\n\nThe way around these problems is to introduce a family of parametric distributions \\(q_\\phi(\\underline{z} \\mid \\underline{x})\\), i.e., distributions that can be completely characterized by a set of parameters (think of Gaussians being a parametric family on the mean and the variance), and to approximate the true marginal distribution with one of these parametric distributions. This conceptual change is at the core of approximate methods for Bayesian inference known as variational inference. The objective then becomes to find the parameters \\(\\phi\\) that, when used to approximate the posterior distribution \\(π_\\theta(\\underline{z}|\\underline{x})\\), it is as close as possible to this posterior distribution. To mathematize this objective, we establish that we want to minimize the Kullback-Leibler divergence (also known as the relative entropy) between the variational distribution \\(q_\\phi(\\underline{z} \\mid \\underline{x})\\) and the posterior distribution \\(π_\\theta(\\underline{z} \\mid \\underline{x})\\), i.e., we aim to find \\(q_\\phi^*\\) such that\n\\[\nq_\\phi^*(\\underline{z} \\mid \\underline{x}) =\n\\min_\\phi \\, D_{KL}\\left(\n    q_\\phi(\\underline{z} \\mid \\underline{x}) \\|\n    π_\\theta(\\underline{z} \\mid \\underline{x})\n\\right).\n\\tag{28}\\]\nAfter some algebraic manipulation involving the well-known Gibbs inequality, we can show that this objective is equivalent to maximizing the so-called evidence lower bound (ELBO), \\(\\mathcal{L}\\), of the marginal likelihood, i.e.,\n\\[\n\\mathcal{L} = \\left\\langle\n    \\log π_\\theta(\\underline{x}, \\underline{z}) -\n    \\log q_\\phi(\\underline{z} \\mid \\underline{x})\n\\right\\rangle_{q_\\phi}\n\\leq \\log π_\\theta(\\underline{x}).\n\\tag{29}\\]\nThus, the optimization problem we want to solve invovles estimating the ELBO given some data \\(\\underline{x}\\). However, the ELBO is an expectation over the variational distribution \\(q_\\phi(\\underline{z} \\mid \\underline{x})\\), which is unknown. Therefore, we need to somehow estimate this expectation. Arguably, the most important contribution of the VAE framework is the introduction of the reparametrization trick [4]. This seemingly innocuous trick allows us to compute an unbiased estimate of the ELBO and its gradient, such that we can maximize it via gradient ascent. For more details on the reparametrization trick, we refer the reader to [4].\nMore recent attempts have focused on modifying \\(q_\\phi(\\underline{z} \\mid\n\\underline{x})\\) to get a better approximation of the true posterior \\(π_\\theta(\\underline{z} \\mid \\underline{x})\\). The basis of the Riemannian Hamiltonian Variational Autoencoder (RHVAE) framework takes inspiration from some of these approaches. Let’s gain some intuition on each of them.\n\n\n\nOne such approach to improve the accuracy of the variational posterior approximation consists in adding a fix number of Markov Chain Monte Carlo (MCMC) steps to the variational posterior approximation, targeting the true posterior [6]. In MCMC, rather than optimizing the parameters of a parametric distribution, we sample an initial position \\(\\underline{z}_0\\) from a simple distribution \\(q(\\underline{z}_0)\\) or \\(q(\\underline{z}_0 \\mid \\underline{x})\\) and subsequently, apply a stochastic transition operator \\(T(\\underline{z}_{t+1} \\mid\n\\underline{z}_t)\\) to draw a new value \\(\\underline{z}_{t+1}\\) from the distribution\n\\[\n\\underline{z}_{t + 1} \\sim T(\\underline{z}_{t+1} \\mid \\underline{z}_t).\n\\tag{30}\\]\nApplying this operator over and over again, we build a Markov chain\n\\[\n\\underline{z}_0 \\to \\underline{z}_1 \\to \\underline{z}_2 \\to \\cdots\n\\to \\underline{z}_\\tau,\n\\tag{31}\\]\nwhose long-term behavior—the so-called stationary distribution—is a very good approximation of the true posterior distribution \\(π_\\theta(\\underline{z}\n\\mid \\underline{x})\\). In other words, by carefully engineering the transition operator \\(T(\\underline{z}_{t+1} \\mid \\underline{z}_t)\\), and applying it over and over again, the “histogram” of the samples \\(\\{\\underline{z}_t\\}_{t=0}^\\tau\\) will converge to the true posterior distribution \\(π_\\theta(\\underline{z} \\mid\n\\underline{x})\\).\nThe central idea to combine MCMC with variational inference is to interpret the Markov chain\n\\[\nq_\\phi(\\underline{z} | \\underline{x}) =\nq_\\phi(\\underline{z}_0 \\mid \\underline{x})\n\\prod_{t=1}^\\tau T(\\underline{z}_t \\mid \\underline{z}_{t-1}, \\underline{x})\n\\tag{32}\\]\nas a variational approximation in an expanded space of variables that includes the original latent variables \\(\\underline{z}\\) and the auxiliary variables \\(\\underline{Z} = \\{\\underline{z}_0, \\underline{z}_1, \\ldots,\n\\underline{z}_{\\tau-1}\\}\\). Note that \\(\\underline{Z}\\) reaches up to \\(\\tau-1\\) steps, since we consider the final step \\(\\underline{z}_\\tau\\) to be the original latent variable \\(\\underline{z}\\). In other words, our variational distribution now writes \\[\nq_\\phi(\\underline{z}_\\tau, \\underline{Z} \\mid \\underline{x}) =\nq_\\phi(\\underline{z}_0 \\mid \\underline{x})\n\\prod_{t=1}^\\tau T(\\underline{z}_t \\mid \\underline{z}_{t-1}, \\underline{x}).\n\\tag{33}\\]\nIntegrating this expression into the ELBO (Equation 29) consists of substituting the variational distribution \\(q_\\phi(\\underline{z} \\mid \\underline{x})\\) with the expression in Equation 33, i.e., \\[\n\\mathcal{L}_{\\text {aux }} =\n\\left\\langle\\log\n    \\left[\n        \\pi_\\theta\\left(\\underline{x}, \\underline{z}_T\\right)\n        r\\left(\\underline{Z} \\mid \\underline{z}_T, \\underline{x}\\right)\n    \\right] -\n    \\log q\\left(\\underline{Z}, \\underline{z}_T \\mid \\underline{x}\\right)\n\\right\\rangle_{q\\left(\\underline{Z}, \\underline{z}_T \\mid \\underline{x}\\right)}\n\\tag{34}\\]\nwhere \\(r(\\underline{Z} \\mid \\underline{z}_T, \\underline{x})\\) is an auxiliary inference distribution that we can choose freely. This auxiliary distribution is necessary to ensure we account for the auxiliary variables \\(\\underline{Z}\\) that are now part of our variational distribution. By splitting the joint distribution in Equation 33 as\n\\[\nq_\\phi(\\underline{z}_\\tau, \\underline{Z} \\mid \\underline{x}) =\nq_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x}) q_\\phi(\\underline{z}_\\tau \\mid \\underline{x}),\n\\tag{35}\\]\nand substituting this expression into Equation 34, we get\n\\[\n\\mathcal{L}_{\\text {aux }} =\n\\left\\langle\\log\n    \\left[\n        \\pi_\\theta\\left(\\underline{x}, \\underline{z}_T\\right)\n        r\\left(\\underline{Z} \\mid \\underline{z}_T, \\underline{x}\\right)\n    \\right] -\n    \\log \\left[q_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x}) q_\\phi(\\underline{z}_\\tau \\mid \\underline{x})\\right]\n\\right\\rangle_{q_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x}) q_\\phi(\\underline{z}_\\tau \\mid \\underline{x})}.\n\\tag{36}\\]\nRearranging the terms, we get \\[\n\\begin{aligned}\n\\mathcal{L}_{\\text {aux }}\n& = \\left\\langle\n    \\log \\frac{\\pi_\\theta\\left(\\underline{x}, \\underline{z}_\\tau\\right)}{q_\\phi(\\underline{z}_\\tau \\mid \\underline{x})} -\n    \\log \\frac{q_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})}{r(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})}\n\\right\\rangle_{\n    q_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})\n    q_\\phi(\\underline{z}_\\tau \\mid \\underline{x})\n}, \\\\\n& = \\left\\langle\n    \\log \\frac{\\pi_\\theta\\left(\\underline{x}, \\underline{z}_\\tau\\right)}{q_\\phi(\\underline{z}_\\tau \\mid \\underline{x})}\n\\right\\rangle_{\n    q_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})\n    q_\\phi(\\underline{z}_\\tau \\mid \\underline{x})\n} -\n\\left\\langle\n    \\log \\frac{q_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})}{r(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})}\n\\right\\rangle_{\n    q_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})\n    q_\\phi(\\underline{z}_\\tau \\mid \\underline{x})\n}.\n\\end{aligned}\n\\tag{37}\\]\nFrom here, we note that the first term in Equation 37 is the original ELBO with the extra expectation taken over the auxiliary variables \\(\\underline{Z}\\). However, this term does not depend on the auxiliary variables \\(\\underline{Z}\\), thus, we can take it out of the expectation, i.e.,\n\\[\n\\mathcal{L}_{\\text {aux }} = \\mathcal{L} -\n\\left\\langle\n    \\log \\frac{q_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})}{r(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})}\n\\right\\rangle_{\n    q_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})\n    q_\\phi(\\underline{z}_\\tau \\mid \\underline{x})\n}.\n\\tag{38}\\]\nFor the second term, taking the expectation over the auxiliary variables makes it equivalent to the KL divergence between the variational distribution \\(q_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})\\) and the auxiliary distribution \\(r(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})\\), i.e.,\n\\[\n\\mathcal{L}_{\\text {aux }} = \\mathcal{L} -\n\\left\\langle\n    D_{K L}\\left[\n        q\\left(\\underline{Z} \\mid \\underline{z}_T, \\underline{x}\\right) \\|\n        r\\left(\\underline{Z} \\mid \\underline{z}_T, \\underline{x}\\right)\n    \\right]\n\\right\\rangle_{q\\left(\\underline{z}_T \\mid \\underline{x}\\right)}\n\\leq \\mathcal{L} \\leq \\log [\\pi_\\theta(\\underline{x})].\n\\tag{39}\\]\nwhere the inequality follows from the non-negativity of the KL divergence and the fact that the ELBO is a lower bound to the marginal likelihood. Since the variational distribution \\(q_\\phi(\\underline{z}_\\tau,\\mid \\underline{x})\\) we care about comes from the marginalization of the variational distribution over the auxiliary variables \\(\\underline{Z}\\),\n\\[\nq_\\phi(\\underline{z}_\\tau \\mid \\underline{x}) =\n\\int d\\underline{Z} \\,\nq_\\phi(\\underline{Z}, \\underline{z}_\\tau \\mid \\underline{x}),\n\\tag{40}\\]\nit is now a very rich family of distributions that can be used to better approximate the true posterior distribution \\(π_\\theta(\\underline{z} \\mid\n\\underline{x})\\). However, we are still left with the task of choosing the auxiliary distribution \\(r(\\underline{Z} \\mid \\underline{z}_\\tau,\n\\underline{x})\\). What Salimans & Kingma [6] propose is to define this distribution also as a Markov chain, but in reverse order, i.e.,\n\\[\nr(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x}) =\n\\prod_{t=1}^\\tau\nT^\\dagger(\\underline{z}_{t-1} \\mid \\underline{z}_t, \\underline{x}),\n\\tag{41}\\]\nwhere \\(T^\\dagger(\\underline{z}_{t-1} \\mid \\underline{z}_t, \\underline{x})\\) is the reverse of the transition operator \\(T(\\underline{z}_{t+1} \\mid\n\\underline{z}_t, \\underline{x})\\). Notice that the dependence on \\(\\underline{z}_\\tau\\) is dropped.\nThis choice of the auxiliary distribution leads to an upper bound on the marginal log likelihood, of the form\n\\[\n\\begin{aligned}\n\\log \\pi_\\theta(\\underline{x}) & \\geq\n\\left\\langle\n    \\log \\pi_\\theta\\left(\\underline{x}, \\underline{z}_\\tau\\right) -\n    \\log q_\\phi\\left(\n        \\underline{z}_0, \\ldots, \\underline{z}_\\tau \\mid \\underline{x}\n    \\right) +\n    \\log r\\left(\n        \\underline{z}_0, \\ldots, \\underline{z}_{\\tau-1} \\mid \\underline{x}, \\underline{z}_\\tau\n    \\right)\n\\right\\rangle_{q_\\phi} \\\\\n& = \\left\\langle\n    \\log \\left[\\frac{\n        \\pi_\\theta\\left(\\underline{x}, \\underline{z}_\\tau\\right)\n        }{\n            q_\\phi\\left(\\underline{z}_0 \\mid \\underline{x}\\right)\n        }\n    \\right] +\n    \\sum_{t=1}^\\tau \\log \\left[\n        \\frac{\n            T^\\dagger\\left(\\underline{z}_{t-1} \\mid \\underline{z}_t, \\underline{x}\\right)\n        }{\n            T\\left(\\underline{z}_t \\mid \\underline{z}_{t-1}, \\underline{x}\\right)\n        }\n    \\right]\n\\right\\rangle_{q_\\phi}.\n\\end{aligned}\n\\tag{42}\\]\nIn other words, we can interpret the Markov chain as a parametric distribution over the latent variables \\(\\underline{z}\\) that we can use to approximate the true posterior distribution \\(π_\\theta(\\underline{z} \\mid \\underline{x})\\). The unbiased estimator of the marginal likelihood is then given by\n\\[\n\\hat{π}_\\theta(\\underline{x}) =\n\\frac{\n    π_\\theta(\\underline{x}, \\underline{z}_T)\n    \\prod_{t=1}^T T^\\dagger(\\underline{z}_{t-1} \\mid \\underline{z}_t, \\underline{x})\n}{\n    q_\\phi(\\underline{z}_0 \\mid \\underline{x})\n    \\prod_{t=1}^T T(\\underline{z}_t \\mid \\underline{z}_{t-1}, \\underline{x})\n}.\n\\tag{43}\\]\nLet’s unpack this expression. When performing MCMC, we build a Markov chain\n\\[\n\\underline{z}_0 \\to \\underline{z}_1 \\to \\underline{z}_2 \\to \\cdots\n\\to \\underline{z}_T,\n\\tag{44}\\]\nwhere \\(\\underline{z}_0 \\sim q_\\phi(\\underline{z} \\mid \\underline{x})\\) is the initial position in the chain. At each step, we sample a new position from the transition kernel \\(T(\\underline{z}_{t} \\mid \\underline{z}_{t-1},\n\\underline{x})\\), which is a function of the current position and the observed data. This way, we build a Markov chain that converges to the true posterior distribution. Therefore, the denominator of Equation 43 computes the probability of the initial position of the chain \\(q_\\phi(\\underline{z}_0 \\mid\n\\underline{x})\\) times the product of the transition probabilities for each of the \\(\\tau\\) steps, \\(\\prod_{t=1}^\\tau T(\\underline{z}_{t} \\mid\n\\underline{z}_{t-1}, \\underline{x})\\).\nThe numerator of Equation 43 computes the equivalent probability, but in reverse order, i.e., starts by sampling the final position of the chain \\(\\underline{z}_T\\) from the joint distribution \\(π_\\theta(\\underline{x},\n\\underline{z}_T)\\) and then samples the previous positions of the chain \\(\\underline{z}_{T-1}, \\underline{z}_{T-2}, \\ldots, \\underline{z}_0\\) from the transition kernel run backwards, \\(T^\\dagger(\\underline{z}_{t-1} \\mid\n\\underline{z}_{t}, \\underline{x})\\). Including a Markov chain into the variational posterior approximation has the effect of trading off computational efficiency for better posterior approximation. In other words, to improve the quality of the posterior approximation we make use of arguably the most accurate family of inference methods known to date—Markov Chain Monte Carlo (MCMC)—at the cost of increased computational cost. Intuitively, our estimate of the unbiased gradient of the ELBO becomes more and more accurate the more steps we include in the Markov chain, but at the same time, the cost of computing this gradient becomes higher and higher.\n\n\n\nAnother approach to imrpove the posterior approximation is to consider the composition of simple smooth invertible transformations [7], such that a variable \\(\\underline{z}_K\\) consists of \\(K\\) of such transformations of a random variable \\(\\underline{z}_0\\), sampled from a simple distribution, i.e., \\(\\underline{z}_K\\) is built from chaining \\(K\\) simple transformations, \\(f_x^k\\), of \\(\\underline{z}_0\\),\n\\[\n\\underline{z}_K =\nf_x^K \\circ f_x^{K-1} \\circ \\cdots \\circ f_x^1(\\underline{z}_0),\n\\tag{45}\\]\nwhere \\(f_i \\circ f_j(x) = f_i(f_j(x))\\). Because we define the transformations to be smooth and invertible, the change of variables formula tells us that the density of \\(\\underline{z}_K\\) writes\n\\[\nq_\\phi(\\underline{z}_K \\mid \\underline{x}) =\nq_\\phi(\\underline{z}_0 \\mid \\underline{x}) \\prod_{k=1}^K |\\det J_{f_x^k}|^{-1},\n\\tag{46}\\]\nwhere\n\\[\nJ_{f_x^k} = \\frac{\\partial f_x^k}{\\partial \\underline{z}},\n\\tag{47}\\]\nis the Jacobian matrix of the transformation \\(f_x^k\\). In other words, the transformation of the random variable upon a composition of simple transformations is a product of the Jacobians of each transformation. Thus, if we choose a sequence of simple transformations, with a simple Jacobian matrix, we can construct a more complex distribution over the latent variables. Again, this has the effect of trading off computational efficiency for better posterior approximation, this time in the form of smooth invertible transformations of the original variational distribution. In other words, say that approximating the posterior distribution using a simple distribution, such as a Gaussian, is too simple. We can use a sequence of smooth invertible transformations to transform this simple distribution into a more complex one that can better approximate the true posterior. This again, comes at the cost of increased computational cost.\n\n\n\nAlthough seemingly unrelated to the previous two approaches, the Hamiltonian Markov Chain Monte Carlo (HMC) framework provides a way to construct a Markov chain that is informed by the target distribution we are trying to approximate. This property is exploited by the predecessor of the RHVAE framework, the Hamiltonian Variational Autoencoder that we will review in the next section. But first, let us give a brief overview of the main ideas of the HMC framework. We direct the interested reader to the excellent review [8] for a more detailed treatment of the topic.\nIn the HMC framework, a random variable \\(\\underline{z}\\) is assumed to live in an Euclidean space with a target density \\(π(\\underline{z} \\mid \\underline{x})\\) derived from a potential \\(U_{\\underline{x}}(\\underline{z})\\), such that the distribution writes\n\\[\nπ(\\underline{z} \\mid \\underline{x}) =\n\\frac{e^{-U_{\\underline{x}}(\\underline{z})}}\n{\\int d^d\\underline{z} \\, e^{-U_{\\underline{x}}(\\underline{z})}},\n\\tag{48}\\]\nwhere we define the potential energy as\n\\[\nU_{\\underline{x}}(\\underline{z}) \\equiv -\\log π(\\underline{z} \\mid \\underline{x}).\n\\tag{49}\\]\nNotice that upon substitution of this definition into Equation 48, we get the trivial equality\n\\[\nπ(\\underline{z} \\mid \\underline{x}) =\n\\frac{\\exp[-(-\\log π(\\underline{z} \\mid \\underline{x}))]}\n{\\int d^d\\underline{z} \\, \\exp[-(-\\log π(\\underline{z} \\mid \\underline{x}))]} =\n\\frac{\n    π(\\underline{z} \\mid \\underline{x})\n}{\n    \\int d^d\\underline{z} \\, π(\\underline{z} \\mid \\underline{x})\n} =\nπ(\\underline{z} \\mid \\underline{x}).\n\\tag{50}\\]\nSince it is most of the time impossible to sample directly from \\(π(\\underline{z}\n\\mid \\underline{x})\\), an independent auxiliary variable \\(\\underline{\\rho} \\in\n\\mathbb{R}^d\\) is introduced and used to sample \\(\\underline{z}\\) more efficiently. This variable—referred as the momentum—is such that:\n\\[\n\\underline{\\rho} \\sim \\mathcal{N}(\\underline{0}, \\underline{\\underline{M}}),\n\\tag{51}\\]\nwhere \\(\\underline{\\underline{M}}\\) is the so-called mass matrix. The idea behind HMC is then to work with a distribution that extends the target distribution \\(π(\\underline{z} \\mid \\underline{x})\\) to a distribution over both position and momentum variables, i.e.,\n\\[\nπ(\\underline{z}, \\underline{\\rho} \\mid \\underline{x}) =\nπ(\n    \\underline{z} \\mid \\underline{x}, \\underline{\\rho})π(\\underline{\\rho}\n    \\mid \\underline{x}) =\nπ(\\underline{z} \\mid \\underline{x})π(\\underline{\\rho}),\n\\tag{52}\\]\nwhere we assume the value of \\(\\underline{z}\\) does not directly depend on the momentum variable \\(\\underline{\\rho}\\) but only on the observed data \\(\\underline{x}\\), and that the momentum distribution \\(π(\\underline{\\rho})\\) is independent of all other variables. Using Equation 48, the density of this extended distribution can be analogously written as\n\\[\nπ(\\underline{z}, \\underline{\\rho} \\mid \\underline{x}) =\n\\frac{e^{-H_{\\underline{x}}(\\underline{z}, \\underline{\\rho})}}\n{\\displaystyle\\iint\n    d^d\\underline{z} \\,\n    d^d\\underline{\\rho} \\,\n    e^{-H_{\\underline{x}}(\\underline{z}, \\underline{\\rho})}\n}.\n\\tag{53}\\]\nwhere \\(H_{\\underline{x}}(\\underline{z}, \\underline{\\rho})\\) is the so-called Hamiltonian, defined as\n\\[\nH_{\\underline{x}}(\\underline{z}, \\underline{\\rho}) =\n-\\log π(\\underline{z}, \\underline{\\rho} \\mid \\underline{x}) =\n-\\log π(\\underline{z} \\mid \\underline{x}) - \\log π(\\underline{\\rho}).\n\\tag{54}\\]\nSubstituting \\(π(\\underline{\\rho})\\) from Equation 51 into Equation 54 gives\n\\[\n\\begin{aligned}\nH_{\\underline{x}}(\\underline{z}, \\underline{\\rho}) &=\n-\\log π(\\underline{z} \\mid \\underline{x}) +\n\\frac{1}{2}\n\\left[\\log((2π)^d|\\underline{\\underline{M}}|)\n+ \\underline{\\rho}^T \\underline{\\underline{M}}^{-1} \\underline{\\rho}\\right] \\\\\n&= U_{\\underline{x}}(\\underline{z}) + \\kappa(\\underline{\\rho}).\n\\end{aligned}\n\\tag{55}\\]\nIn physical systems, the Hamiltonian gives the total energy of a system having a position \\(\\underline{z}\\) and a momentum \\(\\underline{\\rho}\\). \\(U_{\\underline{x}}(\\underline{z})\\) is referred as the potential energy (since it is position-dependent) and \\(\\kappa(\\underline{\\rho})\\) is the kinetic energy, since it is momentum-dependent.\nThe point of writing the extended target distribution in this form is that we can take advantage of one specific desirable property of Hamiltonian dynamics: Hamiltonian flows are volume preserving [8]. In other words, the volume in phase space defined by the position and momentum variables is invariant under Hamiltonian dynamics. What this means for our purposes of sampling from a posterior distribution \\(\\pi(\\underline{z} \\mid \\underline{x})\\) is that a walker starting in some position \\(\\underline{z}\\) can “traverse” the volume of the target distribution very efficiently traveling through lines of equal probability. In this sense, the Hamiltonian dynamics can be seen as a way to explore the posterior distribution \\(π(\\underline{z} \\mid \\underline{x})\\) very efficiently. To do so, we compute the time evolution of a walker exploring the space of the position and momentum variables using Hamilton’s equations \\[\n\\begin{aligned} \\frac{d\\underline{z}}{dt} &= \\frac{\\partial\nH_{\\underline{x}}}{\\partial \\underline{\\rho}}, \\\\ \\frac{d\\underline{\\rho}}{dt} &=\n-\\frac{\\partial H_{\\underline{x}}}{\\partial \\underline{z}}. \\end{aligned}\n\\tag{56}\\]\nOne can show that for the particular choice of momentum distribution in Equation 51, these equations take the form \\[\n\\begin{aligned}\n\\frac{d\\underline{z}}{dt} &= \\underline{\\underline{M}}^{-1} \\underline{\\rho}, \\\\\n\\frac{d\\underline{\\rho}}{dt} &=\n-\\nabla_{\\underline{z}} \\log \\pi(\\underline{z} \\mid \\underline{x}),\n\\end{aligned}\n\\tag{57}\\]\nwhere \\(\\nabla_{\\underline{z}}\\) is the gradient with respect to the position variable \\(\\underline{z}\\). Unfortunately, the resulting system of partial differential equations is almost always intractable analytically. Therefore, we must use specialized numerical integrators to solve it. The most popular choice is the so-called leapfrog integrator, which is a symplectic integrator that preserves the volume of the phase space. To update the position and momentum variables, the leapfrog integrator takes three steps:\n\nA half step for the momentum:\n\n\\[\n\\underline{\\rho}\\left(t + \\frac{\\epsilon}{2}\\right) =\n\\underline{\\rho}(t) -\n\\frac{\\epsilon}{2} \\nabla_{\\underline{z}} H_{\\underline{x}}\\left(\n    \\underline{z}(t), \\underline{\\rho}(t)\n\\right).\n\\tag{58}\\]\n\nA full step for the position:\n\n\\[\n\\underline{z}\\left(t + \\epsilon\\right) =\n\\underline{z}(t) +\n\\epsilon \\nabla_{\\underline{\\rho}} H_{\\underline{x}}\\left(\n    \\underline{z}(t), \\underline{\\rho}\\left(t + \\frac{\\epsilon}{2}\\right)\n\\right).\n\\tag{59}\\]\n\nA half step for the momentum:\n\n\\[\n\\underline{\\rho}\\left(t + \\frac{\\epsilon}{2}\\right) =\n\\underline{\\rho}\\left(t + \\frac{\\epsilon}{2}\\right) -\n\\frac{\\epsilon}{2} \\nabla_{\\underline{z}} H_{\\underline{x}}\\left(\n    \\underline{z}(t + \\epsilon), \\underline{\\rho}\\left(t + \\frac{\\epsilon}{2}\\right)\n\\right),\n\\tag{60}\\]\nwhere \\(\\epsilon\\) is the so-called leapfrog step size.\nWithout going further into the details of the HMC framework, we hope the reader can imagine how using this symplectic integrator, a walker aiming to explore the posterior distribution \\(π(\\underline{z} \\mid \\underline{x})\\) can traverse the volume of the target distribution very efficiently.\nWith this conceptual background, we are now ready to review the predecessor of the RHVAE framework, the Hamiltonian Variational Autoencoder framework.\n\n\n\nGiven the dynamics defined in Equation 57, and the numerical integrator defined in Equation 58, Equation 59, and Equation 60, we can now take \\(K\\) iterations of the leapfrog integrator to sample a point \\((\\underline{z}_K, \\underline{\\rho}_K)\\) from the extended target distribution defined in Equation 52. This transition from \\((\\underline{z}_0, \\underline{\\rho}_0)\\) to \\((\\underline{z}_K,\n\\underline{\\rho}_K)\\) via the symplectic integrator can be thought of as a transformation of the form\n\\[\n\\{\\phi_{\\epsilon,\\underline{x}}^{(K)} :\n\\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}^d \\times \\mathbb{R}^d\\},\n\\tag{61}\\]\ni.e., \\(\\phi_{\\epsilon,\\underline{x}}^{(K)}\\) is a function that takes some input value \\((\\underline{z}, \\underline{\\rho})\\) and advances it \\(K\\) steps in phase space using the leapfrog integrator. The function includes \\(\\epsilon\\) to remind the step size of the integrator and \\(\\underline{x}\\) to remind its data dependence. To advance \\(\\ell\\) steps, we simply compose a one step iteration \\((\\ell=1)\\) with the \\(\\ell-1\\) function, i.e.,\n\\[\n\\phi_{\\epsilon,\\underline{x}}^{(\\ell)} =\n\\phi_{\\epsilon,\\underline{x}}^{(\\ell-1)} \\circ \\phi_{\\epsilon,\\underline{x}}^{(1)}\n\\tag{62}\\]\nBy induction, we can see that \\(\\phi_{\\epsilon,\\underline{x}}^{(\\ell)}\\) can be built by \\(\\ell\\) compositions of \\(\\phi_{\\epsilon,\\underline{x}}^{(1)}\\), defining the entire transformation as\n\\[\n\\phi_{\\epsilon,\\underline{x}}^{(K)} = \\phi_{\\epsilon,\\underline{x}}^{(1)} \\circ\n\\phi_{\\epsilon,\\underline{x}}^{(1)} \\circ \\cdots \\circ \\phi_{\\epsilon,\\underline{x}}^{(1)}.\n\\tag{63}\\]\nIt is no coincidence that this resembles the composition of functions used in the normalizing flows framework described earlier in Equation 45. We can then think of a \\(K\\)-step leapfrog integrator trajectory that takes\n\\[\n\\left(\\underline{z}_0, \\underline{\\rho}_0\\right) \\rightarrow\n\\left(\\underline{z}_K, \\underline{\\rho}_K\\right)\n\\tag{64}\\]\nas an invertible transformation formed by composing \\(K\\) one-step leapfrog integrator trajectories. The original HVAE framework [9] includes an extra step in between each leapfrog step that we now review. This step consisted of a tempering step where an initial temperature \\(β_0\\) is proposed and the momentum is decreased by a factor\n\\[\nα_k = \\sqrt{\\frac{β_{k-1}}{β_k}},\n\\tag{65}\\]\nafter each leapfrog step \\(k\\). This simply means that for step \\(k\\), the momentum is computed as\n\\[\n\\underline{\\rho}_k = α_k \\, \\underline{\\rho}_{k-1}.\n\\tag{66}\\]\nThe temperature is then updated as\n\\[\n\\sqrt{β_k} = \\left[\n    \\left(1-\\frac{1}{\\sqrt{β_0}}\\right)\\frac{k^2}{K^2} + \\frac{1}{\\sqrt{β_0}}\n\\right]^{-1}.\n\\tag{67}\\]\nThis tempering step tries to produce an effect similar to that of Annealed Importance Sampling (AIS) [10], where the temperature is decreased gradually to produce a smoother distribution that can be better approximated by the variational distribution. Thus, the transformation \\(\\mathcal{H}_{\\underline{x}}\\) used in [9] takes the form\n\\[\n\\mathcal{H}_{\\underline{x}} = g^K \\circ \\phi_{\\epsilon,\\underline{x}}^{(1)}\n\\circ g^{K-1} \\circ \\cdots \\circ \\phi_{\\epsilon,\\underline{x}}^{(1)} \\circ g^0\n\\circ \\phi_{\\epsilon,\\underline{x}}^{(1)}.\n\\tag{68}\\]\nSince each transformation is smooth and differentiable, this entire transformation is amenable to the reparameterization trick used in the variational autoencoder framework. We can then use this transformation to access an unbiased estimator of the gradient of the ELBO with respect to the variational parameters \\(\\phi\\).\nAs mentioned in Section 0.3.3, for a series of smooth invertible transformations that define a normalizing flow, the change of variables formula tells us that the density of the transformed variable writes\n\\[\nq_\\phi(\\underline{z}_K \\mid \\underline{x}) =\nq_\\phi(\\underline{z}_0 \\mid \\underline{x})\n|\\det J_{\\mathcal{H}_{\\underline{x}}}|^{-1}.\n\\tag{69}\\]\nFor our extended posterior distribution, we have that the initial point is sampled as\n\\[\n(\\underline{z}_0, \\underline{\\rho}_0) \\sim\nq_\\phi(\\underline{z}_0, \\underline{\\rho}_0 \\mid \\underline{x}) =\nq_\\phi(\\underline{z}_0 \\mid \\underline{x}) \\, q_\\phi(\\underline{\\rho}_0),\n\\tag{70}\\]\nmeaning that only the initial position is sampled from the variational distribution, while the momentum is sampled from a standard normal distribution. For the Jacobian, each step consists of the composition of two functions:\n\nThe leapfrog integrator step \\(\\phi_{\\epsilon,\\underline{x}}^{(1)}\\).\nThe tempering step \\(g^k\\).\n\nTherefore, the determinant of the Jacobian of the transformation is given by the product of the determinants of each of the steps, i.e.,\n\\[\n|\\det J_{\\mathcal{H}_{\\underline{x}}}| =\n\\prod_{k=0}^K |\\det J_{g^k}|\n|\\det J_{\\phi_{\\epsilon,\\underline{x}}^{(1)}}|.\n\\tag{71}\\]\nThe resulting variational distribution is then given by\n\\[\n\\begin{aligned}\nq_{\\phi}(\\underline{z}_K, \\underline{\\rho}_K \\mid \\underline{x}) &=\n    q_{\\phi}(\\underline{z}_0 \\mid \\underline{x})\n    q_{\\phi}(\\underline{\\rho}_0)\n    |\\det J_{\\mathcal{H}_{\\underline{x}}}|^{-1} \\\\\n    &=\n    q_{\\phi}(\\underline{z}_0 \\mid \\underline{x})\n    q_{\\phi}(\\underline{\\rho}_0)\n    \\left[\n        \\prod_{k=0}^K \\left| \\det J_{g^k} \\right|\n        \\left| \\det J_{\\phi_{\\epsilon,\\underline{x}}^{(1)}} \\right|\n    \\right]^{-1}\n\\end{aligned}\n\\tag{72}\\]\nHowever, recall that Hamiltonian flows are volume preserving. This means that the volume of the phase space is invariant under the transformation and thus the determinant of the Jacobian is equal to one, i.e.,\n\\[\n|\\det J_{\\phi_{\\epsilon,\\underline{x}}^{(1)}}| = 1.\n\\tag{73}\\]\nFor the tempering step, since \\((\\underline{z}, \\underline{\\rho}) \\rightarrow\n(\\underline{z}, \\alpha_k\\underline{\\rho})\\), the determinant of the Jacobian is given by\n\\[\n|\\det J_{g^k}| = \\alpha_k^d = \\left(\\frac{β_{k-1}}{β_k}\\right)^{\\frac{d}{2}}.\n\\tag{74}\\]\nWith this setting in hand [9] proposes an algorithm to estimate the gradient of the ELBO with respect to the variational parameters that expands the the usual procedure to includes a series of leapfrog steps and a tempering step (see Algorithm 1 in [9] for details).\n\n\n\nTo improve the representational power of the variational posterior approximation, we can make the reasonable assumption that the latent variables \\(\\underline{z}\\) live in a Riemannian manifold \\(\\mathcal{M}\\) endowed with a Riemannian metric \\(\\underline{\\underline{G}}(\\underline{z})\\). Although not entirely correct, one can think of this Riemannian metric as some sort of “position dependent scale bar” for a flat map representing a curved space. In the context of the HMC framework, this Riemannian metric can be included if we allow the momentum variable to be given by\n\\[\n\\underline{\\rho} \\sim\n\\mathcal{N}\\left(\\underline{0}, \\underline{\\underline{G}}(\\underline{z})\\right),\n\\tag{75}\\]\ni.e., the auxiliary momentum variable is no longer independent of the position variable \\(\\underline{z}\\) but rather is distributed according to a normal distribution with mean zero and covariance matrix given by the position-dependent Riemannian metric \\(\\underline{\\underline{G}}(\\underline{z})\\). The Hamiltonian governing the dynamics of the system then becomes\n\\[\nH_{\\underline{x}}^{\\mathcal{M}}(\\underline{z}, \\underline{\\rho}) =\nU_{\\underline{x}}^{\\mathcal{M}}(\\underline{z}) +\n\\kappa_{\\underline{x}}^{\\mathcal{M}}(\\underline{z}, \\underline{\\rho}),\n\\tag{76}\\]\nwhere the \\(\\mathcal{M}\\) superscript reminds us that we now consider the latent variables \\(\\underline{z}\\) living on a Riemannian manifold. Although the potential energy \\(U_{\\underline{x}}^{\\mathcal{M}}(\\underline{z})\\) is still given by Equation 49, the kinetic energy term is now position-dependent, given by\n\\[\n\\kappa_{\\underline{x}}^{\\mathcal{M}}(\\underline{z}, \\underline{\\rho}) =\n\\frac{1}{2}\n\\log((2π)^d|\\underline{\\underline{G}}(\\underline{z})|) +\n\\frac{1}{2}\n\\underline{\\rho}^T \\underline{\\underline{G}}(\\underline{z})^{-1} \\underline{\\rho}\n\\tag{77}\\]\nOur target distribution \\(\\pi(\\underline{z} \\mid \\underline{x})\\) is still given by Equation 52, but with the Hamiltonian given by Equation 76. To show that we still recover the target distribution with this change in the Hamiltonian, let us substitute Equation 76 into Equation 52. This results in\n\\[\n\\pi(\\underline{z} \\mid \\underline{x}) =\n\\frac{\n    \\displaystyle\\int d^d\\underline{\\rho} \\,\n    \\exp\\left\\{\n        -U_{\\underline{x}}^{\\mathcal{M}}(\\underline{z}) -\n        \\frac{1}{2} \\log((2π)^d|\\underline{\\underline{G}}(\\underline{z})|) -\n        \\frac{1}{2} \\underline{\\rho}^T \\underline{\\underline{G}}(\\underline{z})^{-1} \\underline{\\rho}\n    \\right\\}\n}{\n    \\displaystyle\\iint d^d\\underline{z} \\, d^d\\underline{\\rho} \\,\n    \\exp\\left\\{\n        -U_{\\underline{x}}^{\\mathcal{M}}(\\underline{z}) -\n        \\frac{1}{2} \\log((2π)^d|\\underline{\\underline{G}}(\\underline{z})|) -\n        \\frac{1}{2} \\underline{\\rho}^T \\underline{\\underline{G}}(\\underline{z})^{-1} \\underline{\\rho}\n    \\right\\}\n}.\n\\tag{78}\\]\nRearranging terms, we can see that the target distribution is given by\n\\[\n\\pi(\\underline{z} \\mid \\underline{x}) =\n\\frac{\n    e^{-U_{\\underline{x}}^{\\mathcal{M}}(\\underline{z})}\n    \\displaystyle\\int d^d\\underline{\\rho} \\,\n    \\left[\n        \\frac{\n            e^{-\\frac{1}{2}\n            \\underline{\\rho}^T\n            \\underline{\\underline{G}}(\\underline{z})^{-1}\n            \\underline{\\rho}}\n        }{\n            (2\\pi)^d \\sqrt{|\\underline{\\underline{G}}(\\underline{z})|}\n        }\n    \\right]\n}\n{\n    \\displaystyle\\int d^d\\underline{z} \\,\n    e^{-U_{\\underline{x}}^{\\mathcal{M}}(\\underline{z})}\n    \\displaystyle\\int d^d\\underline{\\rho} \\,\n    \\left[\n        \\frac{\n            e^{\n                -\\frac{1}{2}\n                \\underline{\\rho}^T\n                \\underline{\\underline{G}}(\\underline{z})^{-1}\n                \\underline{\\rho}\n            }\n        }{\n            (2\\pi)^d \\sqrt{|\\underline{\\underline{G}}(\\underline{z})|}\n        }\n    \\right]\n}.\n\\tag{79}\\]\nwhere we can recognize the terms in square brackets as the probability density function of a normal distribution. Thus, integrating out the momentum variable \\(\\underline{\\rho}\\) from the target distribution, we obtain\n\\[\n\\pi(\\underline{z} \\mid \\underline{x}) =\n\\frac{e^{-U_{\\underline{x}}^{\\mathcal{M}}(\\underline{z})}}\n{\n    \\displaystyle\\int d^d\\underline{z} \\,\n    e^{-U_{\\underline{x}}^{\\mathcal{M}}(\\underline{z})}\n},\n\\tag{80}\\]\ni.e., the correct target distribution as defined in Equation 48. Therefore, including the position-dependent Riemannian metric in the momentum variable does not change the target distribution. However, the same cannot be said for the Hamiltonian equations of motion, as we will see next.\nRecall that the Hamiltonian equations in Equation 56 require us to compute the gradient of the potential energy with respect to the position variable \\(\\underline{z}\\) and the gradient of the kinetic energy with respect to the momentum variable \\(\\underline{\\rho}\\). For the position variable, our result from Equation 57 still holds, with the only difference that the mass matrix is now given by the Riemannian metric \\(\\underline{\\underline{G}}(\\underline{z})\\). For a single entry of \\(z_i\\), dynamics are given by\n\\[\n\\frac{d z_i}{d t} =\n\\frac{\\partial H_{\\underline{x}}^{\\mathcal{M}}(\\underline{z}, \\underline{\\rho})}\n{\n    \\partial \\rho_i\n} =\n\\left(\n    \\nabla_{\\underline{z}} H_{\\underline{x}}^{\\mathcal{M}}\n    (\\underline{z}, \\underline{\\rho})\n\\right)_i =\n\\left(\n    \\underline{\\underline{G}}(\\underline{z})^{-1} \\underline{\\rho}\n\\right)_i,\n\\tag{81}\\] where \\(\\left(\\cdot\\right)_i\\) denotes the \\(i\\)-th entry of a vector.\nFor the momentum variable, we use three relatively standard results from matrix calculus:\n\nThe gradient of the inverse of a matrix function with respect to the argument is given by\n\n\\[\n\\frac{\n    \\partial \\underline{\\underline{G}}(\\underline{z})^{-1}\n}{\n    \\partial \\underline{z}\n} =\n-\\underline{\\underline{G}}(\\underline{z})^{-1}\n\\frac{d \\underline{\\underline{G}}(\\underline{z})}{d \\underline{z}}\n\\underline{\\underline{G}}(\\underline{z})^{-1}.\n\\tag{82}\\]\n\nThe gradient of the determinant of a matrix function with respect to the argument is given by\n\n\\[\n\\frac{\n    d\n}{\n    d \\underline{z}\n} \\operatorname{det} \\underline{\\underline{G}}(\\underline{z}) =\n\\operatorname{tr}\\left(\n    \\operatorname{adj}(\\underline{\\underline{G}}(\\underline{z}))\n    \\frac{d \\underline{\\underline{G}}(\\underline{z})}{d \\underline{z}}\n\\right) =\n\\operatorname{det} \\underline{\\underline{G}}(\\underline{z})\n\\operatorname{tr}\\left(\n    \\underline{\\underline{G}}(\\underline{z})^{-1}\n    \\frac{d \\underline{\\underline{G}}(\\underline{z})}{d \\underline{z}}\n\\right),\n\\tag{83}\\] where \\(\\operatorname{adj}\\) is the adjoint operator, \\(\\operatorname{tr}\\) is the trace operator, and \\(\\operatorname{det}\\) is the determinant operator.\n\nThe gradient of the logarithm of the determinant of a matrix function with respect to the argument is given by \\[  \n\\frac{d}{d \\underline{z}}\n\\log (\\operatorname{det} \\underline{\\underline{G}}(\\underline{z})) =\n\\operatorname{tr}\\left(\n\\underline{\\underline{G}}(\\underline{z})^{-1}\n\\frac{d \\underline{\\underline{G}}(\\underline{z})}{d \\underline{z}}\n\\right).\n\\tag{84}\\]\n\nGiven these results, we can now compute the dynamics of the momentum variable. For a single entry of \\(\\underline{\\rho}\\), \\(\\rho_i\\), the resulting expression is of the form\n\\[\n\\frac{d \\rho_i}{d t} =\n\\frac{\n    \\partial H_{\\underline{x}}^{\\mathcal{M}}(\\underline{z}, \\underline{\\rho})\n}{\n    \\partial z_i\n} =\n\\frac{\\partial \\ln \\pi(\\underline{z} \\mid \\underline{x})}{\\partial z_i} -\n\\frac{1}{2} \\operatorname{tr}\\left(\n    \\underline{\\underline{G}}(\\underline{z})\n    \\frac{\\partial \\underline{\\underline{G}}(\\underline{z})}{\\partial z_i}\n\\right) +\n\\frac{1}{2}\n\\underline{\\rho}^T\n\\underline{\\underline{G}}(\\underline{z})^{-1}\n\\frac{\\partial \\underline{\\underline{G}}(\\underline{z})}{\\partial z_i}\n\\underline{\\underline{G}}(\\underline{z})^{-1} \\underline{\\rho}.\n\\tag{85}\\]\nAs before, we can adapt the leapfrog integrator to this setting to produce an unbiased estimator of the gradient of the ELBO with respect to the variational parameters. However, the previously presented leapfrog integrator is no longer volume preserving in this setting due to the position-dependent Riemannian metric. Thus, we need to use a generalized version of the leapfrog integrator. This has been previously derived and [3] lists the following steps for the generalized leapfrog integrator as\n\nA half step for the momentum variable\n\n\\[\n\\underline{\\rho}\\left(t+\\frac{\\varepsilon}{2}\\right) =\n\\underline{\\rho}(t) -\n\\frac{\\varepsilon}{2}\n\\nabla_{\\underline{z}} H_{\\underline{x}}^{\\mathcal{M}}\\left(\n    \\underline{z}(t),\n    \\underline{\\rho}\\left(t+\\frac{\\varepsilon}{2}\\right)\n\\right).\n\\tag{86}\\]\n\nA full step for the position variable\n\n\\[\n\\underline{z}(t+\\varepsilon) =\n\\underline{z}(t) +\n\\frac{\\varepsilon}{2}\n\\left[\n    \\nabla_{\\underline{z}} H_{\\underline{x}}^{\\mathcal{M}}\\left(\n        \\underline{z}(t),\n        \\underline{\\rho}\\left(t+\\frac{\\varepsilon}{2}\\right)\n    \\right) +\n    \\nabla_{\\underline{\\rho}} H_{\\underline{x}}^{\\mathcal{M}}\\left(\n        \\underline{z}(t+\\varepsilon),\n        \\underline{\\rho}\\left(t+\\frac{\\varepsilon}{2}\\right)\n    \\right)\n\\right].\n\\tag{87}\\]\n\nA half step for the momentum variable\n\n\\[\n\\underline{\\rho}(t+\\varepsilon) =\n\\underline{\\rho}\\left(t+\\frac{\\varepsilon}{2}\\right) -\n\\frac{\\varepsilon}{2}\n\\nabla_{\\underline{z}} H_{\\underline{x}}^{\\mathcal{M}}\\left(\n    \\underline{z}(t+\\varepsilon),\n    \\underline{\\rho}\\left(t+\\frac{\\varepsilon}{2}\\right)\n\\right).\n\\tag{88}\\]\nSince the left hand side terms appear on the right hand side, these equations must be solved using fixed point iterations. This means that the numerical implementation of this generalized integrator iterates a step multiple times until it finds a “fixed point”, i.e., a point that, when fed to the right hand side of the equation, does not change the value of the left hand side. In practice, we set the number of iterations to a small number.\n\n\n\nThe Riemannian Hamiltonian Variational Autoencoder (RHVAE) method extends the HVAE idea to account for non-Euclidean geometry of the latent space. This means that the latent variables \\(\\underline{z}\\) live on a Riemannian manifold. This manifold is endowed with a Riemannian metric \\(\\underline{\\underline{G}}(\\underline{z})\\), that is position-dependent. We navigate through this manifold using the Hamiltonian equations of motion, utilizing the geometric information encoded in the Riemannian metric.\nAs with the HAVE, using the generalized leapfrog integrator (Equation 86, Equation 87, Equation 88) along with a tempering step creates a smooth mapping\n\\[\n(\\underline{z}_0, \\underline{\\rho}_0) \\rightarrow\n(\\underline{z}_K, \\underline{\\rho}_K),\n\\tag{89}\\]\nthat can be thought of as a kind of normalizing flow informed by the target distribution and the geometry of the latent space. Using the same procedure as the HVAE that includes a tempering step, the resulting variational distribution takes the form of Equation 72, with the only difference that the momentum variable is now position-dependent, i.e.,\n\\[\nq_{\\phi}(\\underline{z}_K, \\underline{\\rho}_K \\mid \\underline{x}) =\nq_{\\phi}(\\underline{z}_0 \\mid \\underline{x})\nq_{\\phi}(\\underline{\\rho}_0 \\mid \\underline{z}_0)\n\\left[\n    \\prod_{k=1}^{K}\n    \\left| \\det \\underline{\\underline{J}}_{g^k} \\right|\n    \\left| \\det \\underline{\\underline{J}}_{\\phi_{\\epsilon,\\underline{x}}^{(1)}} \\right|\n\\right]^{-1},\n\\tag{90}\\]\nwhere, as before, \\(\\underline{\\underline{J}}_{\\phi_{\\epsilon,\\underline{x}}^{(1)}}\\) is the Jacobian of the generalized leapfrog integrator with step size \\(\\epsilon\\), and \\(\\underline{\\underline{J}}_{g^k}\\) is the Jacobian of the tempering step. Since we established that Hamiltonian dynamics are volume preserving, the determinant of the Jacobian of the tempering step is one, i.e., \\(\\left| \\det\n\\underline{\\underline{J}}_{\\phi_{\\epsilon,\\underline{x}}^{(1)}} \\right| = 1\\). Moreover, since we are using the same type of tempering step as in the HVAE, we have the same result for the determinant of the Jacobian of the tempering step as in Equation 74. After substituting these results into Equation 90, we obtain\n\\[\nq_{\\phi}(\\underline{z}_K, \\underline{\\rho}_K \\mid \\underline{x}) =\nq_{\\phi}(\\underline{z}_0 \\mid \\underline{x})\nq_{\\phi}(\\underline{\\rho}_0 \\mid \\underline{z}_0)\n\\beta_0^{d / 2},\n\\tag{91}\\]\nwhere \\(d\\) is the dimension of the latent space. The main difference from the HVAE is that the term \\(q_{\\phi}(\\underline{\\rho}_0 \\mid \\underline{z}_0)\\) depends on the position \\(\\underline{z}_0\\) of the latent variables. To establish this dependence, we must define the functional form for the metric tensor \\(\\underline{\\underline{G}}(\\underline{z})\\).\nIn RHVAE, the metric is learned from the data. However, looking at the generalized leapfrog integrator, we see that the inverse and the determinant of the metric tensor are required. Thus, rather than defining the metric tensor directly, we define the inverse of the metric tensor \\(\\underline{\\underline{G}}(\\underline{z})^{-1}\\), and use the fact that\n\\[\n\\det \\underline{\\underline{G}}(\\underline{z}) =\n\\det \\underline{\\underline{G}}(\\underline{z})^{-1}.\n\\tag{92}\\]\nBy doing so, we do not have to invert the metric tensor for every leapfrog step. [3] proposes an inverse metric tensor of the form\n\\[\n\\underline{\\underline{G}}(\\underline{z}) =\n\\sum_{i=1}^N\n\\underline{\\underline{L}}_{\\Psi_i}\n\\underline{\\underline{L}}_{\\Psi_i}^{\\top}\n\\exp \\left(\n    -\\frac{\n        \\left\\| \\underline{z} - \\underline{\\mu}{(\\underline{x}_i)} \\right\\|_2^2\n        }{\n            T^2\n        }\n    \\right) +\n\\lambda \\underline{\\underline{\\mathbb{I}}}_l,\n\\tag{93}\\]\nwhere \\(\\underline{\\underline{L}}_{\\Psi_i}\\) is a lower triangular matrix with positive diagonal entries, \\(\\top\\) is the transpose operator, \\(\\underline{\\mu}(\\underline{x}_i)\\) is the mean of the \\(i\\)-th component of the dataset, \\(T\\) is a temperature parameter to smooth the metric tensor, \\(\\lambda\\) is a regularization parameter, and \\(\\underline{\\underline{\\mathbb{I}}}_l\\) is the \\(l \\times l\\) identity matrix. This last term with \\(\\lambda\\) is set for the metric tensor not to be zero. However, usually \\(\\lambda\\) is set to a small value, e.g., \\(10^{-3}\\). The terms \\(\\underline{\\mu}{(\\underline{x}_i)}\\) are referred to as the “centroids” and are given by the mean of the variational posterior, such that,\n\\[\nq_{\\phi}(\\underline{z}_i \\mid \\underline{x}_i) =\n\\mathcal{N}\\left(\n    \\underline{\\mu}{(\\underline{x}_i)},\n    \\underline{\\underline{\\Sigma}}(\\underline{x}_i)\n\\right).\n\\tag{94}\\]\nIntuitively, we can think of \\(\\underline{\\underline{L}}_{\\Psi_i}\\) as the triangular matrix in the Cholesky decomposition of \\(\\underline{\\underline{G}}^{-1}(\\underline{\\mu}{(\\underline{x}_i)})\\) up to a regularization factor. This matrix is learned using a neural network with parameters \\(\\Psi_i\\), mapping\n\\[\n\\underline{x}_i \\rightarrow \\underline{\\underline{L}}_{\\Psi_i}.\n\\tag{95}\\]\nThe hyperparameters \\(T\\) and \\(\\lambda\\) could be learned from the data as well, but, for simplicity, we set them to constants. We invite the reader to refer to [3] for a more detailed discussion on the training procedure.\n\n\n\n\n\nIn this section, we provide a theoretical justification for why nonlinear dimensionality reduction methods may offer advantages over linear methods when modeling fitness landscapes. We build upon the concept of a causal manifold that captures the relationship between genotypes and their fitness across multiple environments.\n\n\nThe central concept in our analysis is what we call the causal manifold \\(\\mathcal{M}_c\\)—a low-dimensional space that encodes the information necessary to predict fitness across multiple environments. Another way to think about this manifold is in a data-generating process perspective: the causal manifold is the underlying space from which the fitness data is sampled. We define two key functions:\n\nThe mapping from genotype to manifold coordinates: \\[\n\\phi: g \\in G \\to \\underline{z} \\in \\mathcal{M}_{c}.\n\\tag{96}\\]\nThe mapping from manifold coordinates to fitness in environment \\(E\\): \\[\nF_{E}: \\underline{z} \\in \\mathcal{M}_c \\to f_{E} \\in \\mathbb{R}.\n\\tag{97}\\]\n\nA critical assumption in linear approaches is that \\(F_E\\) takes the form \\[\nF_E(\\underline{z}) = \\underline{\\beta} \\cdot \\underline{z} + b,\n\\tag{98}\\]\nwhere \\(\\underline{\\beta}\\) is a vector of coefficients and \\(b\\) is a scalar offset. However, this assumption may not hold in general. While local linearity is guaranteed by Taylor’s theorem for small changes in \\(\\underline{z}\\)\n\\[\nF_E(\\underline{z} + \\Delta\\underline{z}) \\approx\nF_E({\\underline{z}}) +\n\\nabla F_E(\\underline{z}) \\cdot \\Delta \\underline{z} +\n\\mathcal{O}(\\Delta\\underline{z}^2),\n\\tag{99}\\]\nglobal linearity across the entire manifold is a much stronger constraint that may not reflect the true complexity of biological systems.\n\n\n\nApplying linear dimensionality reduction techniques such as PCA/SVD constructs a flat manifold of dimension \\(d\\) to approximate our fitness data. Through SVD, our \\(N \\times M\\) data matrix \\(\\underline{\\underline{D}}\\) (\\(N\\) being the number of environments and \\(M\\) being the number of genotypes) containing fitness profiles is factorized as\n\\[\n\\underline{\\underline{D}} =\n\\underline{\\underline{U}}\\,\n\\underline{\\underline{\\Sigma}}\\,\n\\underline{\\underline{V}}^T.\n\\tag{100}\\]\nThe coordinates of genotype \\(i\\) in this linear manifold are given by\n\\[\n\\underline{z}_i = \\left[\\begin{matrix}\n\\sigma_1 v_{i1} \\\\\n\\sigma_2 v_{i2} \\\\\n\\vdots \\\\\n\\sigma_d v_{id}\n\\end{matrix}\\right].\n\\tag{101}\\]\nThe key property of this linear approach is that Euclidean distances in the manifold directly correspond to differences in fitness profiles in the truncated space\n\\[\n\\|\\underline{z}_1 - \\underline{z}_2\\| =\n\\|\\underline{\\hat{f}}_1 - \\underline{\\hat{f}}_2\\|.\n\\tag{102}\\]\nWhile mathematically elegant and computationally tractable, this linear constraint may limit our ability to capture the true structure of the underlying phenotypic space efficiently.\n\n\n\nA critical insight comes from the Whitney Embedding Theorem, which states that any smooth \\(d'\\)-dimensional manifold can be embedded in a Euclidean space of dimension \\(2d' + 1\\). This theorem explains the “unreasonable effectiveness of linear maps” while simultaneously highlighting their limitations. If the true causal manifold has intrinsic dimension \\(d'\\) but is nonlinear, capturing its structure with a linear approximation generally requires \\(d &gt; d'\\) dimensions.\nTo formalize this intuition, we consider the causal manifold as a Riemannian manifold with a position-dependent metric tensor \\(\\underline{\\underline{G}}(\\underline{z})\\). On a nonlinear manifold, this tensor is not the identity matrix, meaning\n\\[\n\\|\\underline{z}_1 - \\underline{z}_2\\| \\neq\n\\|\\underline{\\hat{f}}_1 - \\underline{\\hat{f}}_2\\|.\n\\tag{103}\\]\nInstead, the distance between points on a Riemannian manifold is given by \\[\nd_{\\mathcal{M}_c}(\\underline{z}_1, \\underline{z}_2) =\n\\min_{\\underline{\\gamma}} \\int_0^1 dt\n\\sqrt{\n    \\underline{\\dot{\\gamma}}(t)^T \\,\n    \\underline{\\underline{G}}(\\underline{\\gamma}(t)) \\,\n    \\underline{\\dot{\\gamma}}(t)\n},\n\\tag{104}\\]\nwhere \\(\\underline{\\gamma}\\) is a parametric curve with \\(\\underline{\\gamma}(0) =\n\\underline{z}_1\\) and \\(\\underline{\\gamma}(1) = \\underline{z}_2\\). The curve that minimizes this distance is called a geodesic.\nThe key insight is that when the true causal manifold is nonlinear, approximating it with a linear method requires additional dimensions to compensate for the curvature. This is reflected in the singular value spectrum, where\n\\[\n\\|\\underline{\\hat{f}}_1 - \\underline{\\hat{f}}_2\\|^2 =\n\\sum_{j=1}^d \\sigma_j^2(v_{1j} - v_{2j})^2.\n\\tag{105}\\]\nIn this case, dimensions beyond the intrinsic dimensionality \\(d'\\) (with smaller singular values) are effectively “compensating” for the curvature of the true manifold.\n\n\n\nTo learn nonlinear manifolds directly, we employ variational autoencoders (VAEs) that can approximate the joint distribution between fitness profiles \\(\\underline{f}\\) and latent variables \\(\\underline{z}\\) representing coordinates on the causal manifold\n\\[\n\\pi(\\underline{f}, \\underline{z}) \\approx\n\\pi(\\underline{f} | \\underline{z})\\pi(\\underline{z}).\n\\tag{106}\\]\nVAEs employ two key neural networks\n\nAn encoder function that approximates the posterior distribution: \\[\n\\Phi^{(E)}: \\underline{f} \\in \\mathbb{R}^N \\rightarrow\n\\left[\\begin{matrix}\n\\underline{\\mu}^{(E)} \\\\ \\\\\n\\underline{\\log \\underline{\\sigma}^{(E)}}\n\\end{matrix}\\right] \\in \\mathbb{R}^{2d}.\n\\tag{107}\\]\nA decoder function that maps latent points to fitness profiles: \\[\n\\Psi^{(D)}:\n\\underline{z} \\in \\mathcal{M}_c \\rightarrow\n\\underline{\\mu}^{(D)} \\in \\mathbb{R}^N.\n\\tag{108}\\]\n\nFor Riemannian Hamiltonian Variational Autoencoders (RHVAEs), used throughout this work, we add a third network that explicitly learns the metric tensor \\(\\underline{\\underline{G}}(\\underline{z})\\), allowing the model to capture the geometric structure of the manifold directly.\n\n\n\nNonlinear methods offer several theoretical advantages over linear approaches:\n\nDimensionality Efficiency: Nonlinear methods can potentially represent the same information with fewer dimensions. If the true causal manifold has intrinsic dimension \\(d'\\) but is nonlinear, linear methods may require \\(d &gt; d'\\) dimensions to achieve comparable accuracy.\nGeometric Fidelity: By learning the metric tensor directly, nonlinear methods can capture the true geometric structure of the fitness landscape, including features like multiple peaks, valleys, and saddle points that may be difficult to represent in a linear space.\nPredictive Power: If the true relationship between genotype and fitness is nonlinear, nonlinear methods should provide better predictive accuracy, especially for genotypes distant from those in the training set.\nInformation Compression: By capturing the curvature of the manifold explicitly rather than through additional dimensions, nonlinear methods can provide a more compact and interpretable representation of the fitness landscape.\n\n\n\n\nWhile the theoretical advantages of nonlinear methods are clear, empirical validation is essential. We observe this advantage in practice through several metrics:\n\nReconstruction Error: Nonlinear methods achieve lower reconstruction error for the same latent dimension compared to linear methods.\nSingular Value Spectrum: The slow decay of singular values in the data matrix suggests inherent nonlinearity in the fitness landscape.\nGeneralization Performance: Nonlinear methods show better performance when predicting fitness for held-out genotypes.\n\nHowever, it’s important to note that nonlinear methods come with their own challenges:\n\nOverfitting Risk: Nonlinear methods may introduce spurious complexity when the true structure is simple.\nInterpretation Challenges: The biological meaning of nonlinear latent dimensions can be less straightforward than those from linear methods.\nTraining Complexity: Nonlinear methods typically require more data and computational resources for effective training.\nValidation Requirements: Demonstrating that a learned nonlinear structure reflects true biological relationships rather than mathematical artifacts requires careful validation.\n\nIn this work, we address these challenges through rigorous cross-validation, comparison with linear baselines, and direct analysis of the learned metric structure to ensure biological relevance of our nonlinear representations.\n\n\n\n\n\nTo rigorously evaluate whether nonlinear latent space coordinates capture more information about the underlying phenotypic state than linear projections, we implemented a cross-validation framework that tests each model’s ability to predict responses to unseen antibiotics. This approach extends the bi-cross validation methodology described by [11], adapting it for both linear (PCA/SVD) and nonlinear (VAE and RHVAE) dimensionality reduction techniques.\n\n\nThe core insight of our cross-validation approach is that if latent space coordinates genuinely capture the underlying phenotypic state of the system, they should enable accurate prediction of cellular responses to antibiotics not used in training. We test this hypothesis by systematically holding out one antibiotic at a time, learning latent space coordinates without information from that antibiotic, and then evaluating how well these coordinates predict the response to the held-out antibiotic.\n\n\n\nFor linear models, we implement the bi-cross validation method of [11]. Given our \\(IC_{50}\\) data matrix \\(\\underline{\\underline{X}} \\in \\mathbb{R}^{m\n\\times n}\\) where rows represent antibiotics and columns represent genotypes, we partition the matrix into four quadrants\n\\[\n\\underline{\\underline{X}} =\n\\begin{bmatrix}\n\\underline{\\underline{A}} & \\underline{\\underline{B}} \\\\\n\\underline{\\underline{C}} & \\underline{\\underline{D}}\n\\end{bmatrix}\n\\tag{109}\\]\nHere, \\(\\underline{\\underline{A}}\\) represents the target quadrant containing the held-out antibiotic-genotype combinations we aim to predict. Specifically, for a given held-out antibiotic \\(i\\):\n\n\\(\\underline{\\underline{A}} \\in \\mathbb{R}^{1 \\times k}\\) contains \\(IC_{50}\\) values for antibiotic \\(i\\) and the validation set genotypes (\\(k\\) genotypes)\n\\(\\underline{\\underline{B}} \\in \\mathbb{R}^{1 \\times (n-k)}\\) contains \\(IC_{50}\\) values for antibiotic \\(i\\) and the training set genotypes\n\\(\\underline{\\underline{C}} \\in \\mathbb{R}^{(m-1) \\times k}\\) contains \\(IC_{50}\\) values for all other antibiotics and the validation set genotypes\n\\(\\underline{\\underline{D}} \\in \\mathbb{R}^{(m-1) \\times (n-k)}\\) contains \\(IC_{50}\\) values for all other antibiotics and the training set genotypes\n\nThe theoretical foundation of this approach rests on the observation that for a full-rank matrix, we can estimate \\(\\underline{\\underline{A}}\\) as\n\\[\n\\underline{\\underline{A}} =\n\\underline{\\underline{B}}\\, \\underline{\\underline{D}}^+ \\underline{\\underline{C}}\n\\tag{110}\\]\nwhere \\(\\underline{\\underline{D}}^+\\) is the Moore-Penrose pseudoinverse of \\(\\underline{\\underline{D}}\\). To evaluate how well a low-rank approximation performs, we compute rank-\\(r\\) approximations of \\(\\underline{\\underline{D}}\\) using SVD:\n\nPerform SVD on \\(\\underline{\\underline{D}}\\):\n\n\\[\n\\underline{\\underline{D}} =\n\\underline{\\underline{U}}\\, \\underline{\\underline{\\Sigma}} \\,\n\\underline{\\underline{V}}^T\n\\tag{111}\\]\n\nCreate rank-\\(r\\) approximation by retaining only the top \\(r\\) singular values:\n\n\\[\n\\underline{\\underline{D}}_r = \\underline{\\underline{U}}\\,\n\\underline{\\underline{\\Sigma}}_r \\underline{\\underline{V}}^T\n\\tag{112}\\]\n\nCompute the predicted matrix:\n\n\\[\n\\underline{\\underline{\\hat{A}}}_r =\n\\underline{\\underline{B}}\\, \\underline{\\underline{D}}_r^+\n\\underline{\\underline{C}}\n\\tag{113}\\]\n\nCalculate the mean squared error (MSE) between \\(\\underline{\\underline{A}}\\) and \\(\\underline{\\underline{\\hat{A}}}_r\\):\n\n\\[\n\\text{MSE}_r =\n\\frac{1}{|\\underline{\\underline{A}}|}\\sum_{i,j}(\\underline{\\underline{A}}_{ij} -\n\\underline{\\underline{\\hat{A}}}_{r,ij})^2\n\\tag{114}\\]\nBy examining how MSE varies with rank, we can determine the minimum dimensionality needed to accurately predict the held-out antibiotic data.\n\n\n\nFor nonlinear models (VAE and RHVAE), we adapt the cross-validation approach to account for the different model architecture. The procedure for each held-out antibiotic \\(i\\) consists of two phases:\n\n\nFirst, we train a complete model (encoder + decoder) on \\(IC_{50}\\) data from all antibiotics except the held-out antibiotic \\(i\\). This yields an 85%-15% train-validation split of genotypes for robust training. Denoting the set of all antibiotics as \\(\\mathcal{A}\\) and the held-out antibiotic as \\(a_i\\), we train on the data matrix \\(\\underline{\\underline{X}}_{\\mathcal{A} \\setminus \\{a_i\\}}\\).\nFor the VAE, we maximize the evidence lower bound (ELBO):\n\\[\n\\mathcal{L}(\\theta, \\phi) =\n\\left\\langle\n    \\log p_\\theta(\\underline{x}|\\underline{z})\n\\right\\rangle_{q_\\phi(\\underline{z}|\\underline{x})} -\nD_{KL}(q_\\phi(\\underline{z}|\\underline{x}) || \\pi(\\underline{z}))\n\\tag{115}\\]\nwhere \\(\\underline{x}\\) represents the \\(\\log IC_{50}\\) values for a genotype across all antibiotics except \\(a_i\\), \\(\\underline{z}\\) is the latent representation, \\(q_\\phi(\\underline{z}|\\underline{x})\\) is the encoder, and \\(p_\\theta(\\underline{x}|\\underline{z})\\) is the decoder.\nFor the RHVAE, we similarly maximize the ELBO but with modifications to account for the Riemannian geometry of the latent space. The model learns a position-dependent metric tensor \\(\\underline{\\underline{G}}(\\underline{z})\\) along with the encoder and decoder parameters.\nAfter training, the encoder maps inputs \\(\\underline{x}\\) to latent coordinates \\(\\underline{z}\\) that capture the underlying phenotypic state without information from antibiotic \\(a_i\\).\n\n\n\nOnce the encoder is trained, we freeze its parameters—effectively fixing the latent space coordinates for each genotype. We then train a decoder-only model to predict the response to the held-out antibiotic \\(a_i\\) using a 50%-50% train-validation split of genotypes\n\\[\n\\underline{z} = \\text{Encoder}_\\phi(\\underline{x})\n\\tag{116}\\]\n\\[\n\\hat{y}_i = \\text{Decoder}_\\theta(\\underline{z})\n\\tag{117}\\]\nwhere \\(\\hat{y}_i\\) is the predicted \\(IC_{50}\\) value for antibiotic \\(a_i\\).\nThis training maximizes the likelihood of the observed \\(IC_{50}\\) values given the latent coordinates:\n\\[\n\\mathcal{L}_{\\text{decoder}} =\n\\left\\langle\n    \\log p_\\theta(y_i|\\underline{z})\n\\right\\rangle_{q_\\phi(\\underline{z}|\\underline{x})}\n\\tag{118}\\]\nFor both VAE and RHVAE, we use a 2-dimensional latent space to ensure fair comparison with linear methods. After training, we evaluate the model’s predictive performance on the validation set by computing the mean squared error between predicted and actual \\(IC_{50}\\) values.\n\n\n\n\nThe specific implementation involved several key components:\n\nData Organization: The \\(IC_{50}\\) values were organized in a 3D tensor with dimensions [antibiotics × genotypes × MCMC samples], where the MCMC samples represent posterior samples from the Bayesian inference procedure used to estimate \\(IC_{50}\\) values in [1].\nTraining Protocol: For each held-out antibiotic:\n\nThe full model (encoder + decoder) was trained for the linear mapping from all antibiotics except the held-out one to the latent space.\nThe encoder parameters were frozen, and a new decoder was trained to map from the latent space to the held-out antibiotic’s \\(IC_{50}\\) values.\nA tempering parameter \\(\\beta_0\\) was set to 0.3 to improve stability during training.\nTraining used the Adam optimizer with a learning rate of \\(10^{-3}\\).\nModels were trained for 50 epochs with a batch size of 256.\n\nModel Architecture:\n\nThe encoder consisted of a joint logarithmic encoder that maps inputs to latent space.\nThe decoder was a simple network mapping from latent space to antibiotic responses.\nFor RHVAE, the model incorporated a metric chain with position-dependent Riemannian metric.\n\nEvaluation Metrics: We assessed predictive performance using mean squared error (MSE) between actual and predicted \\(IC_{50}\\) values on the validation set.\n\n\n\n\nTo fairly compare the predictive power of linear and nonlinear dimensionality reduction techniques, we plotted the MSE for SVD at different ranks alongside the MSE for 2D VAE and 2D RHVAE models. This visualization, shown in Figure 7 in the main text, demonstrates that for all antibiotics, the 2D nonlinear latent space coordinates provide more accurate predictions than any number of linear dimensions.\nThe key finding is that the nonlinear latent spaces capture the underlying phenotypic structure of the system more effectively than linear projections, thus enabling better predictions of out-of-sample data. This validates our hypothesis that the phenotypic space of antibiotic resistance has an inherently nonlinear structure that is better represented by VAE and especially RHVAE models.\n\n\n\nIn Figure 7 in the main text, we tested the predictive power of 2D nonlinear latent space coordinates compared to linear latent space coordinates. There, we showed via a custom cross-validation scheme that the 2D latent space coordinates were more predictive of out-of-sample data than the linear latent space coordinates. Here, we repeat a similar analysis, but using 3D latent space coordinates.\nFollowing the same cross-validation scheme as in the main text, we trained a full model on all but one antibiotic (85%-15% splits for training and validation data). This generates the latent space coordinates without using any information from the antibiotic that was left out. Next, we froze the encoder parameters —equivalent to fixing the latent space coordinates—and trained a decoder-only model on the missing antibiotic data (50%-50% splits for training and validation data). Figure 5 shows the results of this analysis. We can see that other than a couple of exceptions (KM and NFLX), the 3D latent space coordinates are marginally more predictive of out-of-sample data than the 2D latent space coordinates. This suggests that the phenotypic changes associated with the experimental setup can be captured with two effective degrees of freedom.\n\n\n\n\n\n\nFigure 5: Comparison of 2D and 3D nonlinear latent space models for predicting out-of-sample antibiotic data. Reconstruction error for each missing antibiotic as a function of linear dimensions used in SVD cross-validation. Horizontal lines represent the accuracy of nonlinear models: 2D-VAE (dark blue, dashed), 2D-RHVAE (dark red, solid), 3D-VAE (dark green, dotted), and 3D-RHVAE (dark purple, dash-dotted). The 3D models show marginal improvement over their 2D counterparts for most antibiotics, suggesting that two effective degrees of freedom capture most of the relevant phenotypic variation.\n\n\n\n\n\n\n\n\nIn this section, we explore an interesting and surprising property of some of the resulting evolutionary trajectories in latent space. As detailed in the main text, we trained an RHVAE on the \\(IC_{50}\\) data from [1]. The data used to train the RHVAE consists of a matrix with eight rows—one per each antibiotic—and \\(\\approx\\) 1300 columns representing one of the lineages at some time point in the experiment. However, at no point, the RHVAE has any knowledge of what genotype or time point corresponds to any particular column in the data matrix. All it sees at every epoch is a random subset of columns of this matrix. Nevertheless, it is interesting to ask whether there is some regularity in the resulting evolutionary trajectories in the learned latent space. In other words, once the RHVAE is trained, we can map the time series corresponding to a particular lineage to the corresponding latent space trajectory. A few of these trajectories are shown in Figure 6 as gold connected points.\nOne of the unique features of the RHVAE model is the co-learning of the metric tensor in the latent space. A space endowed with such mathematical structure gives us the ability to compute the shortest path between any two points, commonly referred to as a geodesic. Geodesics are the generalization of the idea of a straight line in Euclidean space to curved spaces, and it is a fundamental concept in Riemannian geometry. Let us define this curve as \\(\\underline{\\gamma}(t)\\) where \\(t \\in [0, 1]\\). This means that the geodesic is a parametric curve \\(\\underline{\\gamma}\\) for which we only need to define the initial and final points of the curve, \\(\\underline{\\gamma}(0) = \\underline{z}_0\\) and \\(\\underline{\\gamma}(1) = \\underline{z}_1\\). What makes it a geodesic is that it minimizes the length of the curve connecting the initial and final points, i.e., it minimizes\n\\[\nL(\\underline{\\gamma}) = \\int_0^1\ndt \\,\n\\sqrt{\n    \\underline{\\dot{\\gamma}}(t)^T\n    \\underline{\\underline{G}}(\\underline{\\gamma}(t))\n    \\underline{\\dot{\\gamma}}(t)\n}\n\\tag{119}\\]\nwhere \\(\\underline{\\dot{\\gamma}}(t) = \\frac{d}{dt} \\underline{\\gamma}(t)\\) is the velocity vector of the curve and \\(\\underline{\\underline{G}}(\\underline{\\gamma}(t))\\) is the metric tensor at the point \\(\\underline{\\gamma}(t)\\). Computing this geodesic is a non-trivial task, especially for a data-derived metric tensor with no closed-form expression. Fortunately, we can leverage the generality of neural networks to parameterize the curve and compute the geodesic numerically [12]. Again, all we need to do is define the initial and final points of the curve, provide the metric tensor learned by the RHVAE, and let the neural network approximate the shortest path between the two points. Figure 6 shows the corresponding curves between the initial point (black crosses) and the final point (black triangles) as red curves for a few lineages. We highlight the fact that the curves are not straight lines in the latent space, but rather curved paths that follow the curvature of the latent space. For these few examples, we see that the geodesic curves are very similar to the gold curves, which are the actual trajectories of the lineages in the latent space. This rather surprising result suggests that the best spatial representation for the evolutionary trajectories coincides with the shortest path in the latent space.\n\n\n\n\n\n\nFigure 6: Geodesic paths in latent space follow evolutionary trajectories The figure shows the latent space representation of evolutionary trajectories from the Iwasawa et al. dataset [1]. Background coloring represents the metric volume (determinant of the metric tensor), with lighter regions indicating higher local curvature. Gold connected points show actual evolutionary trajectories of different lineages, while red curves represent the computed geodesics (shortest paths) between initial points (black crosses) and final points (black triangles). The striking similarity between geodesics and actual trajectories suggests that evolution in phenotype space tends to follow paths that minimize distance in the geometry-informed latent space.\n\n\n\nSo far, these few examples suggest that the shortest path in the latent space coincides with the actual evolutionary trajectory. Since the latent space coordinates of any data point capture information about the resistance profile of the corresponding strain, we expect that the qualitative matching between the geodesic and the actual trajectory should mean that the resulting resistance profile predicted by the geodesic is very similar to the actual experimental resistance profile. To see if this is the case, we must decode the latent space trajectory back to the original space. Doing so projects back all of the points sampled along the geodesic curve to the original data space with eight antibiotic resistance values. However, there is a catch: the geodesic curve is a smooth continuous function of time, but the resistance profile of a strain is a discrete set of values with different step sizes. In other words, when constructing the geodesic curve, we define a continuous function connecting the initial and final point with no sudden jumps in the coordinates. But there is no a priori reason that the resulting experimental curve should be smooth. This statement is not necessarily a consequence of the discrete sampling of the data with a one-day time step that could be resolved by increasing the sampling frequency, but rather a consequence of the genotype-phenotype map. Although in the simulations presented in the main text we assume that all evolutionary steps follow certain distribution of step sizes as a convenient mathematical simplification, the complexity of the genotype-phenotype map might be such that this simplification is not valid. To emphasize this point even further, we can think of the geodesic curve as a prediction of what the cross-resistance values for the different antibiotics ought to be without information about when those values must evolve in time.\nThe consequence of this is that it is not clear how to align the continuous geodesic curve with the discrete experimental data. We can, however, make use of methodologies developed for similar purposes in the form of time-warping algorithms. In this case, we can use dynamic time warping (DTW) to align the continuous geodesic curve with the discrete experimental data. DTW is a well-known algorithm in the signal processing community for aligning two time series with different step sizes. Effectively, DTW finds the optimal pairing of points between the two time series, with the constraint that the alignment is monotonic, i.e., if point \\(x_i\\) in one time series is aligned to point \\(y_j\\) in the other time series, then \\(x_{i+1}\\) is either aligned to \\(y_{j}\\) or \\(y_{j+1}\\), never stepping backwards. We use this algorithm to align the points sampled along the geodesic curve in latent space with the resulting experimental curve also in latent space. This point, although subtle, is important: the alignment is done in latent space, not in the original data space. This means that the alignment is done based on the metric tensor learned by the RHVAE, and not on the original data.\nOnce we have the aligned latent space trajectory, we can decode it back to the original data space and plot the resulting experimental curve. Figure 7 and Figure 8 show the results of this procedure for the same few lineages shown in Figure 6. On the left, we show the same latent space trajectory as in Figure 6, on the right, we show the resulting \\(IC_{50}\\) curves for each of the eight antibiotics. The gold curves show the original experimental curves, while the red curves show the geodesic-predicted curves after the dynamic time warping alignment. Although far from perfect, the alignment is remarkably good considering that the predictions were drawn from only knowing the initial and final points of the trajectory in latent space and then computing the shortest path between them. This surprising result begs for further experimental investigation along with extensive theoretical analysis of why this phenomenon occurs. One tantalizing, although highly speculative, possibility is that evolution proceeds in phenotype space following a least action principle, i.e., the path taken by the evolutionary trajectory is the one that minimizes the distance traveled in phenotype space subject to the constraints of phenotype accessibility, as modeled by our genotype-phenotype density function in the main text.\n\n\n\n\n\n\nFigure 7: Geodesic predictions of evolutionary trajectories match experimental data after time alignment. Left panel shows latent space trajectories (gold: experimental path; red: geodesic prediction) for selected lineages. Right panel shows the corresponding IC50 values for all eight antibiotics over time, comparing experimental measurements (gold) with geodesic predictions after dynamic time warping alignment (red).\n\n\n\n\n\n\n\n\n\nFigure 8: Additional examples of geodesic-based predictions of antibiotic resistance evolution. Each panel pair shows another set of lineages where geodesic paths in latent space (left, red curves) were used to predict the temporal evolution of resistance profiles (right). Despite only using initial and final points as inputs, the geodesic predictions (red) capture many features of the actual evolutionary trajectories (gold) after time alignment.\n\n\n\n\n\n\n\nIn the main text, Figure 5 shows the ability of the RHVAE to reconstruct the underlying fitness landscapes’ topography from simulated data. There, we demonstrated that the relative position, number of peaks, and overall shape of the fitness landscapes are well-reproduced by the RHVAE latent space. It is therefore interesting to show the same reconstructions using the experimental data from [1].\nAfter training an RHVAE model on the \\(IC_{50}\\) data from [1]—the same model used in the main text in Figure 6—we apply the same methodology to reconstruct the fitness landscapes for the eight antibiotics used in the experiment. Figure 9 shows the reconstructed fitness landscapes for all eight antibiotics along with the latent space metric volume. One obvious difference between the landscapes reconstructed from the simulated and experimental data is the lack of clear peaks in the experimental landscapes. Although it is possible that such peaks do not exist, it is more likely that the lack of peaks is due to the limited resolution of the experimental data, where only a few initial genotypes were measured.\n\n\n\n\n\n\nFigure 9: Reconstructed 2D antibiotic resistance landscapes from experimental data. The first eight panels show the antibiotic resistance landscape reconstructions using a 2D RHVAE trained on the \\(IC_{50}\\) data from [1]. Dotted lines show different level curves on the fitness landscape. The last panel shows the metric volume (the determinant of the metric tensor) of the latent space. The darker the color, the flatter the latent space.\n\n\n\nGiven the lack of regularity in the resulting landscapes, we also show the three-dimensional representations of the resistance landscapes for different antibiotics using experimental data from [1]. Figure 10 shows the same landscapes as in Figure 9, but using the z-axis to represent the fitness value. From this 3D perspective, the lack of clear peaks is more apparent.\n\n\n\n\n\n\nFigure 10: 3D visualization of reconstructed antibiotic resistance landscapes. Three-dimensional representations of the same resistance landscapes shown in Figure 9, using experimental data from [1]. The z-axis represents the fitness value (IC₅₀). This 3D perspective more clearly illustrates the lack of distinct peaks in the experimental data, which may be due to limited sampling of initial genotypes rather than the absence of such features in the actual fitness landscapes.\n\n\n\n\n\nGiven these reconstructed fitness landscapes, we can now explore the evolutionary trajectories of different lineages in the experimental data. In [1], the authors evolved genotypes in the presence of three different antibiotics: Tetracycline (TET), Kanamycin (KAN), and Norfloxacin (NFLX). Naively, if these fitness landscapes are representative of the underlying landscape available to the genotypes, we would expect the trajectories of these lineages to follow a noisy gradient ascent-like path in the latent space, following the fitness gradient predicted by the reconstructed landscapes. Figure 11, Figure 12, and Figure 13 show the evolutionary trajectories of the lineages evolved in the presence of Kanamycin, Tetracycline, and Norfloxacin, respectively. The left and right panels show the same trajectories in the 2D and 3D projection of the latent space, respectively. One of the problematic aspects of these trajectories is that in none of the cases do the trajectories follow a path towards the predicted highest fitness peak. We strongly suspect this is a problem with the experimental setup, rather than the entirety of our approach. Our suspicion is that the selection of initial genotypes completely biased the reconstruction of the fitness landscapes. Some of the lineages selected by the authors were previously evolved in the presence of one of these antibiotics, to then be re-evolved in the presence of the other antibiotics. In other words, there were some strains in the initial pool that were already highly resistant to, say, Kanamycin, and then these strains were re-evolved in the presence of Tetracycline. The presence of these strains explains the existence of those peaks that none of the trajectories reach, since these strains evolved for much longer in the corresponding antibiotic compared to this experimental setup. However, further investigation is needed to determine the cause of this behavior.\n\n\n\n\n\n\nFigure 11: Evolutionary trajectories of lineages evolved in Kanamycin. The left panel shows the 2D projection of evolutionary trajectories in the latent space overlaid on the Kanamycin resistance landscape. The right panel shows the same trajectories in a 3D projection. Different colors represent different lineages. Cross symbols indicate the initial point of the trajectory, while triangles indicate the final point. Note that trajectories do not follow paths toward the highest predicted fitness peaks, suggesting potential biases in the experimental setup or limitations in landscape reconstruction.\n\n\n\n\n\n\n\n\n\nFigure 12: Evolutionary trajectories of lineages evolved in Tetracycline. The left panel shows the 2D projection of evolutionary trajectories in the latent space overlaid on the Tetracycline resistance landscape. The right panel shows the same trajectories in a 3D projection. Different colors represent different lineages. Cross symbols indicate the initial point of the trajectory, while triangles indicate the final point. Similar to the Kanamycin case, trajectories do not consistently follow gradient ascent paths toward fitness peaks.\n\n\n\n\n\n\n\n\n\nFigure 13: Evolutionary trajectories of lineages evolved in Norfloxacin. The left panel shows the 2D projection of evolutionary trajectories in the latent space overlaid on the Norfloxacin resistance landscape. The right panel shows the same trajectories in a 3D projection. Different colors represent different lineages. Cross symbols indicate the initial point of the trajectory, while triangles indicate the final point. As with other antibiotics, the evolutionary paths do not align with the predicted fitness gradients, potentially due to pre-evolved strains in the initial pool biasing landscape reconstruction.\n\n\n\n\n\n\n\n\n\n\nThe neural network architecture implemented in this study is a Riemannian Hamiltonian Variational Autoencoder (RHVAE) [3]. The network was implemented using the Flux.jl framework [13] via the AutoEncoderToolkit.jl package [14], and consists of three main components: an encoder, a decoder, and a metric chain network for the Riemannian metric computations.\n\n\n\n\n\n\n\n\n\n\n\nParameter\nValue\nDescription\n\n\n\n\nLatent Dimensions\n2\nDimensionality of the latent space\n\n\nHidden Layer Size\n128\nNumber of neurons in hidden layers\n\n\nTemperature (T)\n0.8\nTemperature parameter for RHVAE\n\n\nRegularization (λ)\n0.01\nRegularization parameter\n\n\nNumber of Centroids\n256\nNumber of centroids for the manifold approximation\n\n\n\n\n\n\nThe encoder implements a joint Gaussian log-encoder with the following structure:\n\n\n\nLayer\nOutput Dimensions\nActivation Function\n\n\n\n\nInput\nn_env\\(^*\\)\n-\n\n\nDense 1\n128\nIdentity\n\n\nDense 2\n128\nLeakyReLU\n\n\nDense 3\n128\nLeakyReLU\n\n\nDense 4\n128\nLeakyReLU\n\n\nµ output\n2\nIdentity\n\n\nlogσ output\n2\nIdentity\n\n\n\n\\(^*\\) n_env is the number of environments on which fitness is determined to train the model.\n\n\n\nThe decoder implements a simple Gaussian decoder with the following structure:\n\n\n\nLayer\nOutput Dimensions\nActivation Function\n\n\n\n\nInput\n2\n-\n\n\nDense 1\n128\nIdentity\n\n\nDense 2\n128\nLeakyReLU\n\n\nDense 3\n128\nLeakyReLU\n\n\nDense 4\n128\nLeakyReLU\n\n\nOutput\nn_env\nIdentity\n\n\n\n\n\n\nThe metric chain computes the Riemannian metric tensor with the following structure:\n\n\n\nLayer\nOutput Dimensions\nActivation Function\n\n\n\n\nInput\nn_env\n-\n\n\nDense 1\n128\nIdentity\n\n\nDense 2\n128\nLeakyReLU\n\n\nDense 3\n128\nLeakyReLU\n\n\nDense 4\n128\nLeakyReLU\n\n\nDiagonal Output\n2\nIdentity\n\n\nLower Triangular Output\n1\nIdentity\n\n\n\n\n\n\nThe input data underwent the following preprocessing steps:\n\nLogarithmic transformation of fitness values.\nZ-score standardization (mean = 0, std = 1) per environment.\nK-medoids clustering to select centroids for the manifold approximation.\n\n\n\n\nThe model was implemented using:\n\nJulia programming language.\nFlux.jl for neural network architecture.\nAutoEncoderToolkit.jl for RHVAE-specific components.\nRandom seed set to 42 for reproducibility.\n\nThe complete model state and architecture were saved in JLD2 format for reproducibility and future use.\nAll of the code used to implement the model is available in the GitHub repository for this project.\n\n\n\n\nThe neural network architecture implemented in this study is a Neural Geodesic model designed to find geodesics on the latent space manifold of a Riemannian Hamiltonian Variational Autoencoder (RHVAE) [12]. The network is implemented using the Flux.jl framework and learns to parameterize geodesic curves between points in the latent space.\n\n\n\n\n\n\n\n\n\n\nParameter\nValue\nDescription\n\n\n\n\nHidden Layer Size\n32\nNumber of neurons in hidden layers\n\n\nInput Dimension\n1\nTime parameter t ∈ [0,1]\n\n\nOutput Dimension\n2\nDimensionality of the latent space\n\n\nEndpoints\nz_init=[0,0], z_end=[1,1]\nStart and end points of the geodesic\n\n\n\n\n\n\nThe neural geodesic implements a feed-forward network that maps from the time domain to the latent space with the following structure:\n\n\n\nLayer\nOutput Dimensions\nActivation Function\n\n\n\n\nInput\n1\n-\n\n\nDense 1\n32\nIdentity\n\n\nDense 2\n32\nTanH\n\n\nDense 3\n32\nTanH\n\n\nDense 4\n32\nTanH\n\n\nOutput\n2\nIdentity\n\n\n\n\n\n\n\nInput Processing\n\nTakes a single time parameter \\(t \\in [0,1]\\).\nMaps to the latent space dimension (2D in this implementation).\nDesigned to learn smooth geodesic curves.\n\nNetwork Structure\n\nFollows a deep architecture with 4 hidden layers.\nUses tanh activation for smooth curve generation.\nIdentity activation at input and output layers for unrestricted range.\n\nOutput Constraints\n\nNetwork outputs must satisfy boundary conditions:\n\n\\(\\gamma(0) = \\underline{z}_{\\text{init}}\\).\n\\(\\gamma(1) = \\underline{z}_{\\text{end}}\\).\n\nGenerated curve represents path in latent space.\n\n\n\n\n\nThe model is implemented with:\n\nJulia programming language.\nFlux.jl for neural network architecture.\nAutoEncode.diffgeo package for geodesic-specific components.\nRandom seed set to 42 for reproducibility.\n\nThe model is designed to work in conjunction with a pre-trained RHVAE model, using its latent space structure to inform the geodesic computation. The complete model state and architecture are saved in JLD2 format for reproducibility and future use.\n\n\n\nThis neural geodesic model complements the RHVAE architecture by:\n\nUsing the same latent space dimensionality.\nLearning geodesics that respect the Riemannian metric of the RHVAE\nProviding a parameterized way to interpolate between latent points\n\n\n\n\n\n\nWhen working with different dimensionality reduction techniques such as PCA, VAE, and RHVAE, the resulting latent spaces often have arbitrary orientations that make direct comparisons challenging. To facilitate meaningful comparisons between these latent representations and the ground truth phenotype space, we employed Procrustes analysis. This section details the mathematical foundations and implementation of our alignment procedure.\n\n\nProcrustes analysis finds an optimal rigid transformation (rotation, scaling, and potentially translation) that aligns one set of points with another while minimizing the sum of squared differences. Given two sets of points \\(\\underline{\\underline{X}}\\) and \\(\\underline{\\underline{Y}}\\), where each column represents a point in \\(\\mathbb{R}^d\\), the objective is to find a transformation of \\(\\underline{\\underline{X}}\\) that best aligns with \\(\\underline{\\underline{Y}}\\).\nThe Procrustes problem can be formulated as:\n\\[\n\\min_{R, s} \\|\n    \\underline{\\underline{Y}} -\n    s\\underline{\\underline{X}}\\,\\underline{\\underline{R}}\n\\|_F^2\n\\tag{120}\\]\nwhere:\n\n\\(\\underline{\\underline{R}}\\) is a \\(d \\times d\\) orthogonal rotation matrix (\\(\\underline{\\underline{R}}^T\\underline{\\underline{R}} =\n\\underline{\\underline{I}}\\))\n\\(s\\) is a scalar scaling factor\n\\(\\|\\cdot\\|_F\\) denotes the Frobenius norm\n\nIf the data is centered (mean-subtracted), the transformation involves only rotation and scaling. The solution to this optimization problem is obtained through Singular Value Decomposition (SVD).\n\n\n\nOur implementation of Procrustes analysis follows these steps:\n\nData Standardization: Each latent representation (PCA, VAE, RHVAE) and the ground truth phenotype data are standardized to have zero mean and unit standard deviation along each dimension:\n\n\\[\n\\hat{\\underline{\\underline{X}}} =\n\\frac{\n    \\underline{\\underline{X}} -\n    \\underline{\\mu}_X\n}{\n    \\underline{\\sigma}_X\n}\n\\tag{121}\\]\nwhere \\(\\underline{\\mu}_X\\) and \\(\\underline{\\sigma}_X\\) are the mean and standard deviation vectors computed across all points in the respective space.\n\nInner Product Computation: We compute the inner product matrix between the standardized ground truth phenotype data \\(\\hat{\\underline{\\underline{Y}}}\\) and each standardized latent representation \\(\\hat{\\underline{\\underline{X}}}\\):\n\n\\[\n\\underline{\\underline{A}} =\n\\hat{\\underline{\\underline{Y}}}\\,\n\\hat{\\underline{\\underline{X}}}^T\n\\tag{122}\\]\n\nSingular Value Decomposition: We perform SVD on the inner product matrix:\n\n\\[\n\\underline{\\underline{A}} =\n\\underline{\\underline{U}}\\,\n\\underline{\\underline{\\Sigma}}\\,\n\\underline{\\underline{V}}^T\n\\tag{123}\\]\nwhere \\(\\underline{\\underline{U}}\\) and \\(\\underline{\\underline{V}}\\) are orthogonal matrices, and \\(\\underline{\\underline{\\Sigma}}\\) is a diagonal matrix of singular values.\n\nRotation Matrix Computation: The optimal rotation matrix is given by:\n\n\\[\n\\underline{\\underline{R}} =\n\\underline{\\underline{U}}\\,\n\\underline{\\underline{V}}^T\n\\tag{124}\\]\n\nScaling Factor Computation: The optimal scaling factor is calculated as:\n\n\\[\ns = \\frac{\n        \\text{tr}(\\underline{\\underline{\\Sigma}})\n    }{\n        \\|\\hat{\\underline{\\underline{X}}}\\|_F^2\n    }\n\\tag{125}\\]\nwhere \\(\\text{tr}(\\underline{\\underline{\\Sigma}})\\) is the trace of \\(\\underline{\\underline{\\Sigma}}\\) (sum of singular values) and \\(\\|\\hat{\\underline{\\underline{X}}}\\|_F^2\\) is the squared Frobenius norm of \\(\\hat{\\underline{\\underline{X}}}\\).\n\nTransformation Application: Each latent representation is transformed using the corresponding rotation matrix:\n\n\\[\n\\underline{\\underline{X}}_{\\text{aligned}} =\n\\underline{\\underline{R}}\\,\\hat{\\underline{\\underline{X}}}\n\\tag{126}\\]\n\nSimilarity Metric Calculation: We compute a correlation-like measure to quantify the goodness-of-fit between the aligned spaces:\n\n\\[\n\\rho = \\frac{\n    \\text{tr}(\\underline{\\underline{\\Sigma}})\n}{\n    \\|\\hat{\\underline{\\underline{X}}}\\|_F\\|\\hat{\\underline{\\underline{Y}}}\\|_F\n}\n\\tag{127}\\]\nThis metric ranges from 0 (no similarity) to 1 (perfect alignment).\n\n\n\nThe Procrustes alignment enables several critical aspects of our analysis:\n\nDirect Visual Comparison: By aligning all latent spaces to the same orientation as the ground truth phenotype space, we can directly visualize and compare the structural preservation properties of each dimensionality reduction technique.\nQuantitative Assessment: The correlation metric \\(\\rho\\) provides a quantitative measure of how well each latent representation preserves the geometric structure of the ground truth phenotype space.\nGeometric Structure Preservation: The alignment allows us to assess how features such as local neighborhoods, relative distances, and global structure are preserved in each latent representation.\nTrajectory Analysis: For evolutionary trajectories, the alignment enables comparison of path lengths, directional changes, and convergence patterns across different representations.\n\nThis alignment procedure forms the foundation for the comparative analysis presented in the main text, where we evaluate the effectiveness of different dimensionality reduction techniques in capturing the underlying structure of the phenotype space.\n\n\n\n\n\nIn the main text, we described a simple algorithm for evolving a population as a biased random walk on a phenotypic space controlled by both a fitness function and a genotype-phenotype density function. Here, we propose an alternative algorithm that combines elements of the Metropolis-Hastings algorithm with Kimura’s classical population genetics theory.\n\n\nIn Section 0.2, we described a simple framework with a fitness function and a genotype-phenotype density function. Here, we use the same framework but introduce a biologically motivated algorithm for the evolutionary dynamics that combines elements of the Metropolis-Hastings algorithm with Kimura’s classical population genetics theory. This approach implements a two-step process that separately models:\n\nThe probability of mutation occurring (mutation accessibility) as governed by the genotype-phenotype density.\nThe probability of fixation within a population (selection) determined by the fitness effect of such mutation and the effective population size.\n\n\n\nIn natural populations, evolution proceeds through two distinct processes:\n\nMutation: New variants arise through random genetic changes. The probability of specific mutations depends on molecular mechanisms and constraints of the genotype-phenotype map.\nFixation: Once a mutation occurs, it must spread through the population to become fixed. The probability of fixation depends on the selection coefficient (fitness advantage) and the effective population size.\n\n\n\n\n\n\nThe probability of a mutation from phenotype \\(\\underline{x}\\) to \\(\\underline{x}'\\) is modeled using a Metropolis-like criterion based on the genotype-phenotype density:\n\\[\n\\pi_{\\text{mut}}(\\underline{x} \\to \\underline{x}') =\n\\min\\left(1, \\left(\\frac{GP(\\underline{x}')}{GP(\\underline{x})}\\right)^{\\beta}\\right)\n\\tag{128}\\]\nHere, \\(GP(\\underline{x})\\) represents the genotype-phenotype density at phenotype \\(\\underline{x}\\), and \\(\\beta\\) is a parameter controlling the strength of mutational constraints. The higher \\(\\beta\\) is, the less likely a mutation going downhill in genotype-phenotype density is to be accepted.\n\n\n\nOnce a mutation occurs, its probability of fixation in a population of effective size \\(N\\) is given by Kimura’s formula\n\\[\n\\pi_{\\text{fix}}(\\underline{x} \\to \\underline{x}') =\n\\frac{1 - e^{-2s}}{1 - e^{-2Ns}}\n\\tag{129}\\]\nwhere \\(s\\) is the selection coefficient. We compute this selection coefficient as the difference in fitness between the new and old phenotype divided by the fitness of the old phenotype:\n\\[\ns = \\frac{F_E(\\underline{x}') - F_E(\\underline{x})}{F_E(\\underline{x})}\n\\tag{130}\\]\nThis equation captures a fundamental result from population genetics: beneficial mutations (\\(s &gt; 0\\)) have a higher probability of fixation, with the probability approaching \\(2s\\) for small positive selection coefficients and approaching 1 for large positive selection coefficients. Deleterious mutations (\\(s &lt; 0\\)) have an exponentially decreasing probability of fixation as population size increases.\n\n\n\n\nThe overall acceptance probability—defined as the probability of a proposed step \\(\\underline{x} \\rightarrow \\underline{x}'\\)—is the product of the mutation probability and the fixation probability:\n\\[\n\\pi_{\\text{accept}}(\\underline{x} \\to \\underline{x}') =\n\\pi_{\\text{mut}}(\\underline{x} \\to \\underline{x}') \\cdot\n\\pi_{\\text{fix}}(\\underline{x} \\to \\underline{x}')\n\\tag{131}\\]\nThis two-step process better reflects the biological reality of evolution, where both mutational accessibility and selection contribute to evolutionary trajectories.\n\n\n\n\nFigure 14 shows the effect of population size parameter \\(N\\) influences evolutionary trajectories in our Metropolis-Kimura framework. Through a series of panels showing both phenotypic trajectories (upper) and fitness dynamics (lower), we can observe how different population sizes affect the balance between exploration and exploitation in phenotypic space.\nAt small population sizes (left panels), the evolutionary trajectories exhibit highly stochastic behavior, with populations taking meandering paths through the phenotypic landscape. This represents a regime where genetic drift plays a dominant role. As population size increases (moving right), the trajectories become increasingly deterministic, with populations taking more direct paths toward fitness peaks, while avoiding genotype-to-phenotype density valleys. This transition reflects a shift from drift-dominated to selection-dominated dynamics, where populations more strictly follow the local fitness gradient. The fitness trajectories in the lower panels further reinforce this pattern, showing more erratic fitness fluctuations at small population sizes and more monotonic increases at large population sizes, consistent with stronger selective pressures.\n\n\n\n\n\n\nFigure 14: Effect of population size on adaptive dynamics. Upper panels: Population trajectories on a landscape with a single fitness peak and four genotype-phenotype density peaks. Black dashed lines indicate fitness landscape contours, while white solid lines show genotype-phenotype density contours. Colored lines represent independent population trajectories, with initial positions (crosses) and final positions (triangles). Lower panels: Temporal evolution of population fitness. Panels from left to right show increasing values of population size (\\(N\\)), demonstrating increasingly deterministic adaptive trajectories.\n\n\n\n\n\n\nHaving developed this alternative algorithm for the population dynamics, we reproduce the results from the main text using this new algorithm. In particular, Figure 2, 4, and 5 in the main text used synthetic data generated by the algorithm described in Section 0.2. We now show that the same results can be obtained using the Metropolis-Kimura algorithm.\n\n\n\n\n\n\n\n\nFigure 15: Evolutionary dynamics on phenotype space with Metropolis-Kimura dynamics. (A) Top: Metropolis-like evolutionary dynamics on phenotype space. Each line represents the trajectory of a lineage as it evolves over time, with crosses and triangles denoting the initial phenotypic coordinate of a few selected lineages. Note that trajectories tend to move towards higher fitness values, avoiding low genotype-to-phenotype density regions. Bottom: Fitness over time of the same trajectories shown above. (B) The same trajectories shown in (A) overlaid on different fitness maps determined by different environments. Although the phenotypic coordinates of the genotypes remain the same (top panels), the resulting fitness readouts change as the topography of the environment-dependent fitness landscape changes (bottom panels).\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16: Geometry-informed latent space captures phenotypic structure and improves reconstruction accuracy. (A) Latent space coordinates of simulated fitness profiles colored by lineage, after alignment with the ground truth phenotype space using Procrustes analysis. The RHVAE better preserves phenotypic relationships than PCA or vanilla VAE, as evidenced by the highlighted points (diamond markers). (B) Comparison of all pairwise Euclidean distances between genotypes in the ground truth phenotypic space versus the different latent spaces, showing RHVAE better preserves the original distances. Given the large number of data points and the even larger number of pairwise comparisons, the data are shown as a smear rather than single points. (C) Reconstruction error (MSE) comparison showing 2D-RHVAE achieves accuracy comparable to a 10-dimensional PCA model. (D) Metric volume visualization of the RHVAE latent space (left) and with projected data points (right), revealing regions of high curvature that correspond to areas of low genotype-phenotype density in the simulation.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 17: Geometry-informed latent space enables accurate reconstruction of complex fitness landscapes. Comparison of ground truth and reconstructed fitness landscapes across multiple environments. The first row shows examples of the original simulated fitness landscapes used to generate the data. Subsequent rows show reconstructions using the RHVAE (second row), VAE (third row), and PCA (fourth row) models. The RHVAE accurately captures the underlying topography, including the number and relative positions of fitness peaks. While the VAE captures general landscape shapes, it lacks a geometric metric to define prediction reliability regions. PCA fails to represent the nonlinear structure of the landscapes, demonstrating the limitations of linear dimensionality reduction for complex fitness landscape reconstruction."
  },
  {
    "objectID": "supplementary.html#bayesian-inference-of-ic_50-resistance-values",
    "href": "supplementary.html#bayesian-inference-of-ic_50-resistance-values",
    "title": "Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps",
    "section": "",
    "text": "The experimental data used throughout this work was kindly provided by the first author of the excellent paper [1]. In there, the authors evolved E. coli strains on different antibiotics via an automated liquid handling platform. Here, we will detail our analysis of the raw data to infer the \\(IC_{50}\\) values of the strains on a panel of antibiotics.\n\n\nTo justify the analysis, we first need to understand the experimental design. To determine the antibiotic resistance on multiple antibiotics, the authors adopted a 96-well plate design depicted in Figure 1. On a 96-well plate, each row contained a titration of one of eight antibiotics. This format allowed measuring the optical density of the bacterial culture in each well as a function of each of the antibiotic concentrations. To evolve the strains on a particular antibiotic (e.g., tetracycline as in Figure 1), the authors propagated the strains every day by inoculating a fresh plate with the same panel of antibiotic titrations solely from the well with the highest tetracycline concentration in which there was still measurable growth the day before. In this way, the authors selected for the strains with the highest antibiotic resistance while still measuring the effects of this adaptation on the other antibiotics.\n\n\n\n\n\n\nFigure 1: Iwasawa et al. experimental design. Schematic of the experimental design used by Iwasawa et al. [1]. On a 96-well plate, each of the eight rows contained a titration of one of eight antibiotics. For strains evolved in tetracycline (TET), the plate for each day \\(t+1\\) was inoculated by propagating from the well with the highest tetracycline concentration in which there was still measurable growth on day \\(t\\) to all 96 wells.\n\n\n\nFigure 2 shows a typical progression of the optical density curves over the course of the experimental evolution. When adapted to a particular antibiotic, the optical density curves with their sigmoidal shape shifted to the right, indicating that the strains were able to grow at higher concentrations of the antibiotic.\n\n\n\n\n\n\nFigure 2: Iwasawa et al. typical experimental data. The optical density curves of the bacterial culture in the evolution antibiotic over the course of the experiment. The curves are shifted to the right, indicating that the strains were able to grow at higher concentrations of the antibiotic.\n\n\n\n\n\n\nGiven the natural sigmoidal shape of the optical density curves, we can model the optical density \\(OD(x)\\) of the bacterial culture as a function of the antibiotic concentration \\(x\\). Following the approach of [1], we model the optical density as a function of the antibiotic concentration \\(x\\) as\n\\[\nf(x) = \\frac{a}\n{1+\\exp \\left[b\\left(\\log _2 x-\\log _2 \\mathrm{IC}_{50}\\right)\\right]} + c\n\\tag{1}\\]\nwhere \\(a\\), \\(b\\), and \\(c\\) are nuisance parameters of the model, \\(\\mathrm{IC}_{50}\\) is the parameter of interest, and \\(x\\) is the antibiotic concentration. We can define a function to compute this model.\nGiven the model presented in Equation 1, and the data, our objective is to infer the value of all parameters (including the nuisance parameters). By Bayes theorem, we write \\[\n\\pi(\\mathrm{IC}_{50}, a, b, c \\mid \\text{data}) =\n\\frac{\\pi(\\text{data} \\mid \\mathrm{IC}_{50}, a, b, c)\n\\pi(\\mathrm{IC}_{50}, a, b, c)}\n{\\pi(\\text{data})},\n\\tag{2}\\]\nwhere \\(\\text{data}\\) consists of the pairs of antibiotic concentration and optical density. Let’s begin by defining the likelihood function. For simplicity, we assume each datum is independent and identically distributed (i.i.d.) and write\n\\[\n\\pi(\\text{data} \\mid \\mathrm{IC}_{50}, a, b, c) =\n\\prod_{i=1}^n \\pi(d_i \\mid \\mathrm{IC}_{50}, a, b, c),\n\\tag{3}\\]\nwhere \\(d_i = (x_i, y_i)\\) is the \\(i\\)-th pair of antibiotic concentration and optical density, respectively, and \\(n\\) is the total number of data points. We assume that our experimental measurements can be expressed as\n\\[\ny_i = f(x_i, \\mathrm{IC}_{50}, a, b, c) + \\epsilon_i,\n\\tag{4}\\]\nwhere \\(\\epsilon_i\\) is the experimental error. Furthermore, we assume that the experimental error is normally distributed, i.e.,\n\\[\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2),\n\\tag{5}\\]\nwhere \\(\\sigma^2\\) is an unknown variance parameter that must be included in our inference. Notice that we assume the same variance parameter for all data points since \\(\\sigma^2\\) is not indexed by \\(i\\).\nGiven this likelihood function, we must update our inference on the parameters as\n\\[\n\\pi(\\mathrm{IC}_{50}, a, b, c, \\sigma^2 \\mid \\text{data}) =\n\\frac{\\pi(\\text{data} \\mid \\mathrm{IC}_{50}, a, b, c, \\sigma^2)\n\\pi(\\mathrm{IC}_{50}, a, b, c, \\sigma^2)}\n{\\pi(\\text{data})},\n\\tag{6}\\]\nto include the new parameter \\(\\sigma^2\\). Our likelihood function is then of the form\n\\[\ny_i \\mid \\mathrm{IC}_{50}, a, b, c, \\sigma^2 \\sim\n\\mathcal{N}(f(x_i, \\mathrm{IC}_{50}, a, b, c), \\sigma^2).\n\\tag{7}\\]\nFor the prior, we assume that all parameters are independent and write \\[\n\\pi(\\mathrm{IC}_{50}, a, b, c, \\sigma^2) =\n\\pi(\\mathrm{IC}_{50}) \\pi(a) \\pi(b) \\pi(c) \\pi(\\sigma^2).\n\\tag{8}\\]\nLet’s detail each prior.\n\n\\(\\mathrm{IC}_{50}\\): The \\(IC_{50}\\) is a strictly positive parameter. However, we will fit for \\(\\log_2(\\mathrm{IC}_{50})\\). Thus, we will use a normal prior for \\(\\log_2(\\mathrm{IC}_{50})\\). This means we have \\[\n\\log_2(\\mathrm{IC}_{50}) \\sim\n\\mathcal{N}(\n\\mu_{\\log_2(\\mathrm{IC}_{50})}, \\sigma_{\\log_2(\\mathrm{IC}_{50})}^2\n).\n\\tag{9}\\]\n\\(a\\): This nuisance parameter scales the logistic function. Again, the natural scale for this parameter is a strictly positive real number. Thus, we will use a lognormal prior for \\(a\\). This means we have \\[\na \\sim \\text{LogNormal}(\\mu_a, \\sigma_a^2).\n\\tag{10}\\]\n\\(b\\): This parameter controls the steepness of the logistic function. Again, the natural scale for this parameter is a strictly positive real number. Thus, we will use a lognormal prior for \\(b\\). This means we have \\[\nb \\sim \\text{LogNormal}(\\mu_b, \\sigma_b^2).\n\\tag{11}\\]\n\\(c\\): This parameter controls the minimum value of the logistic function. Since this is a strictly positive real number that does not necessarily scale with the data, we will use a half-normal prior for \\(c\\). This means we have \\[\nc \\sim \\text{Half-}\\mathcal{N}(0, \\sigma_c^2).\n\\tag{12}\\]\n\\(\\sigma^2\\): This parameter controls the variance of the experimental error. Since this is a strictly positive real number that does not necessarily scale with the data, we will use a half-normal prior for \\(\\sigma^2\\). This means we have \\[\n\\sigma^2 \\sim \\text{Half-}\\mathcal{N}(0, \\sigma_{\\sigma^2}^2).\n\\tag{13}\\]\n\n\n\nTo identify and remove outliers from our optical density measurements, we implemented a residual-based outlier detection method. First, we fit the logistic model (Equation 1) to the data using a deterministic least-squares approach. We then computed the residuals between the fitted model and the experimental measurements. Points with residuals exceeding a threshold of two standard deviations from the mean residual were classified as outliers and removed from subsequent analysis. This approach helped eliminate experimental artifacts while preserving the underlying sigmoidal relationship between antibiotic concentration and optical density.\n\n\n\n\nWith this setup, we can sample the posterior distribution using Markov Chain Monte Carlo (MCMC) methods. In particular, we used the Turing.jl package [2] implementation of the No-U-Turn Sampler (NUTS). All code is available in the GitHub repository for this work. Figure 3 shows the posterior predictive checks of the posterior distribution for the \\(IC_{50}\\) parameter. Briefly, the posterior predictive checks are a visual method to assess the fit of the model to the data. The idea is to generate samples from the posterior distribution and compare them to the data. If the model is a good fit, the samples should be able to reproduce the data.\nThe effectiveness of the residual-based outlier detection method is demonstrated in Figure 3, where the posterior predictive checks show significantly tighter credible intervals after outlier removal, particularly in regions of high antibiotic concentration where experimental noise tends to be more pronounced.\n\n\n\n\n\n\nFigure 3: Posterior predictive check. The posterior predictive check of the posterior distribution for the \\(IC_{50}\\) parameter inference. The blue line is the mean of the posterior distribution, the different shades of blue represent the 95%, 68%, and 50% credible regions. The black dots are the raw optical density measurements as provided by Iwasawa et al. [1]."
  },
  {
    "objectID": "supplementary.html#sec-metropolis",
    "href": "supplementary.html#sec-metropolis",
    "title": "Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps",
    "section": "",
    "text": "In the main text, we describe a simple algorithm for evolving a population as a biased random walk on a phenotypic space controlled by both a fitness function and a genotype-phenotype density function. Here, we provide a more detailed description of the algorithm and its implementation.\n\n\nWe assume that phenotypes driving adaptation can be described by real-valued numbers. Let \\(\\underline{x}(t) = (x_1(t), x_2(t), \\cdots, x_N(t))\\) be an \\(N\\) dimensional vector describing the phenotype of a population at time \\(t\\). The fitness for a given environment \\(E\\) is a scalar function \\(F_E(\\underline{x})\\) such that\n\\[\nF_E: \\mathbb{R}^N \\to \\mathbb{R},\n\\tag{14}\\]\ni.e, the coordinates in phenotype space map to a fitness value. Throughout this work, we consider a series of Gaussian fitness peaks, i.e.,\n\\[\nF_E(\\underline{x}) =\n\\sum_{p=1}^P A_p\n\\exp\\left(\n    -\\frac{1}{2}\n    (\\underline{x} - \\underline{\\hat{x}}^{(p)})^T\n    \\Sigma_p^{-1}\n    (\\underline{x} - \\underline{\\hat{x}}^{(p)})\n\\right),\n\\tag{15}\\]\nwhere \\(P\\) is the number of peaks, \\(A_p\\) is the amplitude of the \\(p\\)-th peak, \\(\\underline{\\hat{x}}^{(p)}\\) is the \\(N\\)-dimensional vector of optimal phenotypes for the \\(p\\)-th peak, and \\(\\Sigma_p\\) is the \\(N \\times N\\) covariance matrix for the \\(p\\)-th peak. This formulation allows for multiple peaks in the fitness landscape, each potentially having different heights, widths, and covariance structures between dimensions. The covariance matrix \\(\\Sigma_p\\) captures potential interactions between different phenotypic dimensions, allowing for more complex and realistic fitness landscapes.\nIn the same vein, we define a genotype-phenotype density function \\(GP(\\underline{x})\\) as a scalar function that maps a phenotype to a mutational effect, i.e.,\n\\[\nGP: \\mathbb{R}^N \\to \\mathbb{R},\n\\tag{16}\\]\nwhere \\(GP(\\underline{x})\\) is related to the probability of a genotype mapping to a phenotype \\(\\underline{x}\\). As a particular density function we will consider a set of negative Gaussian peaks that will serve as “mutational barriers” that limit the range of phenotypes that can be reached. The deeper the peak, the less probable it is that a genotype will map to a phenotype within the peak. This idea captured by the following mutational landscape\n\\[\nGP(\\underline{x}) =\n\\sum_{p=1}^P -B_p\n\\exp\\left(\n    -\\frac{1}{2}\n    (\\underline{x} - \\underline{\\hat{x}}^{(p)})^T\n    \\Sigma_p^{-1}\n    (\\underline{x} - \\underline{\\hat{x}}^{(p)})\n\\right),\n\\tag{17}\\]\nwhere \\(B_p\\) is the depth of the \\(p\\)-th peak, and \\(\\underline{\\hat{x}}^{(p)}\\) is the \\(N\\)-dimensional vector of optimal phenotypes for the \\(p\\)-th peak.\n\n\n\nThe Metropolis-Hastings algorithm is a Markov Chain Monte Carlo (MCMC) method used to sample from a target probability distribution \\(\\pi(\\underline{x})\\). In our evolutionary context, \\(\\pi(\\underline{x})\\) can be thought of as the steady-state distribution of phenotypes under the combined influence of selection and mutation. We can define this distribution as\n\\[\n\\pi(\\underline{x}) \\propto \\exp\\left(\n    -\\beta U(\\underline{x})\n\\right),\n\\tag{18}\\]\nwhere \\(U(\\underline{x})\\) is the potential function defined as\n\\[\nU(\\underline{x}) = \\ln F_E(\\underline{x}) + \\ln GP(\\underline{x}).\n\\tag{19}\\]\nThis functional form of the potential function is motivated by the desired property of the potential function being a sum of the fitness and genotype-phenotype density functions. \\(\\beta\\) in Equation 18 is an inverse temperature parameter (from statistical mechanics) that controls the level of stochasticity in the system. A higher \\(\\beta\\) means the system is more likely to move towards higher fitness and genotype-phenotype density. In a sense, \\(\\beta\\) can be related to the effective population size, controlling the directness of the evolutionary dynamics.\nAt each step, we propose a new phenotype \\(\\underline{x}{\\prime}\\) from a proposal distribution \\(q(\\underline{x}{\\prime} | \\underline{x})\\). For simplicity, we can use a symmetric proposal distribution, such as a Gaussian centered at the current phenotype. Further, we constrain the proposal distribution to be symmetric, i.e.,\n\\[\nq(\\underline{x}{\\prime} | \\underline{x}) =\nq(\\underline{x} | \\underline{x}{\\prime}).\n\\tag{20}\\]\nThis symmetry simplifies the acceptance probability later on.\nThe acceptance probability \\(P_{\\text{accept}}\\) for moving from \\(\\underline{x}\\) to \\(\\underline{x}{\\prime}\\) is given by\n\\[\nP_{\\text{accept}} =\n\\min\\left(1,\n    \\frac{\\pi(\\underline{x}{\\prime}) q(\\underline{x} | \\underline{x}{\\prime})}\n    {\\pi(\\underline{x}) q(\\underline{x}{\\prime} | \\underline{x})}\n\\right).\n\\tag{21}\\]\nGiven the symmetry of the proposal distribution, \\(q(\\underline{x}{\\prime} |\n\\underline{x}) = q(\\underline{x} | \\underline{x}{\\prime})\\) , so the acceptance probability simplifies to\n\\[\nP_{\\text{accept}} = \\min\\left(\n    1,\n    \\frac{\\pi(\\underline{x}{\\prime})}{\\pi(\\underline{x})}\n    \\right).\n\\tag{22}\\]\nSubstituting Equation 18 and Equation 19 into Equation 22, we get:\n\\[\nP_{\\text{accept}} = \\min\\left(\n    1,\n    e^{\\beta [\\ln F_E(\\underline{x}{\\prime}) + \\ln M(\\underline{x}{\\prime})] -\n    \\beta [\\ln F_E(\\underline{x}) + \\ln M(\\underline{x})]}\n    \\right).\n\\tag{23}\\]\nThis can be rewritten as\n\\[\nP_{\\text{accept}} = \\min\\left(\n    1,\n    \\frac{F_E(\\underline{x}{\\prime})^β GP(\\underline{x}{\\prime})^β}\n    {F_E(\\underline{x})^β GP(\\underline{x})^β}\n\\right),\n\\tag{24}\\]\nThis expression shows that the acceptance probability depends on the difference in fitness and genotype-phenotype density between the current and proposed phenotypes.\n\n\n\nFigure 4 shows the effect of the inverse temperature parameter \\(\\beta\\) influences evolutionary trajectories in our Metropolis-Hastings framework. Through a series of panels showing both phenotypic trajectories (upper) and fitness dynamics (lower), we can observe how different \\(\\beta\\) values affect the balance between exploration and exploitation in phenotypic space.\nAt low \\(\\beta\\) values (left panels), the evolutionary trajectories exhibit highly stochastic behavior, with populations taking meandering paths through the phenotypic landscape. This represents a regime where genetic drift plays a dominant role. As \\(\\beta\\) increases (moving right), the trajectories become increasingly deterministic, with populations taking more direct paths toward fitness peaks, while avoiding genotype-to-phenotype density valleys. This transition reflects a shift from drift-dominated to selection-dominated dynamics, where populations more strictly follow the local fitness gradient. The fitness trajectories in the lower panels further reinforce this pattern, showing more erratic fitness fluctuations at low \\(\\beta\\) and more monotonic increases at high \\(\\beta\\), consistent with stronger selective pressures.\n\n\n\n\n\n\nFigure 4: Effect of inverse temperature on adaptive dynamics. Upper panels: Population trajectories on a landscape with a single fitness peak and four genotype-phenotype density peaks. Black dashed lines indicate fitness landscape contours, while white solid lines show genotype-phenotype density contours. Colored lines represent independent population trajectories, with initial positions (crosses) and final positions (triangles). Lower panels: Temporal evolution of population fitness. Panels from left to right show increasing values of inverse temperature (\\(\\beta\\)), demonstrating increasingly deterministic adaptive trajectories."
  },
  {
    "objectID": "supplementary.html#riemannian-hamiltonian-variational-autoencoders-mathematical-background",
    "href": "supplementary.html#riemannian-hamiltonian-variational-autoencoders-mathematical-background",
    "title": "Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps",
    "section": "",
    "text": "The bulk of this work makes use of the variational autoencoder variation developed by [3] known as Riemannian Hamiltonian Variational Autoencoder (RHVAE). Understanding this model requires some background that we provide in the following sections. It is not necessary to follow every mathematical derivation in detail for these sections, as they don’t directly pertain to the main contribution of this work. However, we believe that some intuition behind some of the concepts that inspired the RHVAE model can go a long way to understand the model. We invite the reader to refer to the original paper [3] and the references therein for additional details.\nLet us start by reviewing the variational autoencoder model setting and the variational inference framework.\n\n\nGiven a data set \\(\\underline{x}^{1:N} \\in \\mathcal{X} \\subseteq \\mathbb{R}^D\\), where the superscript denotes a set of \\(N\\) observations, i.e., \\[\n\\underline{x}^{1:N} =\n\\left\\{\n    \\underline{x}_1, \\underline{x}_2, \\ldots, \\underline{x}_N\n\\right\\},\n\\tag{25}\\] where \\(\\underline{x}_i \\in \\mathbb{R}^D\\) for \\(i = 1, 2, \\ldots, N\\), the variational autoencoder aims to fit a joint distribution \\(π_\\theta(\\underline{x}, \\underline{z})\\) where the data generative process is assumed to involve a set of latent—unobserved—continuous variables \\(\\underline{z} \\in \\mathcal{Z} \\subseteq \\mathbb{R}^d\\), where usually \\(d \\ll D\\). In other words, the VAE assumes that the data we get to observe is a consequence of a set of hidden variables \\(\\underline{z}\\) that we cannot measure directly. We assume that these variables live in a much lower-dimensional space than the one we observe and we are trying to learn this joint distribution such that, if desired, we can generate new data points by sampling from this distribution. Since the data we observe is a “consequence” of the latent variables, we express this joint distribution as\n\\[\nπ_\\theta(\\underline{x}, \\underline{z}) =\nπ_\\theta(\\underline{x}|\\underline{z}) \\, π_\\text{prior}(\\underline{z}),\n\\tag{26}\\]\nwhere \\(π_\\text{prior}(\\underline{z})\\) is the prior distribution over latent variables and \\(π_\\theta(\\underline{x}|\\underline{z})\\) is the likelihood of the data given the latent variables. This looks exactly as the problem one confronts when performing Bayesian inference. What distinguishes the VAE from other Bayesian inference problems, as we will see, is that in many cases, we do not know the functional form of both the prior and the likelihood function that generated our data, but we have enough data samples that we hope to be able to learn these function using the neural networks function approximation capabilities. Thus, the objective of the VAE is to parameterize this likelihood function via a neural network with parameters \\(\\theta\\) given some prior distribution over the latent variables. For simplicity, it is common to choose a standard normal prior distribution, i.e., \\(π_\\text{prior}(\\underline{z}) =\n\\mathcal{N}(\\underline{0}, \\underline{\\underline{I}}_d)\\), where \\(\\underline{\\underline{I}}_d\\) is the \\(d\\)-dimensional identity matrix [4], [5]. This way, once we have learned the parameters of the neural network, we can generate new data points by sampling from this prior distribution and running the resulting samples through the likelihood function encoded by this neural network.\nOur objective then becomes to find the parameters \\(\\theta\\) that maximize the probability of observing the data we actually observed. This is equivalent to maximizing the so-called marginal likelihood of the data, i.e.,\n\\[\n\\max_\\theta \\, π_\\theta(\\underline{x}) =\n\\max_\\theta \\int d^d\\underline{z} \\, π_\\theta(\\underline{x}, \\underline{z}).\n\\tag{27}\\]\nBut there are two problems with this objective:\n\nThe marginalization over the latent variables is intractable for most interesting distributions.\nThe distribution \\(π_\\theta(\\underline{x})\\) is in general unknown and we need to somehow approximate it.\n\nThe way around these problems is to introduce a family of parametric distributions \\(q_\\phi(\\underline{z} \\mid \\underline{x})\\), i.e., distributions that can be completely characterized by a set of parameters (think of Gaussians being a parametric family on the mean and the variance), and to approximate the true marginal distribution with one of these parametric distributions. This conceptual change is at the core of approximate methods for Bayesian inference known as variational inference. The objective then becomes to find the parameters \\(\\phi\\) that, when used to approximate the posterior distribution \\(π_\\theta(\\underline{z}|\\underline{x})\\), it is as close as possible to this posterior distribution. To mathematize this objective, we establish that we want to minimize the Kullback-Leibler divergence (also known as the relative entropy) between the variational distribution \\(q_\\phi(\\underline{z} \\mid \\underline{x})\\) and the posterior distribution \\(π_\\theta(\\underline{z} \\mid \\underline{x})\\), i.e., we aim to find \\(q_\\phi^*\\) such that\n\\[\nq_\\phi^*(\\underline{z} \\mid \\underline{x}) =\n\\min_\\phi \\, D_{KL}\\left(\n    q_\\phi(\\underline{z} \\mid \\underline{x}) \\|\n    π_\\theta(\\underline{z} \\mid \\underline{x})\n\\right).\n\\tag{28}\\]\nAfter some algebraic manipulation involving the well-known Gibbs inequality, we can show that this objective is equivalent to maximizing the so-called evidence lower bound (ELBO), \\(\\mathcal{L}\\), of the marginal likelihood, i.e.,\n\\[\n\\mathcal{L} = \\left\\langle\n    \\log π_\\theta(\\underline{x}, \\underline{z}) -\n    \\log q_\\phi(\\underline{z} \\mid \\underline{x})\n\\right\\rangle_{q_\\phi}\n\\leq \\log π_\\theta(\\underline{x}).\n\\tag{29}\\]\nThus, the optimization problem we want to solve invovles estimating the ELBO given some data \\(\\underline{x}\\). However, the ELBO is an expectation over the variational distribution \\(q_\\phi(\\underline{z} \\mid \\underline{x})\\), which is unknown. Therefore, we need to somehow estimate this expectation. Arguably, the most important contribution of the VAE framework is the introduction of the reparametrization trick [4]. This seemingly innocuous trick allows us to compute an unbiased estimate of the ELBO and its gradient, such that we can maximize it via gradient ascent. For more details on the reparametrization trick, we refer the reader to [4].\nMore recent attempts have focused on modifying \\(q_\\phi(\\underline{z} \\mid\n\\underline{x})\\) to get a better approximation of the true posterior \\(π_\\theta(\\underline{z} \\mid \\underline{x})\\). The basis of the Riemannian Hamiltonian Variational Autoencoder (RHVAE) framework takes inspiration from some of these approaches. Let’s gain some intuition on each of them.\n\n\n\nOne such approach to improve the accuracy of the variational posterior approximation consists in adding a fix number of Markov Chain Monte Carlo (MCMC) steps to the variational posterior approximation, targeting the true posterior [6]. In MCMC, rather than optimizing the parameters of a parametric distribution, we sample an initial position \\(\\underline{z}_0\\) from a simple distribution \\(q(\\underline{z}_0)\\) or \\(q(\\underline{z}_0 \\mid \\underline{x})\\) and subsequently, apply a stochastic transition operator \\(T(\\underline{z}_{t+1} \\mid\n\\underline{z}_t)\\) to draw a new value \\(\\underline{z}_{t+1}\\) from the distribution\n\\[\n\\underline{z}_{t + 1} \\sim T(\\underline{z}_{t+1} \\mid \\underline{z}_t).\n\\tag{30}\\]\nApplying this operator over and over again, we build a Markov chain\n\\[\n\\underline{z}_0 \\to \\underline{z}_1 \\to \\underline{z}_2 \\to \\cdots\n\\to \\underline{z}_\\tau,\n\\tag{31}\\]\nwhose long-term behavior—the so-called stationary distribution—is a very good approximation of the true posterior distribution \\(π_\\theta(\\underline{z}\n\\mid \\underline{x})\\). In other words, by carefully engineering the transition operator \\(T(\\underline{z}_{t+1} \\mid \\underline{z}_t)\\), and applying it over and over again, the “histogram” of the samples \\(\\{\\underline{z}_t\\}_{t=0}^\\tau\\) will converge to the true posterior distribution \\(π_\\theta(\\underline{z} \\mid\n\\underline{x})\\).\nThe central idea to combine MCMC with variational inference is to interpret the Markov chain\n\\[\nq_\\phi(\\underline{z} | \\underline{x}) =\nq_\\phi(\\underline{z}_0 \\mid \\underline{x})\n\\prod_{t=1}^\\tau T(\\underline{z}_t \\mid \\underline{z}_{t-1}, \\underline{x})\n\\tag{32}\\]\nas a variational approximation in an expanded space of variables that includes the original latent variables \\(\\underline{z}\\) and the auxiliary variables \\(\\underline{Z} = \\{\\underline{z}_0, \\underline{z}_1, \\ldots,\n\\underline{z}_{\\tau-1}\\}\\). Note that \\(\\underline{Z}\\) reaches up to \\(\\tau-1\\) steps, since we consider the final step \\(\\underline{z}_\\tau\\) to be the original latent variable \\(\\underline{z}\\). In other words, our variational distribution now writes \\[\nq_\\phi(\\underline{z}_\\tau, \\underline{Z} \\mid \\underline{x}) =\nq_\\phi(\\underline{z}_0 \\mid \\underline{x})\n\\prod_{t=1}^\\tau T(\\underline{z}_t \\mid \\underline{z}_{t-1}, \\underline{x}).\n\\tag{33}\\]\nIntegrating this expression into the ELBO (Equation 29) consists of substituting the variational distribution \\(q_\\phi(\\underline{z} \\mid \\underline{x})\\) with the expression in Equation 33, i.e., \\[\n\\mathcal{L}_{\\text {aux }} =\n\\left\\langle\\log\n    \\left[\n        \\pi_\\theta\\left(\\underline{x}, \\underline{z}_T\\right)\n        r\\left(\\underline{Z} \\mid \\underline{z}_T, \\underline{x}\\right)\n    \\right] -\n    \\log q\\left(\\underline{Z}, \\underline{z}_T \\mid \\underline{x}\\right)\n\\right\\rangle_{q\\left(\\underline{Z}, \\underline{z}_T \\mid \\underline{x}\\right)}\n\\tag{34}\\]\nwhere \\(r(\\underline{Z} \\mid \\underline{z}_T, \\underline{x})\\) is an auxiliary inference distribution that we can choose freely. This auxiliary distribution is necessary to ensure we account for the auxiliary variables \\(\\underline{Z}\\) that are now part of our variational distribution. By splitting the joint distribution in Equation 33 as\n\\[\nq_\\phi(\\underline{z}_\\tau, \\underline{Z} \\mid \\underline{x}) =\nq_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x}) q_\\phi(\\underline{z}_\\tau \\mid \\underline{x}),\n\\tag{35}\\]\nand substituting this expression into Equation 34, we get\n\\[\n\\mathcal{L}_{\\text {aux }} =\n\\left\\langle\\log\n    \\left[\n        \\pi_\\theta\\left(\\underline{x}, \\underline{z}_T\\right)\n        r\\left(\\underline{Z} \\mid \\underline{z}_T, \\underline{x}\\right)\n    \\right] -\n    \\log \\left[q_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x}) q_\\phi(\\underline{z}_\\tau \\mid \\underline{x})\\right]\n\\right\\rangle_{q_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x}) q_\\phi(\\underline{z}_\\tau \\mid \\underline{x})}.\n\\tag{36}\\]\nRearranging the terms, we get \\[\n\\begin{aligned}\n\\mathcal{L}_{\\text {aux }}\n& = \\left\\langle\n    \\log \\frac{\\pi_\\theta\\left(\\underline{x}, \\underline{z}_\\tau\\right)}{q_\\phi(\\underline{z}_\\tau \\mid \\underline{x})} -\n    \\log \\frac{q_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})}{r(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})}\n\\right\\rangle_{\n    q_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})\n    q_\\phi(\\underline{z}_\\tau \\mid \\underline{x})\n}, \\\\\n& = \\left\\langle\n    \\log \\frac{\\pi_\\theta\\left(\\underline{x}, \\underline{z}_\\tau\\right)}{q_\\phi(\\underline{z}_\\tau \\mid \\underline{x})}\n\\right\\rangle_{\n    q_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})\n    q_\\phi(\\underline{z}_\\tau \\mid \\underline{x})\n} -\n\\left\\langle\n    \\log \\frac{q_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})}{r(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})}\n\\right\\rangle_{\n    q_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})\n    q_\\phi(\\underline{z}_\\tau \\mid \\underline{x})\n}.\n\\end{aligned}\n\\tag{37}\\]\nFrom here, we note that the first term in Equation 37 is the original ELBO with the extra expectation taken over the auxiliary variables \\(\\underline{Z}\\). However, this term does not depend on the auxiliary variables \\(\\underline{Z}\\), thus, we can take it out of the expectation, i.e.,\n\\[\n\\mathcal{L}_{\\text {aux }} = \\mathcal{L} -\n\\left\\langle\n    \\log \\frac{q_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})}{r(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})}\n\\right\\rangle_{\n    q_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})\n    q_\\phi(\\underline{z}_\\tau \\mid \\underline{x})\n}.\n\\tag{38}\\]\nFor the second term, taking the expectation over the auxiliary variables makes it equivalent to the KL divergence between the variational distribution \\(q_\\phi(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})\\) and the auxiliary distribution \\(r(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x})\\), i.e.,\n\\[\n\\mathcal{L}_{\\text {aux }} = \\mathcal{L} -\n\\left\\langle\n    D_{K L}\\left[\n        q\\left(\\underline{Z} \\mid \\underline{z}_T, \\underline{x}\\right) \\|\n        r\\left(\\underline{Z} \\mid \\underline{z}_T, \\underline{x}\\right)\n    \\right]\n\\right\\rangle_{q\\left(\\underline{z}_T \\mid \\underline{x}\\right)}\n\\leq \\mathcal{L} \\leq \\log [\\pi_\\theta(\\underline{x})].\n\\tag{39}\\]\nwhere the inequality follows from the non-negativity of the KL divergence and the fact that the ELBO is a lower bound to the marginal likelihood. Since the variational distribution \\(q_\\phi(\\underline{z}_\\tau,\\mid \\underline{x})\\) we care about comes from the marginalization of the variational distribution over the auxiliary variables \\(\\underline{Z}\\),\n\\[\nq_\\phi(\\underline{z}_\\tau \\mid \\underline{x}) =\n\\int d\\underline{Z} \\,\nq_\\phi(\\underline{Z}, \\underline{z}_\\tau \\mid \\underline{x}),\n\\tag{40}\\]\nit is now a very rich family of distributions that can be used to better approximate the true posterior distribution \\(π_\\theta(\\underline{z} \\mid\n\\underline{x})\\). However, we are still left with the task of choosing the auxiliary distribution \\(r(\\underline{Z} \\mid \\underline{z}_\\tau,\n\\underline{x})\\). What Salimans & Kingma [6] propose is to define this distribution also as a Markov chain, but in reverse order, i.e.,\n\\[\nr(\\underline{Z} \\mid \\underline{z}_\\tau, \\underline{x}) =\n\\prod_{t=1}^\\tau\nT^\\dagger(\\underline{z}_{t-1} \\mid \\underline{z}_t, \\underline{x}),\n\\tag{41}\\]\nwhere \\(T^\\dagger(\\underline{z}_{t-1} \\mid \\underline{z}_t, \\underline{x})\\) is the reverse of the transition operator \\(T(\\underline{z}_{t+1} \\mid\n\\underline{z}_t, \\underline{x})\\). Notice that the dependence on \\(\\underline{z}_\\tau\\) is dropped.\nThis choice of the auxiliary distribution leads to an upper bound on the marginal log likelihood, of the form\n\\[\n\\begin{aligned}\n\\log \\pi_\\theta(\\underline{x}) & \\geq\n\\left\\langle\n    \\log \\pi_\\theta\\left(\\underline{x}, \\underline{z}_\\tau\\right) -\n    \\log q_\\phi\\left(\n        \\underline{z}_0, \\ldots, \\underline{z}_\\tau \\mid \\underline{x}\n    \\right) +\n    \\log r\\left(\n        \\underline{z}_0, \\ldots, \\underline{z}_{\\tau-1} \\mid \\underline{x}, \\underline{z}_\\tau\n    \\right)\n\\right\\rangle_{q_\\phi} \\\\\n& = \\left\\langle\n    \\log \\left[\\frac{\n        \\pi_\\theta\\left(\\underline{x}, \\underline{z}_\\tau\\right)\n        }{\n            q_\\phi\\left(\\underline{z}_0 \\mid \\underline{x}\\right)\n        }\n    \\right] +\n    \\sum_{t=1}^\\tau \\log \\left[\n        \\frac{\n            T^\\dagger\\left(\\underline{z}_{t-1} \\mid \\underline{z}_t, \\underline{x}\\right)\n        }{\n            T\\left(\\underline{z}_t \\mid \\underline{z}_{t-1}, \\underline{x}\\right)\n        }\n    \\right]\n\\right\\rangle_{q_\\phi}.\n\\end{aligned}\n\\tag{42}\\]\nIn other words, we can interpret the Markov chain as a parametric distribution over the latent variables \\(\\underline{z}\\) that we can use to approximate the true posterior distribution \\(π_\\theta(\\underline{z} \\mid \\underline{x})\\). The unbiased estimator of the marginal likelihood is then given by\n\\[\n\\hat{π}_\\theta(\\underline{x}) =\n\\frac{\n    π_\\theta(\\underline{x}, \\underline{z}_T)\n    \\prod_{t=1}^T T^\\dagger(\\underline{z}_{t-1} \\mid \\underline{z}_t, \\underline{x})\n}{\n    q_\\phi(\\underline{z}_0 \\mid \\underline{x})\n    \\prod_{t=1}^T T(\\underline{z}_t \\mid \\underline{z}_{t-1}, \\underline{x})\n}.\n\\tag{43}\\]\nLet’s unpack this expression. When performing MCMC, we build a Markov chain\n\\[\n\\underline{z}_0 \\to \\underline{z}_1 \\to \\underline{z}_2 \\to \\cdots\n\\to \\underline{z}_T,\n\\tag{44}\\]\nwhere \\(\\underline{z}_0 \\sim q_\\phi(\\underline{z} \\mid \\underline{x})\\) is the initial position in the chain. At each step, we sample a new position from the transition kernel \\(T(\\underline{z}_{t} \\mid \\underline{z}_{t-1},\n\\underline{x})\\), which is a function of the current position and the observed data. This way, we build a Markov chain that converges to the true posterior distribution. Therefore, the denominator of Equation 43 computes the probability of the initial position of the chain \\(q_\\phi(\\underline{z}_0 \\mid\n\\underline{x})\\) times the product of the transition probabilities for each of the \\(\\tau\\) steps, \\(\\prod_{t=1}^\\tau T(\\underline{z}_{t} \\mid\n\\underline{z}_{t-1}, \\underline{x})\\).\nThe numerator of Equation 43 computes the equivalent probability, but in reverse order, i.e., starts by sampling the final position of the chain \\(\\underline{z}_T\\) from the joint distribution \\(π_\\theta(\\underline{x},\n\\underline{z}_T)\\) and then samples the previous positions of the chain \\(\\underline{z}_{T-1}, \\underline{z}_{T-2}, \\ldots, \\underline{z}_0\\) from the transition kernel run backwards, \\(T^\\dagger(\\underline{z}_{t-1} \\mid\n\\underline{z}_{t}, \\underline{x})\\). Including a Markov chain into the variational posterior approximation has the effect of trading off computational efficiency for better posterior approximation. In other words, to improve the quality of the posterior approximation we make use of arguably the most accurate family of inference methods known to date—Markov Chain Monte Carlo (MCMC)—at the cost of increased computational cost. Intuitively, our estimate of the unbiased gradient of the ELBO becomes more and more accurate the more steps we include in the Markov chain, but at the same time, the cost of computing this gradient becomes higher and higher.\n\n\n\nAnother approach to imrpove the posterior approximation is to consider the composition of simple smooth invertible transformations [7], such that a variable \\(\\underline{z}_K\\) consists of \\(K\\) of such transformations of a random variable \\(\\underline{z}_0\\), sampled from a simple distribution, i.e., \\(\\underline{z}_K\\) is built from chaining \\(K\\) simple transformations, \\(f_x^k\\), of \\(\\underline{z}_0\\),\n\\[\n\\underline{z}_K =\nf_x^K \\circ f_x^{K-1} \\circ \\cdots \\circ f_x^1(\\underline{z}_0),\n\\tag{45}\\]\nwhere \\(f_i \\circ f_j(x) = f_i(f_j(x))\\). Because we define the transformations to be smooth and invertible, the change of variables formula tells us that the density of \\(\\underline{z}_K\\) writes\n\\[\nq_\\phi(\\underline{z}_K \\mid \\underline{x}) =\nq_\\phi(\\underline{z}_0 \\mid \\underline{x}) \\prod_{k=1}^K |\\det J_{f_x^k}|^{-1},\n\\tag{46}\\]\nwhere\n\\[\nJ_{f_x^k} = \\frac{\\partial f_x^k}{\\partial \\underline{z}},\n\\tag{47}\\]\nis the Jacobian matrix of the transformation \\(f_x^k\\). In other words, the transformation of the random variable upon a composition of simple transformations is a product of the Jacobians of each transformation. Thus, if we choose a sequence of simple transformations, with a simple Jacobian matrix, we can construct a more complex distribution over the latent variables. Again, this has the effect of trading off computational efficiency for better posterior approximation, this time in the form of smooth invertible transformations of the original variational distribution. In other words, say that approximating the posterior distribution using a simple distribution, such as a Gaussian, is too simple. We can use a sequence of smooth invertible transformations to transform this simple distribution into a more complex one that can better approximate the true posterior. This again, comes at the cost of increased computational cost.\n\n\n\nAlthough seemingly unrelated to the previous two approaches, the Hamiltonian Markov Chain Monte Carlo (HMC) framework provides a way to construct a Markov chain that is informed by the target distribution we are trying to approximate. This property is exploited by the predecessor of the RHVAE framework, the Hamiltonian Variational Autoencoder that we will review in the next section. But first, let us give a brief overview of the main ideas of the HMC framework. We direct the interested reader to the excellent review [8] for a more detailed treatment of the topic.\nIn the HMC framework, a random variable \\(\\underline{z}\\) is assumed to live in an Euclidean space with a target density \\(π(\\underline{z} \\mid \\underline{x})\\) derived from a potential \\(U_{\\underline{x}}(\\underline{z})\\), such that the distribution writes\n\\[\nπ(\\underline{z} \\mid \\underline{x}) =\n\\frac{e^{-U_{\\underline{x}}(\\underline{z})}}\n{\\int d^d\\underline{z} \\, e^{-U_{\\underline{x}}(\\underline{z})}},\n\\tag{48}\\]\nwhere we define the potential energy as\n\\[\nU_{\\underline{x}}(\\underline{z}) \\equiv -\\log π(\\underline{z} \\mid \\underline{x}).\n\\tag{49}\\]\nNotice that upon substitution of this definition into Equation 48, we get the trivial equality\n\\[\nπ(\\underline{z} \\mid \\underline{x}) =\n\\frac{\\exp[-(-\\log π(\\underline{z} \\mid \\underline{x}))]}\n{\\int d^d\\underline{z} \\, \\exp[-(-\\log π(\\underline{z} \\mid \\underline{x}))]} =\n\\frac{\n    π(\\underline{z} \\mid \\underline{x})\n}{\n    \\int d^d\\underline{z} \\, π(\\underline{z} \\mid \\underline{x})\n} =\nπ(\\underline{z} \\mid \\underline{x}).\n\\tag{50}\\]\nSince it is most of the time impossible to sample directly from \\(π(\\underline{z}\n\\mid \\underline{x})\\), an independent auxiliary variable \\(\\underline{\\rho} \\in\n\\mathbb{R}^d\\) is introduced and used to sample \\(\\underline{z}\\) more efficiently. This variable—referred as the momentum—is such that:\n\\[\n\\underline{\\rho} \\sim \\mathcal{N}(\\underline{0}, \\underline{\\underline{M}}),\n\\tag{51}\\]\nwhere \\(\\underline{\\underline{M}}\\) is the so-called mass matrix. The idea behind HMC is then to work with a distribution that extends the target distribution \\(π(\\underline{z} \\mid \\underline{x})\\) to a distribution over both position and momentum variables, i.e.,\n\\[\nπ(\\underline{z}, \\underline{\\rho} \\mid \\underline{x}) =\nπ(\n    \\underline{z} \\mid \\underline{x}, \\underline{\\rho})π(\\underline{\\rho}\n    \\mid \\underline{x}) =\nπ(\\underline{z} \\mid \\underline{x})π(\\underline{\\rho}),\n\\tag{52}\\]\nwhere we assume the value of \\(\\underline{z}\\) does not directly depend on the momentum variable \\(\\underline{\\rho}\\) but only on the observed data \\(\\underline{x}\\), and that the momentum distribution \\(π(\\underline{\\rho})\\) is independent of all other variables. Using Equation 48, the density of this extended distribution can be analogously written as\n\\[\nπ(\\underline{z}, \\underline{\\rho} \\mid \\underline{x}) =\n\\frac{e^{-H_{\\underline{x}}(\\underline{z}, \\underline{\\rho})}}\n{\\displaystyle\\iint\n    d^d\\underline{z} \\,\n    d^d\\underline{\\rho} \\,\n    e^{-H_{\\underline{x}}(\\underline{z}, \\underline{\\rho})}\n}.\n\\tag{53}\\]\nwhere \\(H_{\\underline{x}}(\\underline{z}, \\underline{\\rho})\\) is the so-called Hamiltonian, defined as\n\\[\nH_{\\underline{x}}(\\underline{z}, \\underline{\\rho}) =\n-\\log π(\\underline{z}, \\underline{\\rho} \\mid \\underline{x}) =\n-\\log π(\\underline{z} \\mid \\underline{x}) - \\log π(\\underline{\\rho}).\n\\tag{54}\\]\nSubstituting \\(π(\\underline{\\rho})\\) from Equation 51 into Equation 54 gives\n\\[\n\\begin{aligned}\nH_{\\underline{x}}(\\underline{z}, \\underline{\\rho}) &=\n-\\log π(\\underline{z} \\mid \\underline{x}) +\n\\frac{1}{2}\n\\left[\\log((2π)^d|\\underline{\\underline{M}}|)\n+ \\underline{\\rho}^T \\underline{\\underline{M}}^{-1} \\underline{\\rho}\\right] \\\\\n&= U_{\\underline{x}}(\\underline{z}) + \\kappa(\\underline{\\rho}).\n\\end{aligned}\n\\tag{55}\\]\nIn physical systems, the Hamiltonian gives the total energy of a system having a position \\(\\underline{z}\\) and a momentum \\(\\underline{\\rho}\\). \\(U_{\\underline{x}}(\\underline{z})\\) is referred as the potential energy (since it is position-dependent) and \\(\\kappa(\\underline{\\rho})\\) is the kinetic energy, since it is momentum-dependent.\nThe point of writing the extended target distribution in this form is that we can take advantage of one specific desirable property of Hamiltonian dynamics: Hamiltonian flows are volume preserving [8]. In other words, the volume in phase space defined by the position and momentum variables is invariant under Hamiltonian dynamics. What this means for our purposes of sampling from a posterior distribution \\(\\pi(\\underline{z} \\mid \\underline{x})\\) is that a walker starting in some position \\(\\underline{z}\\) can “traverse” the volume of the target distribution very efficiently traveling through lines of equal probability. In this sense, the Hamiltonian dynamics can be seen as a way to explore the posterior distribution \\(π(\\underline{z} \\mid \\underline{x})\\) very efficiently. To do so, we compute the time evolution of a walker exploring the space of the position and momentum variables using Hamilton’s equations \\[\n\\begin{aligned} \\frac{d\\underline{z}}{dt} &= \\frac{\\partial\nH_{\\underline{x}}}{\\partial \\underline{\\rho}}, \\\\ \\frac{d\\underline{\\rho}}{dt} &=\n-\\frac{\\partial H_{\\underline{x}}}{\\partial \\underline{z}}. \\end{aligned}\n\\tag{56}\\]\nOne can show that for the particular choice of momentum distribution in Equation 51, these equations take the form \\[\n\\begin{aligned}\n\\frac{d\\underline{z}}{dt} &= \\underline{\\underline{M}}^{-1} \\underline{\\rho}, \\\\\n\\frac{d\\underline{\\rho}}{dt} &=\n-\\nabla_{\\underline{z}} \\log \\pi(\\underline{z} \\mid \\underline{x}),\n\\end{aligned}\n\\tag{57}\\]\nwhere \\(\\nabla_{\\underline{z}}\\) is the gradient with respect to the position variable \\(\\underline{z}\\). Unfortunately, the resulting system of partial differential equations is almost always intractable analytically. Therefore, we must use specialized numerical integrators to solve it. The most popular choice is the so-called leapfrog integrator, which is a symplectic integrator that preserves the volume of the phase space. To update the position and momentum variables, the leapfrog integrator takes three steps:\n\nA half step for the momentum:\n\n\\[\n\\underline{\\rho}\\left(t + \\frac{\\epsilon}{2}\\right) =\n\\underline{\\rho}(t) -\n\\frac{\\epsilon}{2} \\nabla_{\\underline{z}} H_{\\underline{x}}\\left(\n    \\underline{z}(t), \\underline{\\rho}(t)\n\\right).\n\\tag{58}\\]\n\nA full step for the position:\n\n\\[\n\\underline{z}\\left(t + \\epsilon\\right) =\n\\underline{z}(t) +\n\\epsilon \\nabla_{\\underline{\\rho}} H_{\\underline{x}}\\left(\n    \\underline{z}(t), \\underline{\\rho}\\left(t + \\frac{\\epsilon}{2}\\right)\n\\right).\n\\tag{59}\\]\n\nA half step for the momentum:\n\n\\[\n\\underline{\\rho}\\left(t + \\frac{\\epsilon}{2}\\right) =\n\\underline{\\rho}\\left(t + \\frac{\\epsilon}{2}\\right) -\n\\frac{\\epsilon}{2} \\nabla_{\\underline{z}} H_{\\underline{x}}\\left(\n    \\underline{z}(t + \\epsilon), \\underline{\\rho}\\left(t + \\frac{\\epsilon}{2}\\right)\n\\right),\n\\tag{60}\\]\nwhere \\(\\epsilon\\) is the so-called leapfrog step size.\nWithout going further into the details of the HMC framework, we hope the reader can imagine how using this symplectic integrator, a walker aiming to explore the posterior distribution \\(π(\\underline{z} \\mid \\underline{x})\\) can traverse the volume of the target distribution very efficiently.\nWith this conceptual background, we are now ready to review the predecessor of the RHVAE framework, the Hamiltonian Variational Autoencoder framework.\n\n\n\nGiven the dynamics defined in Equation 57, and the numerical integrator defined in Equation 58, Equation 59, and Equation 60, we can now take \\(K\\) iterations of the leapfrog integrator to sample a point \\((\\underline{z}_K, \\underline{\\rho}_K)\\) from the extended target distribution defined in Equation 52. This transition from \\((\\underline{z}_0, \\underline{\\rho}_0)\\) to \\((\\underline{z}_K,\n\\underline{\\rho}_K)\\) via the symplectic integrator can be thought of as a transformation of the form\n\\[\n\\{\\phi_{\\epsilon,\\underline{x}}^{(K)} :\n\\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}^d \\times \\mathbb{R}^d\\},\n\\tag{61}\\]\ni.e., \\(\\phi_{\\epsilon,\\underline{x}}^{(K)}\\) is a function that takes some input value \\((\\underline{z}, \\underline{\\rho})\\) and advances it \\(K\\) steps in phase space using the leapfrog integrator. The function includes \\(\\epsilon\\) to remind the step size of the integrator and \\(\\underline{x}\\) to remind its data dependence. To advance \\(\\ell\\) steps, we simply compose a one step iteration \\((\\ell=1)\\) with the \\(\\ell-1\\) function, i.e.,\n\\[\n\\phi_{\\epsilon,\\underline{x}}^{(\\ell)} =\n\\phi_{\\epsilon,\\underline{x}}^{(\\ell-1)} \\circ \\phi_{\\epsilon,\\underline{x}}^{(1)}\n\\tag{62}\\]\nBy induction, we can see that \\(\\phi_{\\epsilon,\\underline{x}}^{(\\ell)}\\) can be built by \\(\\ell\\) compositions of \\(\\phi_{\\epsilon,\\underline{x}}^{(1)}\\), defining the entire transformation as\n\\[\n\\phi_{\\epsilon,\\underline{x}}^{(K)} = \\phi_{\\epsilon,\\underline{x}}^{(1)} \\circ\n\\phi_{\\epsilon,\\underline{x}}^{(1)} \\circ \\cdots \\circ \\phi_{\\epsilon,\\underline{x}}^{(1)}.\n\\tag{63}\\]\nIt is no coincidence that this resembles the composition of functions used in the normalizing flows framework described earlier in Equation 45. We can then think of a \\(K\\)-step leapfrog integrator trajectory that takes\n\\[\n\\left(\\underline{z}_0, \\underline{\\rho}_0\\right) \\rightarrow\n\\left(\\underline{z}_K, \\underline{\\rho}_K\\right)\n\\tag{64}\\]\nas an invertible transformation formed by composing \\(K\\) one-step leapfrog integrator trajectories. The original HVAE framework [9] includes an extra step in between each leapfrog step that we now review. This step consisted of a tempering step where an initial temperature \\(β_0\\) is proposed and the momentum is decreased by a factor\n\\[\nα_k = \\sqrt{\\frac{β_{k-1}}{β_k}},\n\\tag{65}\\]\nafter each leapfrog step \\(k\\). This simply means that for step \\(k\\), the momentum is computed as\n\\[\n\\underline{\\rho}_k = α_k \\, \\underline{\\rho}_{k-1}.\n\\tag{66}\\]\nThe temperature is then updated as\n\\[\n\\sqrt{β_k} = \\left[\n    \\left(1-\\frac{1}{\\sqrt{β_0}}\\right)\\frac{k^2}{K^2} + \\frac{1}{\\sqrt{β_0}}\n\\right]^{-1}.\n\\tag{67}\\]\nThis tempering step tries to produce an effect similar to that of Annealed Importance Sampling (AIS) [10], where the temperature is decreased gradually to produce a smoother distribution that can be better approximated by the variational distribution. Thus, the transformation \\(\\mathcal{H}_{\\underline{x}}\\) used in [9] takes the form\n\\[\n\\mathcal{H}_{\\underline{x}} = g^K \\circ \\phi_{\\epsilon,\\underline{x}}^{(1)}\n\\circ g^{K-1} \\circ \\cdots \\circ \\phi_{\\epsilon,\\underline{x}}^{(1)} \\circ g^0\n\\circ \\phi_{\\epsilon,\\underline{x}}^{(1)}.\n\\tag{68}\\]\nSince each transformation is smooth and differentiable, this entire transformation is amenable to the reparameterization trick used in the variational autoencoder framework. We can then use this transformation to access an unbiased estimator of the gradient of the ELBO with respect to the variational parameters \\(\\phi\\).\nAs mentioned in Section 0.3.3, for a series of smooth invertible transformations that define a normalizing flow, the change of variables formula tells us that the density of the transformed variable writes\n\\[\nq_\\phi(\\underline{z}_K \\mid \\underline{x}) =\nq_\\phi(\\underline{z}_0 \\mid \\underline{x})\n|\\det J_{\\mathcal{H}_{\\underline{x}}}|^{-1}.\n\\tag{69}\\]\nFor our extended posterior distribution, we have that the initial point is sampled as\n\\[\n(\\underline{z}_0, \\underline{\\rho}_0) \\sim\nq_\\phi(\\underline{z}_0, \\underline{\\rho}_0 \\mid \\underline{x}) =\nq_\\phi(\\underline{z}_0 \\mid \\underline{x}) \\, q_\\phi(\\underline{\\rho}_0),\n\\tag{70}\\]\nmeaning that only the initial position is sampled from the variational distribution, while the momentum is sampled from a standard normal distribution. For the Jacobian, each step consists of the composition of two functions:\n\nThe leapfrog integrator step \\(\\phi_{\\epsilon,\\underline{x}}^{(1)}\\).\nThe tempering step \\(g^k\\).\n\nTherefore, the determinant of the Jacobian of the transformation is given by the product of the determinants of each of the steps, i.e.,\n\\[\n|\\det J_{\\mathcal{H}_{\\underline{x}}}| =\n\\prod_{k=0}^K |\\det J_{g^k}|\n|\\det J_{\\phi_{\\epsilon,\\underline{x}}^{(1)}}|.\n\\tag{71}\\]\nThe resulting variational distribution is then given by\n\\[\n\\begin{aligned}\nq_{\\phi}(\\underline{z}_K, \\underline{\\rho}_K \\mid \\underline{x}) &=\n    q_{\\phi}(\\underline{z}_0 \\mid \\underline{x})\n    q_{\\phi}(\\underline{\\rho}_0)\n    |\\det J_{\\mathcal{H}_{\\underline{x}}}|^{-1} \\\\\n    &=\n    q_{\\phi}(\\underline{z}_0 \\mid \\underline{x})\n    q_{\\phi}(\\underline{\\rho}_0)\n    \\left[\n        \\prod_{k=0}^K \\left| \\det J_{g^k} \\right|\n        \\left| \\det J_{\\phi_{\\epsilon,\\underline{x}}^{(1)}} \\right|\n    \\right]^{-1}\n\\end{aligned}\n\\tag{72}\\]\nHowever, recall that Hamiltonian flows are volume preserving. This means that the volume of the phase space is invariant under the transformation and thus the determinant of the Jacobian is equal to one, i.e.,\n\\[\n|\\det J_{\\phi_{\\epsilon,\\underline{x}}^{(1)}}| = 1.\n\\tag{73}\\]\nFor the tempering step, since \\((\\underline{z}, \\underline{\\rho}) \\rightarrow\n(\\underline{z}, \\alpha_k\\underline{\\rho})\\), the determinant of the Jacobian is given by\n\\[\n|\\det J_{g^k}| = \\alpha_k^d = \\left(\\frac{β_{k-1}}{β_k}\\right)^{\\frac{d}{2}}.\n\\tag{74}\\]\nWith this setting in hand [9] proposes an algorithm to estimate the gradient of the ELBO with respect to the variational parameters that expands the the usual procedure to includes a series of leapfrog steps and a tempering step (see Algorithm 1 in [9] for details).\n\n\n\nTo improve the representational power of the variational posterior approximation, we can make the reasonable assumption that the latent variables \\(\\underline{z}\\) live in a Riemannian manifold \\(\\mathcal{M}\\) endowed with a Riemannian metric \\(\\underline{\\underline{G}}(\\underline{z})\\). Although not entirely correct, one can think of this Riemannian metric as some sort of “position dependent scale bar” for a flat map representing a curved space. In the context of the HMC framework, this Riemannian metric can be included if we allow the momentum variable to be given by\n\\[\n\\underline{\\rho} \\sim\n\\mathcal{N}\\left(\\underline{0}, \\underline{\\underline{G}}(\\underline{z})\\right),\n\\tag{75}\\]\ni.e., the auxiliary momentum variable is no longer independent of the position variable \\(\\underline{z}\\) but rather is distributed according to a normal distribution with mean zero and covariance matrix given by the position-dependent Riemannian metric \\(\\underline{\\underline{G}}(\\underline{z})\\). The Hamiltonian governing the dynamics of the system then becomes\n\\[\nH_{\\underline{x}}^{\\mathcal{M}}(\\underline{z}, \\underline{\\rho}) =\nU_{\\underline{x}}^{\\mathcal{M}}(\\underline{z}) +\n\\kappa_{\\underline{x}}^{\\mathcal{M}}(\\underline{z}, \\underline{\\rho}),\n\\tag{76}\\]\nwhere the \\(\\mathcal{M}\\) superscript reminds us that we now consider the latent variables \\(\\underline{z}\\) living on a Riemannian manifold. Although the potential energy \\(U_{\\underline{x}}^{\\mathcal{M}}(\\underline{z})\\) is still given by Equation 49, the kinetic energy term is now position-dependent, given by\n\\[\n\\kappa_{\\underline{x}}^{\\mathcal{M}}(\\underline{z}, \\underline{\\rho}) =\n\\frac{1}{2}\n\\log((2π)^d|\\underline{\\underline{G}}(\\underline{z})|) +\n\\frac{1}{2}\n\\underline{\\rho}^T \\underline{\\underline{G}}(\\underline{z})^{-1} \\underline{\\rho}\n\\tag{77}\\]\nOur target distribution \\(\\pi(\\underline{z} \\mid \\underline{x})\\) is still given by Equation 52, but with the Hamiltonian given by Equation 76. To show that we still recover the target distribution with this change in the Hamiltonian, let us substitute Equation 76 into Equation 52. This results in\n\\[\n\\pi(\\underline{z} \\mid \\underline{x}) =\n\\frac{\n    \\displaystyle\\int d^d\\underline{\\rho} \\,\n    \\exp\\left\\{\n        -U_{\\underline{x}}^{\\mathcal{M}}(\\underline{z}) -\n        \\frac{1}{2} \\log((2π)^d|\\underline{\\underline{G}}(\\underline{z})|) -\n        \\frac{1}{2} \\underline{\\rho}^T \\underline{\\underline{G}}(\\underline{z})^{-1} \\underline{\\rho}\n    \\right\\}\n}{\n    \\displaystyle\\iint d^d\\underline{z} \\, d^d\\underline{\\rho} \\,\n    \\exp\\left\\{\n        -U_{\\underline{x}}^{\\mathcal{M}}(\\underline{z}) -\n        \\frac{1}{2} \\log((2π)^d|\\underline{\\underline{G}}(\\underline{z})|) -\n        \\frac{1}{2} \\underline{\\rho}^T \\underline{\\underline{G}}(\\underline{z})^{-1} \\underline{\\rho}\n    \\right\\}\n}.\n\\tag{78}\\]\nRearranging terms, we can see that the target distribution is given by\n\\[\n\\pi(\\underline{z} \\mid \\underline{x}) =\n\\frac{\n    e^{-U_{\\underline{x}}^{\\mathcal{M}}(\\underline{z})}\n    \\displaystyle\\int d^d\\underline{\\rho} \\,\n    \\left[\n        \\frac{\n            e^{-\\frac{1}{2}\n            \\underline{\\rho}^T\n            \\underline{\\underline{G}}(\\underline{z})^{-1}\n            \\underline{\\rho}}\n        }{\n            (2\\pi)^d \\sqrt{|\\underline{\\underline{G}}(\\underline{z})|}\n        }\n    \\right]\n}\n{\n    \\displaystyle\\int d^d\\underline{z} \\,\n    e^{-U_{\\underline{x}}^{\\mathcal{M}}(\\underline{z})}\n    \\displaystyle\\int d^d\\underline{\\rho} \\,\n    \\left[\n        \\frac{\n            e^{\n                -\\frac{1}{2}\n                \\underline{\\rho}^T\n                \\underline{\\underline{G}}(\\underline{z})^{-1}\n                \\underline{\\rho}\n            }\n        }{\n            (2\\pi)^d \\sqrt{|\\underline{\\underline{G}}(\\underline{z})|}\n        }\n    \\right]\n}.\n\\tag{79}\\]\nwhere we can recognize the terms in square brackets as the probability density function of a normal distribution. Thus, integrating out the momentum variable \\(\\underline{\\rho}\\) from the target distribution, we obtain\n\\[\n\\pi(\\underline{z} \\mid \\underline{x}) =\n\\frac{e^{-U_{\\underline{x}}^{\\mathcal{M}}(\\underline{z})}}\n{\n    \\displaystyle\\int d^d\\underline{z} \\,\n    e^{-U_{\\underline{x}}^{\\mathcal{M}}(\\underline{z})}\n},\n\\tag{80}\\]\ni.e., the correct target distribution as defined in Equation 48. Therefore, including the position-dependent Riemannian metric in the momentum variable does not change the target distribution. However, the same cannot be said for the Hamiltonian equations of motion, as we will see next.\nRecall that the Hamiltonian equations in Equation 56 require us to compute the gradient of the potential energy with respect to the position variable \\(\\underline{z}\\) and the gradient of the kinetic energy with respect to the momentum variable \\(\\underline{\\rho}\\). For the position variable, our result from Equation 57 still holds, with the only difference that the mass matrix is now given by the Riemannian metric \\(\\underline{\\underline{G}}(\\underline{z})\\). For a single entry of \\(z_i\\), dynamics are given by\n\\[\n\\frac{d z_i}{d t} =\n\\frac{\\partial H_{\\underline{x}}^{\\mathcal{M}}(\\underline{z}, \\underline{\\rho})}\n{\n    \\partial \\rho_i\n} =\n\\left(\n    \\nabla_{\\underline{z}} H_{\\underline{x}}^{\\mathcal{M}}\n    (\\underline{z}, \\underline{\\rho})\n\\right)_i =\n\\left(\n    \\underline{\\underline{G}}(\\underline{z})^{-1} \\underline{\\rho}\n\\right)_i,\n\\tag{81}\\] where \\(\\left(\\cdot\\right)_i\\) denotes the \\(i\\)-th entry of a vector.\nFor the momentum variable, we use three relatively standard results from matrix calculus:\n\nThe gradient of the inverse of a matrix function with respect to the argument is given by\n\n\\[\n\\frac{\n    \\partial \\underline{\\underline{G}}(\\underline{z})^{-1}\n}{\n    \\partial \\underline{z}\n} =\n-\\underline{\\underline{G}}(\\underline{z})^{-1}\n\\frac{d \\underline{\\underline{G}}(\\underline{z})}{d \\underline{z}}\n\\underline{\\underline{G}}(\\underline{z})^{-1}.\n\\tag{82}\\]\n\nThe gradient of the determinant of a matrix function with respect to the argument is given by\n\n\\[\n\\frac{\n    d\n}{\n    d \\underline{z}\n} \\operatorname{det} \\underline{\\underline{G}}(\\underline{z}) =\n\\operatorname{tr}\\left(\n    \\operatorname{adj}(\\underline{\\underline{G}}(\\underline{z}))\n    \\frac{d \\underline{\\underline{G}}(\\underline{z})}{d \\underline{z}}\n\\right) =\n\\operatorname{det} \\underline{\\underline{G}}(\\underline{z})\n\\operatorname{tr}\\left(\n    \\underline{\\underline{G}}(\\underline{z})^{-1}\n    \\frac{d \\underline{\\underline{G}}(\\underline{z})}{d \\underline{z}}\n\\right),\n\\tag{83}\\] where \\(\\operatorname{adj}\\) is the adjoint operator, \\(\\operatorname{tr}\\) is the trace operator, and \\(\\operatorname{det}\\) is the determinant operator.\n\nThe gradient of the logarithm of the determinant of a matrix function with respect to the argument is given by \\[  \n\\frac{d}{d \\underline{z}}\n\\log (\\operatorname{det} \\underline{\\underline{G}}(\\underline{z})) =\n\\operatorname{tr}\\left(\n\\underline{\\underline{G}}(\\underline{z})^{-1}\n\\frac{d \\underline{\\underline{G}}(\\underline{z})}{d \\underline{z}}\n\\right).\n\\tag{84}\\]\n\nGiven these results, we can now compute the dynamics of the momentum variable. For a single entry of \\(\\underline{\\rho}\\), \\(\\rho_i\\), the resulting expression is of the form\n\\[\n\\frac{d \\rho_i}{d t} =\n\\frac{\n    \\partial H_{\\underline{x}}^{\\mathcal{M}}(\\underline{z}, \\underline{\\rho})\n}{\n    \\partial z_i\n} =\n\\frac{\\partial \\ln \\pi(\\underline{z} \\mid \\underline{x})}{\\partial z_i} -\n\\frac{1}{2} \\operatorname{tr}\\left(\n    \\underline{\\underline{G}}(\\underline{z})\n    \\frac{\\partial \\underline{\\underline{G}}(\\underline{z})}{\\partial z_i}\n\\right) +\n\\frac{1}{2}\n\\underline{\\rho}^T\n\\underline{\\underline{G}}(\\underline{z})^{-1}\n\\frac{\\partial \\underline{\\underline{G}}(\\underline{z})}{\\partial z_i}\n\\underline{\\underline{G}}(\\underline{z})^{-1} \\underline{\\rho}.\n\\tag{85}\\]\nAs before, we can adapt the leapfrog integrator to this setting to produce an unbiased estimator of the gradient of the ELBO with respect to the variational parameters. However, the previously presented leapfrog integrator is no longer volume preserving in this setting due to the position-dependent Riemannian metric. Thus, we need to use a generalized version of the leapfrog integrator. This has been previously derived and [3] lists the following steps for the generalized leapfrog integrator as\n\nA half step for the momentum variable\n\n\\[\n\\underline{\\rho}\\left(t+\\frac{\\varepsilon}{2}\\right) =\n\\underline{\\rho}(t) -\n\\frac{\\varepsilon}{2}\n\\nabla_{\\underline{z}} H_{\\underline{x}}^{\\mathcal{M}}\\left(\n    \\underline{z}(t),\n    \\underline{\\rho}\\left(t+\\frac{\\varepsilon}{2}\\right)\n\\right).\n\\tag{86}\\]\n\nA full step for the position variable\n\n\\[\n\\underline{z}(t+\\varepsilon) =\n\\underline{z}(t) +\n\\frac{\\varepsilon}{2}\n\\left[\n    \\nabla_{\\underline{z}} H_{\\underline{x}}^{\\mathcal{M}}\\left(\n        \\underline{z}(t),\n        \\underline{\\rho}\\left(t+\\frac{\\varepsilon}{2}\\right)\n    \\right) +\n    \\nabla_{\\underline{\\rho}} H_{\\underline{x}}^{\\mathcal{M}}\\left(\n        \\underline{z}(t+\\varepsilon),\n        \\underline{\\rho}\\left(t+\\frac{\\varepsilon}{2}\\right)\n    \\right)\n\\right].\n\\tag{87}\\]\n\nA half step for the momentum variable\n\n\\[\n\\underline{\\rho}(t+\\varepsilon) =\n\\underline{\\rho}\\left(t+\\frac{\\varepsilon}{2}\\right) -\n\\frac{\\varepsilon}{2}\n\\nabla_{\\underline{z}} H_{\\underline{x}}^{\\mathcal{M}}\\left(\n    \\underline{z}(t+\\varepsilon),\n    \\underline{\\rho}\\left(t+\\frac{\\varepsilon}{2}\\right)\n\\right).\n\\tag{88}\\]\nSince the left hand side terms appear on the right hand side, these equations must be solved using fixed point iterations. This means that the numerical implementation of this generalized integrator iterates a step multiple times until it finds a “fixed point”, i.e., a point that, when fed to the right hand side of the equation, does not change the value of the left hand side. In practice, we set the number of iterations to a small number.\n\n\n\nThe Riemannian Hamiltonian Variational Autoencoder (RHVAE) method extends the HVAE idea to account for non-Euclidean geometry of the latent space. This means that the latent variables \\(\\underline{z}\\) live on a Riemannian manifold. This manifold is endowed with a Riemannian metric \\(\\underline{\\underline{G}}(\\underline{z})\\), that is position-dependent. We navigate through this manifold using the Hamiltonian equations of motion, utilizing the geometric information encoded in the Riemannian metric.\nAs with the HAVE, using the generalized leapfrog integrator (Equation 86, Equation 87, Equation 88) along with a tempering step creates a smooth mapping\n\\[\n(\\underline{z}_0, \\underline{\\rho}_0) \\rightarrow\n(\\underline{z}_K, \\underline{\\rho}_K),\n\\tag{89}\\]\nthat can be thought of as a kind of normalizing flow informed by the target distribution and the geometry of the latent space. Using the same procedure as the HVAE that includes a tempering step, the resulting variational distribution takes the form of Equation 72, with the only difference that the momentum variable is now position-dependent, i.e.,\n\\[\nq_{\\phi}(\\underline{z}_K, \\underline{\\rho}_K \\mid \\underline{x}) =\nq_{\\phi}(\\underline{z}_0 \\mid \\underline{x})\nq_{\\phi}(\\underline{\\rho}_0 \\mid \\underline{z}_0)\n\\left[\n    \\prod_{k=1}^{K}\n    \\left| \\det \\underline{\\underline{J}}_{g^k} \\right|\n    \\left| \\det \\underline{\\underline{J}}_{\\phi_{\\epsilon,\\underline{x}}^{(1)}} \\right|\n\\right]^{-1},\n\\tag{90}\\]\nwhere, as before, \\(\\underline{\\underline{J}}_{\\phi_{\\epsilon,\\underline{x}}^{(1)}}\\) is the Jacobian of the generalized leapfrog integrator with step size \\(\\epsilon\\), and \\(\\underline{\\underline{J}}_{g^k}\\) is the Jacobian of the tempering step. Since we established that Hamiltonian dynamics are volume preserving, the determinant of the Jacobian of the tempering step is one, i.e., \\(\\left| \\det\n\\underline{\\underline{J}}_{\\phi_{\\epsilon,\\underline{x}}^{(1)}} \\right| = 1\\). Moreover, since we are using the same type of tempering step as in the HVAE, we have the same result for the determinant of the Jacobian of the tempering step as in Equation 74. After substituting these results into Equation 90, we obtain\n\\[\nq_{\\phi}(\\underline{z}_K, \\underline{\\rho}_K \\mid \\underline{x}) =\nq_{\\phi}(\\underline{z}_0 \\mid \\underline{x})\nq_{\\phi}(\\underline{\\rho}_0 \\mid \\underline{z}_0)\n\\beta_0^{d / 2},\n\\tag{91}\\]\nwhere \\(d\\) is the dimension of the latent space. The main difference from the HVAE is that the term \\(q_{\\phi}(\\underline{\\rho}_0 \\mid \\underline{z}_0)\\) depends on the position \\(\\underline{z}_0\\) of the latent variables. To establish this dependence, we must define the functional form for the metric tensor \\(\\underline{\\underline{G}}(\\underline{z})\\).\nIn RHVAE, the metric is learned from the data. However, looking at the generalized leapfrog integrator, we see that the inverse and the determinant of the metric tensor are required. Thus, rather than defining the metric tensor directly, we define the inverse of the metric tensor \\(\\underline{\\underline{G}}(\\underline{z})^{-1}\\), and use the fact that\n\\[\n\\det \\underline{\\underline{G}}(\\underline{z}) =\n\\det \\underline{\\underline{G}}(\\underline{z})^{-1}.\n\\tag{92}\\]\nBy doing so, we do not have to invert the metric tensor for every leapfrog step. [3] proposes an inverse metric tensor of the form\n\\[\n\\underline{\\underline{G}}(\\underline{z}) =\n\\sum_{i=1}^N\n\\underline{\\underline{L}}_{\\Psi_i}\n\\underline{\\underline{L}}_{\\Psi_i}^{\\top}\n\\exp \\left(\n    -\\frac{\n        \\left\\| \\underline{z} - \\underline{\\mu}{(\\underline{x}_i)} \\right\\|_2^2\n        }{\n            T^2\n        }\n    \\right) +\n\\lambda \\underline{\\underline{\\mathbb{I}}}_l,\n\\tag{93}\\]\nwhere \\(\\underline{\\underline{L}}_{\\Psi_i}\\) is a lower triangular matrix with positive diagonal entries, \\(\\top\\) is the transpose operator, \\(\\underline{\\mu}(\\underline{x}_i)\\) is the mean of the \\(i\\)-th component of the dataset, \\(T\\) is a temperature parameter to smooth the metric tensor, \\(\\lambda\\) is a regularization parameter, and \\(\\underline{\\underline{\\mathbb{I}}}_l\\) is the \\(l \\times l\\) identity matrix. This last term with \\(\\lambda\\) is set for the metric tensor not to be zero. However, usually \\(\\lambda\\) is set to a small value, e.g., \\(10^{-3}\\). The terms \\(\\underline{\\mu}{(\\underline{x}_i)}\\) are referred to as the “centroids” and are given by the mean of the variational posterior, such that,\n\\[\nq_{\\phi}(\\underline{z}_i \\mid \\underline{x}_i) =\n\\mathcal{N}\\left(\n    \\underline{\\mu}{(\\underline{x}_i)},\n    \\underline{\\underline{\\Sigma}}(\\underline{x}_i)\n\\right).\n\\tag{94}\\]\nIntuitively, we can think of \\(\\underline{\\underline{L}}_{\\Psi_i}\\) as the triangular matrix in the Cholesky decomposition of \\(\\underline{\\underline{G}}^{-1}(\\underline{\\mu}{(\\underline{x}_i)})\\) up to a regularization factor. This matrix is learned using a neural network with parameters \\(\\Psi_i\\), mapping\n\\[\n\\underline{x}_i \\rightarrow \\underline{\\underline{L}}_{\\Psi_i}.\n\\tag{95}\\]\nThe hyperparameters \\(T\\) and \\(\\lambda\\) could be learned from the data as well, but, for simplicity, we set them to constants. We invite the reader to refer to [3] for a more detailed discussion on the training procedure."
  },
  {
    "objectID": "supplementary.html#sec-nonlinear",
    "href": "supplementary.html#sec-nonlinear",
    "title": "Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps",
    "section": "",
    "text": "In this section, we provide a theoretical justification for why nonlinear dimensionality reduction methods may offer advantages over linear methods when modeling fitness landscapes. We build upon the concept of a causal manifold that captures the relationship between genotypes and their fitness across multiple environments.\n\n\nThe central concept in our analysis is what we call the causal manifold \\(\\mathcal{M}_c\\)—a low-dimensional space that encodes the information necessary to predict fitness across multiple environments. Another way to think about this manifold is in a data-generating process perspective: the causal manifold is the underlying space from which the fitness data is sampled. We define two key functions:\n\nThe mapping from genotype to manifold coordinates: \\[\n\\phi: g \\in G \\to \\underline{z} \\in \\mathcal{M}_{c}.\n\\tag{96}\\]\nThe mapping from manifold coordinates to fitness in environment \\(E\\): \\[\nF_{E}: \\underline{z} \\in \\mathcal{M}_c \\to f_{E} \\in \\mathbb{R}.\n\\tag{97}\\]\n\nA critical assumption in linear approaches is that \\(F_E\\) takes the form \\[\nF_E(\\underline{z}) = \\underline{\\beta} \\cdot \\underline{z} + b,\n\\tag{98}\\]\nwhere \\(\\underline{\\beta}\\) is a vector of coefficients and \\(b\\) is a scalar offset. However, this assumption may not hold in general. While local linearity is guaranteed by Taylor’s theorem for small changes in \\(\\underline{z}\\)\n\\[\nF_E(\\underline{z} + \\Delta\\underline{z}) \\approx\nF_E({\\underline{z}}) +\n\\nabla F_E(\\underline{z}) \\cdot \\Delta \\underline{z} +\n\\mathcal{O}(\\Delta\\underline{z}^2),\n\\tag{99}\\]\nglobal linearity across the entire manifold is a much stronger constraint that may not reflect the true complexity of biological systems.\n\n\n\nApplying linear dimensionality reduction techniques such as PCA/SVD constructs a flat manifold of dimension \\(d\\) to approximate our fitness data. Through SVD, our \\(N \\times M\\) data matrix \\(\\underline{\\underline{D}}\\) (\\(N\\) being the number of environments and \\(M\\) being the number of genotypes) containing fitness profiles is factorized as\n\\[\n\\underline{\\underline{D}} =\n\\underline{\\underline{U}}\\,\n\\underline{\\underline{\\Sigma}}\\,\n\\underline{\\underline{V}}^T.\n\\tag{100}\\]\nThe coordinates of genotype \\(i\\) in this linear manifold are given by\n\\[\n\\underline{z}_i = \\left[\\begin{matrix}\n\\sigma_1 v_{i1} \\\\\n\\sigma_2 v_{i2} \\\\\n\\vdots \\\\\n\\sigma_d v_{id}\n\\end{matrix}\\right].\n\\tag{101}\\]\nThe key property of this linear approach is that Euclidean distances in the manifold directly correspond to differences in fitness profiles in the truncated space\n\\[\n\\|\\underline{z}_1 - \\underline{z}_2\\| =\n\\|\\underline{\\hat{f}}_1 - \\underline{\\hat{f}}_2\\|.\n\\tag{102}\\]\nWhile mathematically elegant and computationally tractable, this linear constraint may limit our ability to capture the true structure of the underlying phenotypic space efficiently.\n\n\n\nA critical insight comes from the Whitney Embedding Theorem, which states that any smooth \\(d'\\)-dimensional manifold can be embedded in a Euclidean space of dimension \\(2d' + 1\\). This theorem explains the “unreasonable effectiveness of linear maps” while simultaneously highlighting their limitations. If the true causal manifold has intrinsic dimension \\(d'\\) but is nonlinear, capturing its structure with a linear approximation generally requires \\(d &gt; d'\\) dimensions.\nTo formalize this intuition, we consider the causal manifold as a Riemannian manifold with a position-dependent metric tensor \\(\\underline{\\underline{G}}(\\underline{z})\\). On a nonlinear manifold, this tensor is not the identity matrix, meaning\n\\[\n\\|\\underline{z}_1 - \\underline{z}_2\\| \\neq\n\\|\\underline{\\hat{f}}_1 - \\underline{\\hat{f}}_2\\|.\n\\tag{103}\\]\nInstead, the distance between points on a Riemannian manifold is given by \\[\nd_{\\mathcal{M}_c}(\\underline{z}_1, \\underline{z}_2) =\n\\min_{\\underline{\\gamma}} \\int_0^1 dt\n\\sqrt{\n    \\underline{\\dot{\\gamma}}(t)^T \\,\n    \\underline{\\underline{G}}(\\underline{\\gamma}(t)) \\,\n    \\underline{\\dot{\\gamma}}(t)\n},\n\\tag{104}\\]\nwhere \\(\\underline{\\gamma}\\) is a parametric curve with \\(\\underline{\\gamma}(0) =\n\\underline{z}_1\\) and \\(\\underline{\\gamma}(1) = \\underline{z}_2\\). The curve that minimizes this distance is called a geodesic.\nThe key insight is that when the true causal manifold is nonlinear, approximating it with a linear method requires additional dimensions to compensate for the curvature. This is reflected in the singular value spectrum, where\n\\[\n\\|\\underline{\\hat{f}}_1 - \\underline{\\hat{f}}_2\\|^2 =\n\\sum_{j=1}^d \\sigma_j^2(v_{1j} - v_{2j})^2.\n\\tag{105}\\]\nIn this case, dimensions beyond the intrinsic dimensionality \\(d'\\) (with smaller singular values) are effectively “compensating” for the curvature of the true manifold.\n\n\n\nTo learn nonlinear manifolds directly, we employ variational autoencoders (VAEs) that can approximate the joint distribution between fitness profiles \\(\\underline{f}\\) and latent variables \\(\\underline{z}\\) representing coordinates on the causal manifold\n\\[\n\\pi(\\underline{f}, \\underline{z}) \\approx\n\\pi(\\underline{f} | \\underline{z})\\pi(\\underline{z}).\n\\tag{106}\\]\nVAEs employ two key neural networks\n\nAn encoder function that approximates the posterior distribution: \\[\n\\Phi^{(E)}: \\underline{f} \\in \\mathbb{R}^N \\rightarrow\n\\left[\\begin{matrix}\n\\underline{\\mu}^{(E)} \\\\ \\\\\n\\underline{\\log \\underline{\\sigma}^{(E)}}\n\\end{matrix}\\right] \\in \\mathbb{R}^{2d}.\n\\tag{107}\\]\nA decoder function that maps latent points to fitness profiles: \\[\n\\Psi^{(D)}:\n\\underline{z} \\in \\mathcal{M}_c \\rightarrow\n\\underline{\\mu}^{(D)} \\in \\mathbb{R}^N.\n\\tag{108}\\]\n\nFor Riemannian Hamiltonian Variational Autoencoders (RHVAEs), used throughout this work, we add a third network that explicitly learns the metric tensor \\(\\underline{\\underline{G}}(\\underline{z})\\), allowing the model to capture the geometric structure of the manifold directly.\n\n\n\nNonlinear methods offer several theoretical advantages over linear approaches:\n\nDimensionality Efficiency: Nonlinear methods can potentially represent the same information with fewer dimensions. If the true causal manifold has intrinsic dimension \\(d'\\) but is nonlinear, linear methods may require \\(d &gt; d'\\) dimensions to achieve comparable accuracy.\nGeometric Fidelity: By learning the metric tensor directly, nonlinear methods can capture the true geometric structure of the fitness landscape, including features like multiple peaks, valleys, and saddle points that may be difficult to represent in a linear space.\nPredictive Power: If the true relationship between genotype and fitness is nonlinear, nonlinear methods should provide better predictive accuracy, especially for genotypes distant from those in the training set.\nInformation Compression: By capturing the curvature of the manifold explicitly rather than through additional dimensions, nonlinear methods can provide a more compact and interpretable representation of the fitness landscape.\n\n\n\n\nWhile the theoretical advantages of nonlinear methods are clear, empirical validation is essential. We observe this advantage in practice through several metrics:\n\nReconstruction Error: Nonlinear methods achieve lower reconstruction error for the same latent dimension compared to linear methods.\nSingular Value Spectrum: The slow decay of singular values in the data matrix suggests inherent nonlinearity in the fitness landscape.\nGeneralization Performance: Nonlinear methods show better performance when predicting fitness for held-out genotypes.\n\nHowever, it’s important to note that nonlinear methods come with their own challenges:\n\nOverfitting Risk: Nonlinear methods may introduce spurious complexity when the true structure is simple.\nInterpretation Challenges: The biological meaning of nonlinear latent dimensions can be less straightforward than those from linear methods.\nTraining Complexity: Nonlinear methods typically require more data and computational resources for effective training.\nValidation Requirements: Demonstrating that a learned nonlinear structure reflects true biological relationships rather than mathematical artifacts requires careful validation.\n\nIn this work, we address these challenges through rigorous cross-validation, comparison with linear baselines, and direct analysis of the learned metric structure to ensure biological relevance of our nonlinear representations."
  },
  {
    "objectID": "supplementary.html#cross-validation-methodology-for-evaluating-latent-space-predictive-power",
    "href": "supplementary.html#cross-validation-methodology-for-evaluating-latent-space-predictive-power",
    "title": "Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps",
    "section": "",
    "text": "To rigorously evaluate whether nonlinear latent space coordinates capture more information about the underlying phenotypic state than linear projections, we implemented a cross-validation framework that tests each model’s ability to predict responses to unseen antibiotics. This approach extends the bi-cross validation methodology described by [11], adapting it for both linear (PCA/SVD) and nonlinear (VAE and RHVAE) dimensionality reduction techniques.\n\n\nThe core insight of our cross-validation approach is that if latent space coordinates genuinely capture the underlying phenotypic state of the system, they should enable accurate prediction of cellular responses to antibiotics not used in training. We test this hypothesis by systematically holding out one antibiotic at a time, learning latent space coordinates without information from that antibiotic, and then evaluating how well these coordinates predict the response to the held-out antibiotic.\n\n\n\nFor linear models, we implement the bi-cross validation method of [11]. Given our \\(IC_{50}\\) data matrix \\(\\underline{\\underline{X}} \\in \\mathbb{R}^{m\n\\times n}\\) where rows represent antibiotics and columns represent genotypes, we partition the matrix into four quadrants\n\\[\n\\underline{\\underline{X}} =\n\\begin{bmatrix}\n\\underline{\\underline{A}} & \\underline{\\underline{B}} \\\\\n\\underline{\\underline{C}} & \\underline{\\underline{D}}\n\\end{bmatrix}\n\\tag{109}\\]\nHere, \\(\\underline{\\underline{A}}\\) represents the target quadrant containing the held-out antibiotic-genotype combinations we aim to predict. Specifically, for a given held-out antibiotic \\(i\\):\n\n\\(\\underline{\\underline{A}} \\in \\mathbb{R}^{1 \\times k}\\) contains \\(IC_{50}\\) values for antibiotic \\(i\\) and the validation set genotypes (\\(k\\) genotypes)\n\\(\\underline{\\underline{B}} \\in \\mathbb{R}^{1 \\times (n-k)}\\) contains \\(IC_{50}\\) values for antibiotic \\(i\\) and the training set genotypes\n\\(\\underline{\\underline{C}} \\in \\mathbb{R}^{(m-1) \\times k}\\) contains \\(IC_{50}\\) values for all other antibiotics and the validation set genotypes\n\\(\\underline{\\underline{D}} \\in \\mathbb{R}^{(m-1) \\times (n-k)}\\) contains \\(IC_{50}\\) values for all other antibiotics and the training set genotypes\n\nThe theoretical foundation of this approach rests on the observation that for a full-rank matrix, we can estimate \\(\\underline{\\underline{A}}\\) as\n\\[\n\\underline{\\underline{A}} =\n\\underline{\\underline{B}}\\, \\underline{\\underline{D}}^+ \\underline{\\underline{C}}\n\\tag{110}\\]\nwhere \\(\\underline{\\underline{D}}^+\\) is the Moore-Penrose pseudoinverse of \\(\\underline{\\underline{D}}\\). To evaluate how well a low-rank approximation performs, we compute rank-\\(r\\) approximations of \\(\\underline{\\underline{D}}\\) using SVD:\n\nPerform SVD on \\(\\underline{\\underline{D}}\\):\n\n\\[\n\\underline{\\underline{D}} =\n\\underline{\\underline{U}}\\, \\underline{\\underline{\\Sigma}} \\,\n\\underline{\\underline{V}}^T\n\\tag{111}\\]\n\nCreate rank-\\(r\\) approximation by retaining only the top \\(r\\) singular values:\n\n\\[\n\\underline{\\underline{D}}_r = \\underline{\\underline{U}}\\,\n\\underline{\\underline{\\Sigma}}_r \\underline{\\underline{V}}^T\n\\tag{112}\\]\n\nCompute the predicted matrix:\n\n\\[\n\\underline{\\underline{\\hat{A}}}_r =\n\\underline{\\underline{B}}\\, \\underline{\\underline{D}}_r^+\n\\underline{\\underline{C}}\n\\tag{113}\\]\n\nCalculate the mean squared error (MSE) between \\(\\underline{\\underline{A}}\\) and \\(\\underline{\\underline{\\hat{A}}}_r\\):\n\n\\[\n\\text{MSE}_r =\n\\frac{1}{|\\underline{\\underline{A}}|}\\sum_{i,j}(\\underline{\\underline{A}}_{ij} -\n\\underline{\\underline{\\hat{A}}}_{r,ij})^2\n\\tag{114}\\]\nBy examining how MSE varies with rank, we can determine the minimum dimensionality needed to accurately predict the held-out antibiotic data.\n\n\n\nFor nonlinear models (VAE and RHVAE), we adapt the cross-validation approach to account for the different model architecture. The procedure for each held-out antibiotic \\(i\\) consists of two phases:\n\n\nFirst, we train a complete model (encoder + decoder) on \\(IC_{50}\\) data from all antibiotics except the held-out antibiotic \\(i\\). This yields an 85%-15% train-validation split of genotypes for robust training. Denoting the set of all antibiotics as \\(\\mathcal{A}\\) and the held-out antibiotic as \\(a_i\\), we train on the data matrix \\(\\underline{\\underline{X}}_{\\mathcal{A} \\setminus \\{a_i\\}}\\).\nFor the VAE, we maximize the evidence lower bound (ELBO):\n\\[\n\\mathcal{L}(\\theta, \\phi) =\n\\left\\langle\n    \\log p_\\theta(\\underline{x}|\\underline{z})\n\\right\\rangle_{q_\\phi(\\underline{z}|\\underline{x})} -\nD_{KL}(q_\\phi(\\underline{z}|\\underline{x}) || \\pi(\\underline{z}))\n\\tag{115}\\]\nwhere \\(\\underline{x}\\) represents the \\(\\log IC_{50}\\) values for a genotype across all antibiotics except \\(a_i\\), \\(\\underline{z}\\) is the latent representation, \\(q_\\phi(\\underline{z}|\\underline{x})\\) is the encoder, and \\(p_\\theta(\\underline{x}|\\underline{z})\\) is the decoder.\nFor the RHVAE, we similarly maximize the ELBO but with modifications to account for the Riemannian geometry of the latent space. The model learns a position-dependent metric tensor \\(\\underline{\\underline{G}}(\\underline{z})\\) along with the encoder and decoder parameters.\nAfter training, the encoder maps inputs \\(\\underline{x}\\) to latent coordinates \\(\\underline{z}\\) that capture the underlying phenotypic state without information from antibiotic \\(a_i\\).\n\n\n\nOnce the encoder is trained, we freeze its parameters—effectively fixing the latent space coordinates for each genotype. We then train a decoder-only model to predict the response to the held-out antibiotic \\(a_i\\) using a 50%-50% train-validation split of genotypes\n\\[\n\\underline{z} = \\text{Encoder}_\\phi(\\underline{x})\n\\tag{116}\\]\n\\[\n\\hat{y}_i = \\text{Decoder}_\\theta(\\underline{z})\n\\tag{117}\\]\nwhere \\(\\hat{y}_i\\) is the predicted \\(IC_{50}\\) value for antibiotic \\(a_i\\).\nThis training maximizes the likelihood of the observed \\(IC_{50}\\) values given the latent coordinates:\n\\[\n\\mathcal{L}_{\\text{decoder}} =\n\\left\\langle\n    \\log p_\\theta(y_i|\\underline{z})\n\\right\\rangle_{q_\\phi(\\underline{z}|\\underline{x})}\n\\tag{118}\\]\nFor both VAE and RHVAE, we use a 2-dimensional latent space to ensure fair comparison with linear methods. After training, we evaluate the model’s predictive performance on the validation set by computing the mean squared error between predicted and actual \\(IC_{50}\\) values.\n\n\n\n\nThe specific implementation involved several key components:\n\nData Organization: The \\(IC_{50}\\) values were organized in a 3D tensor with dimensions [antibiotics × genotypes × MCMC samples], where the MCMC samples represent posterior samples from the Bayesian inference procedure used to estimate \\(IC_{50}\\) values in [1].\nTraining Protocol: For each held-out antibiotic:\n\nThe full model (encoder + decoder) was trained for the linear mapping from all antibiotics except the held-out one to the latent space.\nThe encoder parameters were frozen, and a new decoder was trained to map from the latent space to the held-out antibiotic’s \\(IC_{50}\\) values.\nA tempering parameter \\(\\beta_0\\) was set to 0.3 to improve stability during training.\nTraining used the Adam optimizer with a learning rate of \\(10^{-3}\\).\nModels were trained for 50 epochs with a batch size of 256.\n\nModel Architecture:\n\nThe encoder consisted of a joint logarithmic encoder that maps inputs to latent space.\nThe decoder was a simple network mapping from latent space to antibiotic responses.\nFor RHVAE, the model incorporated a metric chain with position-dependent Riemannian metric.\n\nEvaluation Metrics: We assessed predictive performance using mean squared error (MSE) between actual and predicted \\(IC_{50}\\) values on the validation set.\n\n\n\n\nTo fairly compare the predictive power of linear and nonlinear dimensionality reduction techniques, we plotted the MSE for SVD at different ranks alongside the MSE for 2D VAE and 2D RHVAE models. This visualization, shown in Figure 7 in the main text, demonstrates that for all antibiotics, the 2D nonlinear latent space coordinates provide more accurate predictions than any number of linear dimensions.\nThe key finding is that the nonlinear latent spaces capture the underlying phenotypic structure of the system more effectively than linear projections, thus enabling better predictions of out-of-sample data. This validates our hypothesis that the phenotypic space of antibiotic resistance has an inherently nonlinear structure that is better represented by VAE and especially RHVAE models.\n\n\n\nIn Figure 7 in the main text, we tested the predictive power of 2D nonlinear latent space coordinates compared to linear latent space coordinates. There, we showed via a custom cross-validation scheme that the 2D latent space coordinates were more predictive of out-of-sample data than the linear latent space coordinates. Here, we repeat a similar analysis, but using 3D latent space coordinates.\nFollowing the same cross-validation scheme as in the main text, we trained a full model on all but one antibiotic (85%-15% splits for training and validation data). This generates the latent space coordinates without using any information from the antibiotic that was left out. Next, we froze the encoder parameters —equivalent to fixing the latent space coordinates—and trained a decoder-only model on the missing antibiotic data (50%-50% splits for training and validation data). Figure 5 shows the results of this analysis. We can see that other than a couple of exceptions (KM and NFLX), the 3D latent space coordinates are marginally more predictive of out-of-sample data than the 2D latent space coordinates. This suggests that the phenotypic changes associated with the experimental setup can be captured with two effective degrees of freedom.\n\n\n\n\n\n\nFigure 5: Comparison of 2D and 3D nonlinear latent space models for predicting out-of-sample antibiotic data. Reconstruction error for each missing antibiotic as a function of linear dimensions used in SVD cross-validation. Horizontal lines represent the accuracy of nonlinear models: 2D-VAE (dark blue, dashed), 2D-RHVAE (dark red, solid), 3D-VAE (dark green, dotted), and 3D-RHVAE (dark purple, dash-dotted). The 3D models show marginal improvement over their 2D counterparts for most antibiotics, suggesting that two effective degrees of freedom capture most of the relevant phenotypic variation."
  },
  {
    "objectID": "supplementary.html#geodesics-in-latent-space",
    "href": "supplementary.html#geodesics-in-latent-space",
    "title": "Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps",
    "section": "",
    "text": "In this section, we explore an interesting and surprising property of some of the resulting evolutionary trajectories in latent space. As detailed in the main text, we trained an RHVAE on the \\(IC_{50}\\) data from [1]. The data used to train the RHVAE consists of a matrix with eight rows—one per each antibiotic—and \\(\\approx\\) 1300 columns representing one of the lineages at some time point in the experiment. However, at no point, the RHVAE has any knowledge of what genotype or time point corresponds to any particular column in the data matrix. All it sees at every epoch is a random subset of columns of this matrix. Nevertheless, it is interesting to ask whether there is some regularity in the resulting evolutionary trajectories in the learned latent space. In other words, once the RHVAE is trained, we can map the time series corresponding to a particular lineage to the corresponding latent space trajectory. A few of these trajectories are shown in Figure 6 as gold connected points.\nOne of the unique features of the RHVAE model is the co-learning of the metric tensor in the latent space. A space endowed with such mathematical structure gives us the ability to compute the shortest path between any two points, commonly referred to as a geodesic. Geodesics are the generalization of the idea of a straight line in Euclidean space to curved spaces, and it is a fundamental concept in Riemannian geometry. Let us define this curve as \\(\\underline{\\gamma}(t)\\) where \\(t \\in [0, 1]\\). This means that the geodesic is a parametric curve \\(\\underline{\\gamma}\\) for which we only need to define the initial and final points of the curve, \\(\\underline{\\gamma}(0) = \\underline{z}_0\\) and \\(\\underline{\\gamma}(1) = \\underline{z}_1\\). What makes it a geodesic is that it minimizes the length of the curve connecting the initial and final points, i.e., it minimizes\n\\[\nL(\\underline{\\gamma}) = \\int_0^1\ndt \\,\n\\sqrt{\n    \\underline{\\dot{\\gamma}}(t)^T\n    \\underline{\\underline{G}}(\\underline{\\gamma}(t))\n    \\underline{\\dot{\\gamma}}(t)\n}\n\\tag{119}\\]\nwhere \\(\\underline{\\dot{\\gamma}}(t) = \\frac{d}{dt} \\underline{\\gamma}(t)\\) is the velocity vector of the curve and \\(\\underline{\\underline{G}}(\\underline{\\gamma}(t))\\) is the metric tensor at the point \\(\\underline{\\gamma}(t)\\). Computing this geodesic is a non-trivial task, especially for a data-derived metric tensor with no closed-form expression. Fortunately, we can leverage the generality of neural networks to parameterize the curve and compute the geodesic numerically [12]. Again, all we need to do is define the initial and final points of the curve, provide the metric tensor learned by the RHVAE, and let the neural network approximate the shortest path between the two points. Figure 6 shows the corresponding curves between the initial point (black crosses) and the final point (black triangles) as red curves for a few lineages. We highlight the fact that the curves are not straight lines in the latent space, but rather curved paths that follow the curvature of the latent space. For these few examples, we see that the geodesic curves are very similar to the gold curves, which are the actual trajectories of the lineages in the latent space. This rather surprising result suggests that the best spatial representation for the evolutionary trajectories coincides with the shortest path in the latent space.\n\n\n\n\n\n\nFigure 6: Geodesic paths in latent space follow evolutionary trajectories The figure shows the latent space representation of evolutionary trajectories from the Iwasawa et al. dataset [1]. Background coloring represents the metric volume (determinant of the metric tensor), with lighter regions indicating higher local curvature. Gold connected points show actual evolutionary trajectories of different lineages, while red curves represent the computed geodesics (shortest paths) between initial points (black crosses) and final points (black triangles). The striking similarity between geodesics and actual trajectories suggests that evolution in phenotype space tends to follow paths that minimize distance in the geometry-informed latent space.\n\n\n\nSo far, these few examples suggest that the shortest path in the latent space coincides with the actual evolutionary trajectory. Since the latent space coordinates of any data point capture information about the resistance profile of the corresponding strain, we expect that the qualitative matching between the geodesic and the actual trajectory should mean that the resulting resistance profile predicted by the geodesic is very similar to the actual experimental resistance profile. To see if this is the case, we must decode the latent space trajectory back to the original space. Doing so projects back all of the points sampled along the geodesic curve to the original data space with eight antibiotic resistance values. However, there is a catch: the geodesic curve is a smooth continuous function of time, but the resistance profile of a strain is a discrete set of values with different step sizes. In other words, when constructing the geodesic curve, we define a continuous function connecting the initial and final point with no sudden jumps in the coordinates. But there is no a priori reason that the resulting experimental curve should be smooth. This statement is not necessarily a consequence of the discrete sampling of the data with a one-day time step that could be resolved by increasing the sampling frequency, but rather a consequence of the genotype-phenotype map. Although in the simulations presented in the main text we assume that all evolutionary steps follow certain distribution of step sizes as a convenient mathematical simplification, the complexity of the genotype-phenotype map might be such that this simplification is not valid. To emphasize this point even further, we can think of the geodesic curve as a prediction of what the cross-resistance values for the different antibiotics ought to be without information about when those values must evolve in time.\nThe consequence of this is that it is not clear how to align the continuous geodesic curve with the discrete experimental data. We can, however, make use of methodologies developed for similar purposes in the form of time-warping algorithms. In this case, we can use dynamic time warping (DTW) to align the continuous geodesic curve with the discrete experimental data. DTW is a well-known algorithm in the signal processing community for aligning two time series with different step sizes. Effectively, DTW finds the optimal pairing of points between the two time series, with the constraint that the alignment is monotonic, i.e., if point \\(x_i\\) in one time series is aligned to point \\(y_j\\) in the other time series, then \\(x_{i+1}\\) is either aligned to \\(y_{j}\\) or \\(y_{j+1}\\), never stepping backwards. We use this algorithm to align the points sampled along the geodesic curve in latent space with the resulting experimental curve also in latent space. This point, although subtle, is important: the alignment is done in latent space, not in the original data space. This means that the alignment is done based on the metric tensor learned by the RHVAE, and not on the original data.\nOnce we have the aligned latent space trajectory, we can decode it back to the original data space and plot the resulting experimental curve. Figure 7 and Figure 8 show the results of this procedure for the same few lineages shown in Figure 6. On the left, we show the same latent space trajectory as in Figure 6, on the right, we show the resulting \\(IC_{50}\\) curves for each of the eight antibiotics. The gold curves show the original experimental curves, while the red curves show the geodesic-predicted curves after the dynamic time warping alignment. Although far from perfect, the alignment is remarkably good considering that the predictions were drawn from only knowing the initial and final points of the trajectory in latent space and then computing the shortest path between them. This surprising result begs for further experimental investigation along with extensive theoretical analysis of why this phenomenon occurs. One tantalizing, although highly speculative, possibility is that evolution proceeds in phenotype space following a least action principle, i.e., the path taken by the evolutionary trajectory is the one that minimizes the distance traveled in phenotype space subject to the constraints of phenotype accessibility, as modeled by our genotype-phenotype density function in the main text.\n\n\n\n\n\n\nFigure 7: Geodesic predictions of evolutionary trajectories match experimental data after time alignment. Left panel shows latent space trajectories (gold: experimental path; red: geodesic prediction) for selected lineages. Right panel shows the corresponding IC50 values for all eight antibiotics over time, comparing experimental measurements (gold) with geodesic predictions after dynamic time warping alignment (red).\n\n\n\n\n\n\n\n\n\nFigure 8: Additional examples of geodesic-based predictions of antibiotic resistance evolution. Each panel pair shows another set of lineages where geodesic paths in latent space (left, red curves) were used to predict the temporal evolution of resistance profiles (right). Despite only using initial and final points as inputs, the geodesic predictions (red) capture many features of the actual evolutionary trajectories (gold) after time alignment."
  },
  {
    "objectID": "supplementary.html#geometry-informed-latent-space-reconstruction-of-antibiotic-resistance-landscapes",
    "href": "supplementary.html#geometry-informed-latent-space-reconstruction-of-antibiotic-resistance-landscapes",
    "title": "Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps",
    "section": "",
    "text": "In the main text, Figure 5 shows the ability of the RHVAE to reconstruct the underlying fitness landscapes’ topography from simulated data. There, we demonstrated that the relative position, number of peaks, and overall shape of the fitness landscapes are well-reproduced by the RHVAE latent space. It is therefore interesting to show the same reconstructions using the experimental data from [1].\nAfter training an RHVAE model on the \\(IC_{50}\\) data from [1]—the same model used in the main text in Figure 6—we apply the same methodology to reconstruct the fitness landscapes for the eight antibiotics used in the experiment. Figure 9 shows the reconstructed fitness landscapes for all eight antibiotics along with the latent space metric volume. One obvious difference between the landscapes reconstructed from the simulated and experimental data is the lack of clear peaks in the experimental landscapes. Although it is possible that such peaks do not exist, it is more likely that the lack of peaks is due to the limited resolution of the experimental data, where only a few initial genotypes were measured.\n\n\n\n\n\n\nFigure 9: Reconstructed 2D antibiotic resistance landscapes from experimental data. The first eight panels show the antibiotic resistance landscape reconstructions using a 2D RHVAE trained on the \\(IC_{50}\\) data from [1]. Dotted lines show different level curves on the fitness landscape. The last panel shows the metric volume (the determinant of the metric tensor) of the latent space. The darker the color, the flatter the latent space.\n\n\n\nGiven the lack of regularity in the resulting landscapes, we also show the three-dimensional representations of the resistance landscapes for different antibiotics using experimental data from [1]. Figure 10 shows the same landscapes as in Figure 9, but using the z-axis to represent the fitness value. From this 3D perspective, the lack of clear peaks is more apparent.\n\n\n\n\n\n\nFigure 10: 3D visualization of reconstructed antibiotic resistance landscapes. Three-dimensional representations of the same resistance landscapes shown in Figure 9, using experimental data from [1]. The z-axis represents the fitness value (IC₅₀). This 3D perspective more clearly illustrates the lack of distinct peaks in the experimental data, which may be due to limited sampling of initial genotypes rather than the absence of such features in the actual fitness landscapes.\n\n\n\n\n\nGiven these reconstructed fitness landscapes, we can now explore the evolutionary trajectories of different lineages in the experimental data. In [1], the authors evolved genotypes in the presence of three different antibiotics: Tetracycline (TET), Kanamycin (KAN), and Norfloxacin (NFLX). Naively, if these fitness landscapes are representative of the underlying landscape available to the genotypes, we would expect the trajectories of these lineages to follow a noisy gradient ascent-like path in the latent space, following the fitness gradient predicted by the reconstructed landscapes. Figure 11, Figure 12, and Figure 13 show the evolutionary trajectories of the lineages evolved in the presence of Kanamycin, Tetracycline, and Norfloxacin, respectively. The left and right panels show the same trajectories in the 2D and 3D projection of the latent space, respectively. One of the problematic aspects of these trajectories is that in none of the cases do the trajectories follow a path towards the predicted highest fitness peak. We strongly suspect this is a problem with the experimental setup, rather than the entirety of our approach. Our suspicion is that the selection of initial genotypes completely biased the reconstruction of the fitness landscapes. Some of the lineages selected by the authors were previously evolved in the presence of one of these antibiotics, to then be re-evolved in the presence of the other antibiotics. In other words, there were some strains in the initial pool that were already highly resistant to, say, Kanamycin, and then these strains were re-evolved in the presence of Tetracycline. The presence of these strains explains the existence of those peaks that none of the trajectories reach, since these strains evolved for much longer in the corresponding antibiotic compared to this experimental setup. However, further investigation is needed to determine the cause of this behavior.\n\n\n\n\n\n\nFigure 11: Evolutionary trajectories of lineages evolved in Kanamycin. The left panel shows the 2D projection of evolutionary trajectories in the latent space overlaid on the Kanamycin resistance landscape. The right panel shows the same trajectories in a 3D projection. Different colors represent different lineages. Cross symbols indicate the initial point of the trajectory, while triangles indicate the final point. Note that trajectories do not follow paths toward the highest predicted fitness peaks, suggesting potential biases in the experimental setup or limitations in landscape reconstruction.\n\n\n\n\n\n\n\n\n\nFigure 12: Evolutionary trajectories of lineages evolved in Tetracycline. The left panel shows the 2D projection of evolutionary trajectories in the latent space overlaid on the Tetracycline resistance landscape. The right panel shows the same trajectories in a 3D projection. Different colors represent different lineages. Cross symbols indicate the initial point of the trajectory, while triangles indicate the final point. Similar to the Kanamycin case, trajectories do not consistently follow gradient ascent paths toward fitness peaks.\n\n\n\n\n\n\n\n\n\nFigure 13: Evolutionary trajectories of lineages evolved in Norfloxacin. The left panel shows the 2D projection of evolutionary trajectories in the latent space overlaid on the Norfloxacin resistance landscape. The right panel shows the same trajectories in a 3D projection. Different colors represent different lineages. Cross symbols indicate the initial point of the trajectory, while triangles indicate the final point. As with other antibiotics, the evolutionary paths do not align with the predicted fitness gradients, potentially due to pre-evolved strains in the initial pool biasing landscape reconstruction."
  },
  {
    "objectID": "supplementary.html#neural-network-architecture",
    "href": "supplementary.html#neural-network-architecture",
    "title": "Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps",
    "section": "",
    "text": "The neural network architecture implemented in this study is a Riemannian Hamiltonian Variational Autoencoder (RHVAE) [3]. The network was implemented using the Flux.jl framework [13] via the AutoEncoderToolkit.jl package [14], and consists of three main components: an encoder, a decoder, and a metric chain network for the Riemannian metric computations.\n\n\n\n\n\n\n\n\n\n\n\nParameter\nValue\nDescription\n\n\n\n\nLatent Dimensions\n2\nDimensionality of the latent space\n\n\nHidden Layer Size\n128\nNumber of neurons in hidden layers\n\n\nTemperature (T)\n0.8\nTemperature parameter for RHVAE\n\n\nRegularization (λ)\n0.01\nRegularization parameter\n\n\nNumber of Centroids\n256\nNumber of centroids for the manifold approximation\n\n\n\n\n\n\nThe encoder implements a joint Gaussian log-encoder with the following structure:\n\n\n\nLayer\nOutput Dimensions\nActivation Function\n\n\n\n\nInput\nn_env\\(^*\\)\n-\n\n\nDense 1\n128\nIdentity\n\n\nDense 2\n128\nLeakyReLU\n\n\nDense 3\n128\nLeakyReLU\n\n\nDense 4\n128\nLeakyReLU\n\n\nµ output\n2\nIdentity\n\n\nlogσ output\n2\nIdentity\n\n\n\n\\(^*\\) n_env is the number of environments on which fitness is determined to train the model.\n\n\n\nThe decoder implements a simple Gaussian decoder with the following structure:\n\n\n\nLayer\nOutput Dimensions\nActivation Function\n\n\n\n\nInput\n2\n-\n\n\nDense 1\n128\nIdentity\n\n\nDense 2\n128\nLeakyReLU\n\n\nDense 3\n128\nLeakyReLU\n\n\nDense 4\n128\nLeakyReLU\n\n\nOutput\nn_env\nIdentity\n\n\n\n\n\n\nThe metric chain computes the Riemannian metric tensor with the following structure:\n\n\n\nLayer\nOutput Dimensions\nActivation Function\n\n\n\n\nInput\nn_env\n-\n\n\nDense 1\n128\nIdentity\n\n\nDense 2\n128\nLeakyReLU\n\n\nDense 3\n128\nLeakyReLU\n\n\nDense 4\n128\nLeakyReLU\n\n\nDiagonal Output\n2\nIdentity\n\n\nLower Triangular Output\n1\nIdentity\n\n\n\n\n\n\nThe input data underwent the following preprocessing steps:\n\nLogarithmic transformation of fitness values.\nZ-score standardization (mean = 0, std = 1) per environment.\nK-medoids clustering to select centroids for the manifold approximation.\n\n\n\n\nThe model was implemented using:\n\nJulia programming language.\nFlux.jl for neural network architecture.\nAutoEncoderToolkit.jl for RHVAE-specific components.\nRandom seed set to 42 for reproducibility.\n\nThe complete model state and architecture were saved in JLD2 format for reproducibility and future use.\nAll of the code used to implement the model is available in the GitHub repository for this project."
  },
  {
    "objectID": "supplementary.html#neural-geodesic-architecture",
    "href": "supplementary.html#neural-geodesic-architecture",
    "title": "Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps",
    "section": "",
    "text": "The neural network architecture implemented in this study is a Neural Geodesic model designed to find geodesics on the latent space manifold of a Riemannian Hamiltonian Variational Autoencoder (RHVAE) [12]. The network is implemented using the Flux.jl framework and learns to parameterize geodesic curves between points in the latent space.\n\n\n\n\n\n\n\n\n\n\nParameter\nValue\nDescription\n\n\n\n\nHidden Layer Size\n32\nNumber of neurons in hidden layers\n\n\nInput Dimension\n1\nTime parameter t ∈ [0,1]\n\n\nOutput Dimension\n2\nDimensionality of the latent space\n\n\nEndpoints\nz_init=[0,0], z_end=[1,1]\nStart and end points of the geodesic\n\n\n\n\n\n\nThe neural geodesic implements a feed-forward network that maps from the time domain to the latent space with the following structure:\n\n\n\nLayer\nOutput Dimensions\nActivation Function\n\n\n\n\nInput\n1\n-\n\n\nDense 1\n32\nIdentity\n\n\nDense 2\n32\nTanH\n\n\nDense 3\n32\nTanH\n\n\nDense 4\n32\nTanH\n\n\nOutput\n2\nIdentity\n\n\n\n\n\n\n\nInput Processing\n\nTakes a single time parameter \\(t \\in [0,1]\\).\nMaps to the latent space dimension (2D in this implementation).\nDesigned to learn smooth geodesic curves.\n\nNetwork Structure\n\nFollows a deep architecture with 4 hidden layers.\nUses tanh activation for smooth curve generation.\nIdentity activation at input and output layers for unrestricted range.\n\nOutput Constraints\n\nNetwork outputs must satisfy boundary conditions:\n\n\\(\\gamma(0) = \\underline{z}_{\\text{init}}\\).\n\\(\\gamma(1) = \\underline{z}_{\\text{end}}\\).\n\nGenerated curve represents path in latent space.\n\n\n\n\n\nThe model is implemented with:\n\nJulia programming language.\nFlux.jl for neural network architecture.\nAutoEncode.diffgeo package for geodesic-specific components.\nRandom seed set to 42 for reproducibility.\n\nThe model is designed to work in conjunction with a pre-trained RHVAE model, using its latent space structure to inform the geodesic computation. The complete model state and architecture are saved in JLD2 format for reproducibility and future use.\n\n\n\nThis neural geodesic model complements the RHVAE architecture by:\n\nUsing the same latent space dimensionality.\nLearning geodesics that respect the Riemannian metric of the RHVAE\nProviding a parameterized way to interpolate between latent points"
  },
  {
    "objectID": "supplementary.html#latent-space-alignment-via-procrustes-analysis",
    "href": "supplementary.html#latent-space-alignment-via-procrustes-analysis",
    "title": "Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps",
    "section": "",
    "text": "When working with different dimensionality reduction techniques such as PCA, VAE, and RHVAE, the resulting latent spaces often have arbitrary orientations that make direct comparisons challenging. To facilitate meaningful comparisons between these latent representations and the ground truth phenotype space, we employed Procrustes analysis. This section details the mathematical foundations and implementation of our alignment procedure.\n\n\nProcrustes analysis finds an optimal rigid transformation (rotation, scaling, and potentially translation) that aligns one set of points with another while minimizing the sum of squared differences. Given two sets of points \\(\\underline{\\underline{X}}\\) and \\(\\underline{\\underline{Y}}\\), where each column represents a point in \\(\\mathbb{R}^d\\), the objective is to find a transformation of \\(\\underline{\\underline{X}}\\) that best aligns with \\(\\underline{\\underline{Y}}\\).\nThe Procrustes problem can be formulated as:\n\\[\n\\min_{R, s} \\|\n    \\underline{\\underline{Y}} -\n    s\\underline{\\underline{X}}\\,\\underline{\\underline{R}}\n\\|_F^2\n\\tag{120}\\]\nwhere:\n\n\\(\\underline{\\underline{R}}\\) is a \\(d \\times d\\) orthogonal rotation matrix (\\(\\underline{\\underline{R}}^T\\underline{\\underline{R}} =\n\\underline{\\underline{I}}\\))\n\\(s\\) is a scalar scaling factor\n\\(\\|\\cdot\\|_F\\) denotes the Frobenius norm\n\nIf the data is centered (mean-subtracted), the transformation involves only rotation and scaling. The solution to this optimization problem is obtained through Singular Value Decomposition (SVD).\n\n\n\nOur implementation of Procrustes analysis follows these steps:\n\nData Standardization: Each latent representation (PCA, VAE, RHVAE) and the ground truth phenotype data are standardized to have zero mean and unit standard deviation along each dimension:\n\n\\[\n\\hat{\\underline{\\underline{X}}} =\n\\frac{\n    \\underline{\\underline{X}} -\n    \\underline{\\mu}_X\n}{\n    \\underline{\\sigma}_X\n}\n\\tag{121}\\]\nwhere \\(\\underline{\\mu}_X\\) and \\(\\underline{\\sigma}_X\\) are the mean and standard deviation vectors computed across all points in the respective space.\n\nInner Product Computation: We compute the inner product matrix between the standardized ground truth phenotype data \\(\\hat{\\underline{\\underline{Y}}}\\) and each standardized latent representation \\(\\hat{\\underline{\\underline{X}}}\\):\n\n\\[\n\\underline{\\underline{A}} =\n\\hat{\\underline{\\underline{Y}}}\\,\n\\hat{\\underline{\\underline{X}}}^T\n\\tag{122}\\]\n\nSingular Value Decomposition: We perform SVD on the inner product matrix:\n\n\\[\n\\underline{\\underline{A}} =\n\\underline{\\underline{U}}\\,\n\\underline{\\underline{\\Sigma}}\\,\n\\underline{\\underline{V}}^T\n\\tag{123}\\]\nwhere \\(\\underline{\\underline{U}}\\) and \\(\\underline{\\underline{V}}\\) are orthogonal matrices, and \\(\\underline{\\underline{\\Sigma}}\\) is a diagonal matrix of singular values.\n\nRotation Matrix Computation: The optimal rotation matrix is given by:\n\n\\[\n\\underline{\\underline{R}} =\n\\underline{\\underline{U}}\\,\n\\underline{\\underline{V}}^T\n\\tag{124}\\]\n\nScaling Factor Computation: The optimal scaling factor is calculated as:\n\n\\[\ns = \\frac{\n        \\text{tr}(\\underline{\\underline{\\Sigma}})\n    }{\n        \\|\\hat{\\underline{\\underline{X}}}\\|_F^2\n    }\n\\tag{125}\\]\nwhere \\(\\text{tr}(\\underline{\\underline{\\Sigma}})\\) is the trace of \\(\\underline{\\underline{\\Sigma}}\\) (sum of singular values) and \\(\\|\\hat{\\underline{\\underline{X}}}\\|_F^2\\) is the squared Frobenius norm of \\(\\hat{\\underline{\\underline{X}}}\\).\n\nTransformation Application: Each latent representation is transformed using the corresponding rotation matrix:\n\n\\[\n\\underline{\\underline{X}}_{\\text{aligned}} =\n\\underline{\\underline{R}}\\,\\hat{\\underline{\\underline{X}}}\n\\tag{126}\\]\n\nSimilarity Metric Calculation: We compute a correlation-like measure to quantify the goodness-of-fit between the aligned spaces:\n\n\\[\n\\rho = \\frac{\n    \\text{tr}(\\underline{\\underline{\\Sigma}})\n}{\n    \\|\\hat{\\underline{\\underline{X}}}\\|_F\\|\\hat{\\underline{\\underline{Y}}}\\|_F\n}\n\\tag{127}\\]\nThis metric ranges from 0 (no similarity) to 1 (perfect alignment).\n\n\n\nThe Procrustes alignment enables several critical aspects of our analysis:\n\nDirect Visual Comparison: By aligning all latent spaces to the same orientation as the ground truth phenotype space, we can directly visualize and compare the structural preservation properties of each dimensionality reduction technique.\nQuantitative Assessment: The correlation metric \\(\\rho\\) provides a quantitative measure of how well each latent representation preserves the geometric structure of the ground truth phenotype space.\nGeometric Structure Preservation: The alignment allows us to assess how features such as local neighborhoods, relative distances, and global structure are preserved in each latent representation.\nTrajectory Analysis: For evolutionary trajectories, the alignment enables comparison of path lengths, directional changes, and convergence patterns across different representations.\n\nThis alignment procedure forms the foundation for the comparative analysis presented in the main text, where we evaluate the effectiveness of different dimensionality reduction techniques in capturing the underlying structure of the phenotype space."
  },
  {
    "objectID": "supplementary.html#sec-metropolis-kimura",
    "href": "supplementary.html#sec-metropolis-kimura",
    "title": "Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps",
    "section": "",
    "text": "In the main text, we described a simple algorithm for evolving a population as a biased random walk on a phenotypic space controlled by both a fitness function and a genotype-phenotype density function. Here, we propose an alternative algorithm that combines elements of the Metropolis-Hastings algorithm with Kimura’s classical population genetics theory.\n\n\nIn Section 0.2, we described a simple framework with a fitness function and a genotype-phenotype density function. Here, we use the same framework but introduce a biologically motivated algorithm for the evolutionary dynamics that combines elements of the Metropolis-Hastings algorithm with Kimura’s classical population genetics theory. This approach implements a two-step process that separately models:\n\nThe probability of mutation occurring (mutation accessibility) as governed by the genotype-phenotype density.\nThe probability of fixation within a population (selection) determined by the fitness effect of such mutation and the effective population size.\n\n\n\nIn natural populations, evolution proceeds through two distinct processes:\n\nMutation: New variants arise through random genetic changes. The probability of specific mutations depends on molecular mechanisms and constraints of the genotype-phenotype map.\nFixation: Once a mutation occurs, it must spread through the population to become fixed. The probability of fixation depends on the selection coefficient (fitness advantage) and the effective population size.\n\n\n\n\n\n\nThe probability of a mutation from phenotype \\(\\underline{x}\\) to \\(\\underline{x}'\\) is modeled using a Metropolis-like criterion based on the genotype-phenotype density:\n\\[\n\\pi_{\\text{mut}}(\\underline{x} \\to \\underline{x}') =\n\\min\\left(1, \\left(\\frac{GP(\\underline{x}')}{GP(\\underline{x})}\\right)^{\\beta}\\right)\n\\tag{128}\\]\nHere, \\(GP(\\underline{x})\\) represents the genotype-phenotype density at phenotype \\(\\underline{x}\\), and \\(\\beta\\) is a parameter controlling the strength of mutational constraints. The higher \\(\\beta\\) is, the less likely a mutation going downhill in genotype-phenotype density is to be accepted.\n\n\n\nOnce a mutation occurs, its probability of fixation in a population of effective size \\(N\\) is given by Kimura’s formula\n\\[\n\\pi_{\\text{fix}}(\\underline{x} \\to \\underline{x}') =\n\\frac{1 - e^{-2s}}{1 - e^{-2Ns}}\n\\tag{129}\\]\nwhere \\(s\\) is the selection coefficient. We compute this selection coefficient as the difference in fitness between the new and old phenotype divided by the fitness of the old phenotype:\n\\[\ns = \\frac{F_E(\\underline{x}') - F_E(\\underline{x})}{F_E(\\underline{x})}\n\\tag{130}\\]\nThis equation captures a fundamental result from population genetics: beneficial mutations (\\(s &gt; 0\\)) have a higher probability of fixation, with the probability approaching \\(2s\\) for small positive selection coefficients and approaching 1 for large positive selection coefficients. Deleterious mutations (\\(s &lt; 0\\)) have an exponentially decreasing probability of fixation as population size increases.\n\n\n\n\nThe overall acceptance probability—defined as the probability of a proposed step \\(\\underline{x} \\rightarrow \\underline{x}'\\)—is the product of the mutation probability and the fixation probability:\n\\[\n\\pi_{\\text{accept}}(\\underline{x} \\to \\underline{x}') =\n\\pi_{\\text{mut}}(\\underline{x} \\to \\underline{x}') \\cdot\n\\pi_{\\text{fix}}(\\underline{x} \\to \\underline{x}')\n\\tag{131}\\]\nThis two-step process better reflects the biological reality of evolution, where both mutational accessibility and selection contribute to evolutionary trajectories.\n\n\n\n\nFigure 14 shows the effect of population size parameter \\(N\\) influences evolutionary trajectories in our Metropolis-Kimura framework. Through a series of panels showing both phenotypic trajectories (upper) and fitness dynamics (lower), we can observe how different population sizes affect the balance between exploration and exploitation in phenotypic space.\nAt small population sizes (left panels), the evolutionary trajectories exhibit highly stochastic behavior, with populations taking meandering paths through the phenotypic landscape. This represents a regime where genetic drift plays a dominant role. As population size increases (moving right), the trajectories become increasingly deterministic, with populations taking more direct paths toward fitness peaks, while avoiding genotype-to-phenotype density valleys. This transition reflects a shift from drift-dominated to selection-dominated dynamics, where populations more strictly follow the local fitness gradient. The fitness trajectories in the lower panels further reinforce this pattern, showing more erratic fitness fluctuations at small population sizes and more monotonic increases at large population sizes, consistent with stronger selective pressures.\n\n\n\n\n\n\nFigure 14: Effect of population size on adaptive dynamics. Upper panels: Population trajectories on a landscape with a single fitness peak and four genotype-phenotype density peaks. Black dashed lines indicate fitness landscape contours, while white solid lines show genotype-phenotype density contours. Colored lines represent independent population trajectories, with initial positions (crosses) and final positions (triangles). Lower panels: Temporal evolution of population fitness. Panels from left to right show increasing values of population size (\\(N\\)), demonstrating increasingly deterministic adaptive trajectories.\n\n\n\n\n\n\nHaving developed this alternative algorithm for the population dynamics, we reproduce the results from the main text using this new algorithm. In particular, Figure 2, 4, and 5 in the main text used synthetic data generated by the algorithm described in Section 0.2. We now show that the same results can be obtained using the Metropolis-Kimura algorithm.\n\n\n\n\n\n\n\n\nFigure 15: Evolutionary dynamics on phenotype space with Metropolis-Kimura dynamics. (A) Top: Metropolis-like evolutionary dynamics on phenotype space. Each line represents the trajectory of a lineage as it evolves over time, with crosses and triangles denoting the initial phenotypic coordinate of a few selected lineages. Note that trajectories tend to move towards higher fitness values, avoiding low genotype-to-phenotype density regions. Bottom: Fitness over time of the same trajectories shown above. (B) The same trajectories shown in (A) overlaid on different fitness maps determined by different environments. Although the phenotypic coordinates of the genotypes remain the same (top panels), the resulting fitness readouts change as the topography of the environment-dependent fitness landscape changes (bottom panels).\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16: Geometry-informed latent space captures phenotypic structure and improves reconstruction accuracy. (A) Latent space coordinates of simulated fitness profiles colored by lineage, after alignment with the ground truth phenotype space using Procrustes analysis. The RHVAE better preserves phenotypic relationships than PCA or vanilla VAE, as evidenced by the highlighted points (diamond markers). (B) Comparison of all pairwise Euclidean distances between genotypes in the ground truth phenotypic space versus the different latent spaces, showing RHVAE better preserves the original distances. Given the large number of data points and the even larger number of pairwise comparisons, the data are shown as a smear rather than single points. (C) Reconstruction error (MSE) comparison showing 2D-RHVAE achieves accuracy comparable to a 10-dimensional PCA model. (D) Metric volume visualization of the RHVAE latent space (left) and with projected data points (right), revealing regions of high curvature that correspond to areas of low genotype-phenotype density in the simulation.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 17: Geometry-informed latent space enables accurate reconstruction of complex fitness landscapes. Comparison of ground truth and reconstructed fitness landscapes across multiple environments. The first row shows examples of the original simulated fitness landscapes used to generate the data. Subsequent rows show reconstructions using the RHVAE (second row), VAE (third row), and PCA (fourth row) models. The RHVAE accurately captures the underlying topography, including the number and relative positions of fitness peaks. While the VAE captures general landscape shapes, it lacks a geometric metric to define prediction reliability regions. PCA fails to represent the nonlinear structure of the landscapes, demonstrating the limitations of linear dimensionality reduction for complex fitness landscape reconstruction."
  },
  {
    "objectID": "paper.html",
    "href": "paper.html",
    "title": "Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps",
    "section": "",
    "text": "Abstract\n\nElucidating the complex relationships between genotypes, phenotypes, and fitness remains one of the fundamental challenges in evolutionary biology. Part of the difficulty arises from the enormous number of possible genotypes and the lack of understanding of the underlying phenotypic differences driving adaptation. Here, we present a computational method that takes advantage of modern high-throughput fitness measurements to learn a map from high-dimensional fitness profiles to a low-dimensional latent space in a geometry-informed manner. We demonstrate that our approach using a Riemannian Hamiltonian Variational Autoencoder (RHVAE) outperforms traditional linear dimensionality reduction techniques by capturing the nonlinear structure of the phenotype-fitness map. When applied to simulated adaptive dynamics, we show that the learned latent space retains information about the underlying adaptive phenotypic space and accurately reconstructs complex fitness landscapes. We then apply this method to a dataset of high-throughput fitness measurements of E. coli under different antibiotic pressures and demonstrate superior predictive power for out-of-sample data compared to linear approaches. Our work provides a data-driven implementation of Fisher’s geometric model of adaptation, transforming it from a theoretical framework into an empirically grounded approach for understanding evolutionary dynamics using modern deep learning methods."
  },
  {
    "objectID": "paper.html#simple-phenotype-fitness-model",
    "href": "paper.html#simple-phenotype-fitness-model",
    "title": "Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps",
    "section": "Simple Phenotype-Fitness Model",
    "text": "Simple Phenotype-Fitness Model\nWith the advent of high-throughput fitness assays, determining the fitness of multiple genotypes in a plethora of different environments has become a routine procedure in biology [7], [8], [9], [10]. Nevertheless, we argue that we still lack a simple conceptual framework to guide our interpretation of the patterns observed in these datasets. To that end, we propose a simple model of phenotype-fitness dynamics to serve as a guidepost for our interpretation of the data.\nFigure 1(A) shows a schematic of the genotype-phenotype-fitness map. A genotype \\(g\\) produces a set of measurable traits or phenotypes \\(\\underline{p}\\)—depicted as two potential molecular phenotypes, such as an efflux pump rate and the sensitivity of a membrane receptor. The fitness of each genotype is determined by the value of these phenotypes and the environment \\(E\\)—depicted as the doubling time of a bacterial population. To formalize this mathematically, we represent genotypes as points in a multi-dimensional phenotypic space, where each dimension is a different trait (\\(\\underline{p} \\in \\mathbb{R}^P\\), where \\(P\\) is the number of traits). Thus, any genotype \\(g\\) has a unique mapping to a phenotypic coordinate. However, this mapping is not necessarily bijective, i.e., multiple genotypes may map to the same phenotype. To represent the likelihood of sampling some genotype \\(g\\) that maps to some coordinate \\(\\underline{p}\\) in phenotype space, we introduce a genotype-to-phenotype density map \\(GP\\) that takes a given phenotypic coordinate \\(\\underline{p}\\) and returns a number proportional to the probability of sampling a genotype \\(g\\) that maps to \\(\\underline{p}\\). This function captures the empirical observation that the genotype-to-phenotype map is highly redundant, with many genotypes mapping to equivalent phenotypes and some phenotypes being extremely rare [5]. Figure 1(B, left) shows a schematic of one possible \\(GP\\) map overlaid on a 2D phenotype space. For illustration purposes, we set the \\(GP\\) map to contain four negative Gaussian peaks, representing four regions of the phenotype space that are unlikely to be sampled (see Supplementary Material for a more detailed discussion). The location and size of these peaks are assumed to be fixed by the genotype-to-phenotype map, irrespective of the environment. Finally, we introduce a fitness function \\(F\\) that maps a phenotypic coordinate \\(\\underline{p}\\) to a fitness value in a particular environment. We assume the topography of the fitness landscape (the number, size, and position of fitness peaks) is determined by the environment \\(E\\). Figure 1(B, right) shows a schematic of a simple fitness landscape with a single symmetric Gaussian peak.\nIn recent experimental evolution studies, genotypes are evolved under a fixed environment, but the fitness is measured across multiple environments [7], [9]. To reflect this, Figure 1(C) shows the effect of changing the environment on the phenotype-fitness map. Even though the mapping from genotype to phenotype (coordinates of the example points) and the genotype-to-phenotype density map (white contours) are invariant to the environment, the fitness landscape (black dashed contours) changes as the environment varies. We model this by changing the topography of the fitness map in terms of the number, position, height, and width of the Gaussian peaks. Therefore, we conceptualize the measured fitness profiles on multiple environments as measuring the heights of a fixed phenotypic coordinate on multiple fitness landscapes. Figure 1(D) shows the equivalent experimental observable—the fitness values of any genotype \\(g\\) on multiple environments, hereafter referred to as a fitness profile. We ask whether extracting information about the latent phenotypic coordinates is possible based only on such high-dimensional fitness profiles. Conceptually, this is equivalent to asking whether it is possible to reverse the mapping from phenotypes to fitness values and predict the phenotypic coordinates of a genotype based only on its fitness values across multiple environments.\n\n\n\n\n\n\nFigure 1: Model of phenotype-fitness dynamics. (A) Schematic of the genotype-phenotype-fitness map. A pair of genotypes \\(g^{(\\text{b})}\\) and \\(g^{(\\text{r})}\\) are shown. These genotypes map to two relevant phenotypes \\(p_1\\) and \\(p_2\\) via a function \\(\\phi\\). The fitness of each genotype is determined by the value of these phenotypes and their relevance in the environment via a function \\(F\\). (B) Phenotype space functions. Left: The genotype-to-phenotype density map \\(GP\\) where the probability of observing a phenotype \\(\\underline{p}\\) is shown as a function of the phenotype values \\(p_1\\) and \\(p_2\\). White lines show iso-probability contours. Right: The fitness landscape \\(F\\) as a function of the phenotype values \\(p_1\\) and \\(p_2\\). White lines show iso-fitness contours. The points show the coordinates of the two genotypes in phenotype space. (C) Examples of the effect of changing the environment on the phenotype-fitness landscape. The phenotypic coordinates of any genotype and the genotype-to-phenotype density map are not affected by changes in the environment. The fitness landscape, however, does change as the environment changes. Green landscape shows the evolution condition where selection happens. Gray landscapes show other environments with different topographies (position, number, and height of fitness peaks). Black dashed lines indicate fitness landscape contours, while white solid lines show genotype-phenotype density contours. (D) Fitness profiles. The fitness values of any genotype measured across multiple environments represent an experimental observable."
  },
  {
    "objectID": "paper.html#evolutionary-dynamics-on-phenotype-space",
    "href": "paper.html#evolutionary-dynamics-on-phenotype-space",
    "title": "Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps",
    "section": "Evolutionary Dynamics on Phenotype Space",
    "text": "Evolutionary Dynamics on Phenotype Space\nHaving defined a simple phenotype-fitness model, we now instantiate a set of rules for evolutionary dynamics on this phenotypic space. We implement a simple model that captures two key features of evolutionary processes: the tendency of populations to move towards higher fitness and the constraints imposed by the accessibility of different phenotypes. We frame evolution as a biased random walk where each step is influenced by both the fitness landscape \\(F(\\underline{p})\\) and the genotype-phenotype density \\(GP(\\underline{p})\\).\nAt each time step, the population can transition from its current phenotype \\(\\underline{p}\\) to a new phenotype \\(\\underline{p}'\\), where the proposed phenotype \\(\\underline{p}'\\) is drawn from a proposal distribution \\(q(\\underline{p}' \\mid \\underline{p})\\). Whether this transition occurs—equivalent to the mutation being fixed in the population—depends on the relative fitness of the new phenotype and how easily it can be reached through mutations. Specifically, as derived in the Supplementary Materials, the probability of accepting a transition is given by\n\\[\nP_{\\text{accept}} = \\min\\left(\n    1,\n    \\frac{F(\\underline{p}')^\\beta GP(\\underline{p}')^\\beta}\n    {F(\\underline{p})^\\beta GP(\\underline{p})^\\beta}\n\\right),\n\\tag{1}\\]\nwhere \\(\\beta\\) is a parameter that controls the balance between selection and drift. This parameter can be interpreted as the strength of selection—higher values of \\(\\beta\\) lead to more deterministic trajectories that closely follow fitness gradients, while lower values allow for more stochastic exploration of the phenotype space. Detailed mathematical derivations and analysis of this algorithm can be found in the Supplementary Materials.\nThis formulation represents a simplified model of evolutionary dynamics with specific assumptions: First, transitions to phenotypes with higher fitness are more likely to be fixed, reflecting selection’s influence in our model, though in natural populations even beneficial mutations may be lost due to drift and other factors. Second, transitions to lower fitness phenotypes can occur with some probability, incorporating stochastic elements of evolution. Finally, the \\(GP(\\underline{p})\\) term accounts for the mutational accessibility of phenotypes—phenotypes requiring rare genetic variants are less likely to be reached even if highly fit. This approach draws on elements from adaptive dynamics and Metropolis-Hastings algorithms applied to evolutionary processes. Figure 2(A, top) shows example trajectories in phenotypic space of multiple lineages evolving on a simple single-peaked fitness landscape. Figure 2(A, bottom) shows the fitness trajectory of these evolving lineages over time, i.e., the equivalent of the experimental observable we can access in a laboratory setting. When we examine these same phenotypic trajectories under different environmental conditions (Figure 2(B)), we observe that, as expected, while the path through phenotype space remains fixed, the fitness values we measure vary because each environment imposes its unique fitness landscape. The fitness values in all environments for a given lineage at a given time point define one fitness profile, as the ones depicted in Figure 1(D). As shown in the Supplementary Materials, our main results using a more detailed population genetics framework yielded consistent conclusions.\nIn what follows, we use synthetic data generated from this model, where ten lineages (initial phenotype coordinates) in two replicates (different instantiations of the random dynamics) evolve for 300 time steps on 50 different fitness landscapes (different topographies, meaning number, position, and relative height of fitness peaks). For each of these conditions, the fitness in all other environments is also determined, giving a high-dimensional dataset analogous to the ones in recent experimental studies [7], [9], [10], [16]. We use this dataset to test whether it is possible to reverse the mapping from phenotypes to fitness values and reconstruct the phenotypic coordinates of a genotype based only on fitness profiles.\n\n\n\n\n\n\nFigure 2: Evolutionary dynamics on phenotype space. (A) Top: Metropolis-like evolutionary dynamics on phenotype space. Each line represents the trajectory of a lineage as it evolves over time, with crosses and triangles denoting the initial phenotypic coordinate of a few selected lineages. Note that trajectories tend to move towards higher fitness values, avoiding low genotype-to-phenotype density regions. Bottom: Fitness over time of the same trajectories shown above. (B) The same trajectories shown in (A) overlaid on different fitness maps determined by different environments. Although the phenotypic coordinates of the genotypes remain the same (top panels), the resulting fitness readouts change as the topography of the environment-dependent fitness landscape changes (bottom panels)."
  },
  {
    "objectID": "paper.html#data-driven-dimensionality-reduction-via-geometric-variational-autoencoders",
    "href": "paper.html#data-driven-dimensionality-reduction-via-geometric-variational-autoencoders",
    "title": "Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps",
    "section": "Data-Driven Dimensionality Reduction via Geometric Variational Autoencoders",
    "text": "Data-Driven Dimensionality Reduction via Geometric Variational Autoencoders\nUsing our simulated fitness profiles that mimic data from modern experimental evolution studies [7], [8], [9], [10], [16], we now develop a data-driven approach to infer phenotypic coordinates from fitness measurements across different environments. Figure 3(A) shows a schematic depiction of the idea behind dimensionality reduction applied to fitness profiles. With fitness profiles as vectors on a high-dimensional data space, dimensionality reduction techniques implicitly assume that the positions of points in the data space are highly constrained to occupy only a subset of the available volume. More formally, dimensionality reduction techniques assume that the data points lie on a low-dimensional manifold embedded in the high-dimensional data space. Here, low-dimensionality refers to having the effective number of degrees of freedom required to describe the data being lower than the dimension of the data space. The goal of dimensionality reduction is to find such an embedded manifold that captures the underlying structure of the data. When possible, plotting this embedded manifold as a flat space (either two or three dimensions) allows for the visualization of the underlying data structure. Given the conceptual picture we have developed so far, the hope is that the low-dimensional latent space captures features of the structure of the underlying phenotypic space.\nRecently, the field has focused on linear dimensionality reduction techniques such as singular value decomposition [7] or related methods such as sparse structure discovery [17]. Although computationally efficient, these methods are limited by the rigid assumption that fitness is a linear function of some latent phenotypes. To step away from such strong assumptions, we can take advantage of the advances in deep-learning-based dimensionality reduction techniques, such as the variational autoencoder (VAE) [18]. However, the original VAE framework comes with the limitation that the learned latent representation—built from nonlinear transformations of the input data—does not contain any geometric information about how such nonlinear transformations affect the geometry of the learned space [19]. This is analogous to having a 2D map of the surface of the earth (the latent space), where, to know how distances between two points on the map correspond to distances on the surface of the planet (the data space), we need a position-dependent “scale bar”—more formally, a metric tensor—that allows us to convert distances between representations. This metric tensor is not encoded in the original VAE framework, rendering distances between points in the latent space meaningless as they are not directly related to the distances in the data space.\nTo address both limitations on the linear and non-geometric nature of popular methods, we propose the use of a recent extension of the VAE framework—the Riemannian Hamiltonian VAE (RHVAE) [15]—which models the latent space as a Riemannian manifold whose metric tensor is also learned from the data, allowing for the computation of meaningful distances between points. Figure 3(B) shows a schematic depiction of the RHVAE architecture. The network consists of three main components: an encoder network, a decoder network, and a specialized network that learns the metric tensor of the latent space. The numbers below the architecture point to the conceptual steps of the RHVAE workflow shown in Figure 3(C). The RHVAE is conceptually similar to the traditional VAE, taking a high-dimensional fitness input \\(\\underline{f}\\) as input into an encoder network. This encoder network outputs a sampled latent coordinate \\(\\underline{z}\\) [18]. However, before passing this sampled coordinate through a decoder network to reconstruct the fitness profile, the RHVAE uses concepts from Riemannian geometry and Hamiltonian mechanics to jointly train a separate network that learns the metric tensor of the latent space. This metric tensor network, thus, contains all the information needed to map distances in the low-dimensional map to distances in the original data space. We consider this feature of the RHVAE to be of utmost importance if the learned latent space is to be taken seriously as a representation of the underlying phenotypic space. Just as a map without a scale bar is not a useful guide to navigate the terrain, a latent space without a metric tensor is an incomplete representation of the underlying phenotypic space. We direct the reader to the Supplementary Material for a mathematically rigorous description of the model.\nNext, we turn our attention to applying the RHVAE to the simulated fitness profiles. It is important to highlight that at no point does the RHVAE obtain any knowledge about the identity of the input data. In other words, the RHVAE has no information about the lineage or time point of the fitness profile that it is processing. The network only uses the unidentified fitness profiles to learn the geometric structure of the latent space.\n\n\n\n\n\n\nFigure 3: Dimensionality reduction via geometric variational autoencoder. (A) Schematic of dimensionality reduction applied to fitness profiles. High-dimensional fitness vectors are embedded onto a lower-dimensional manifold that captures the underlying structure of the data. (B) Architecture of the Riemannian Hamiltonian VAE (RHVAE). The network consists of three components: an encoder that maps high-dimensional fitness profiles to latent coordinates, a decoder that reconstructs the original fitness profiles, and a metric tensor network that learns the geometric structure of the latent space. (C) RHVAE workflow. The high-dimensional input is mapped into a low-dimensional latent space coordinate. From there, the network learns the metric tensor of the latent space via a specialized network. The latent coordinate is then decoded into an accurate reconstruction of the original fitness profile input."
  },
  {
    "objectID": "paper.html#geometry-informed-latent-space-captures-the-underlying-phenotypic-space",
    "href": "paper.html#geometry-informed-latent-space-captures-the-underlying-phenotypic-space",
    "title": "Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps",
    "section": "Geometry-Informed Latent Space Captures the Underlying Phenotypic Space",
    "text": "Geometry-Informed Latent Space Captures the Underlying Phenotypic Space\nAs described in the Supplementary Material, we trained three models on the simulated profiles depicted in Figure 2 for comparison: A linear model (via PCA), a vanilla VAE, and an RHVAE. Each model allows us to project the high-dimensional fitness profiles into a low-dimensional latent space. Nevertheless, none of the resulting latent spaces need to be aligned with the ground truth phenotype, as they were constructed with no information about the relative coordinates in phenotype space. To visually compare whether the resulting latent coordinates for the simulated data preserve the general structure of the phenotype space, we aligned each latent space to the ground truth phenotype space using Procrustes analysis (see Supplementary Material). Figure 4(A) shows the resulting latent space coordinates colored by the lineage of the simulated profiles. Qualitatively, we observe that, although all models partially capture the relative positions of each lineage with respect to the others, the RHVAE-derived latent space appears to qualitatively preserve the structure of the phenotype space better than the other two models; for example, the four regions of low genotype-to-phenotype density are clearly visible in the RHVAE-derived latent space, while they are not as pronounced in the other two latent spaces. Moreover, when looking at the commonly used PCA-derived latent space, we might be misled into believing that the two highlighted points (diamond markers) are genotypes with very similar phenotypic effects, when, in fact, they are extremely different, as captured by the nonlinear latent spaces. This point is further highlighted in Figure 4(B), where we show the comparison of all pairwise distances between genotypes in the ground truth phenotypic space vs. the different latent spaces. Overall, the RHVAE-derived latent space preserves the pairwise distances between the genotypes better than the other two models.\nHaving visually confirmed that the RHVAE-derived latent space preserves the structure of the phenotype space better than the other two models, we now investigate whether these latent coordinates also contain enough information to reconstruct the fitness profiles. To this end, we compare the reconstruction accuracy of the RHVAE to that of the PCA and VAE models. Figure 4(C) shows the reconstruction error, defined as\n\\[\n\\text{MSE} =\n\\frac{1}{N}\n\\sum_{n=1}^{N}\n\\sum_{e=1}^{E}\n\\left( f^{(n)}_e - \\hat{f}^{(n)}_e \\right)^2,\n\\tag{2}\\]\nwhere \\(f^{(n)}_e\\) and \\(\\hat{f}^{(n)}_e\\) are the ground truth and reconstructed fitness values, respectively, for the \\(n\\)-th genotype in the \\(e\\)-th environment. We observe that the vanilla 2D-VAE model achieves a reconstruction error equivalent to that of an eight-dimensional PCA model. In contrast, the 2D-RHVAE consistently outperforms the other two models, achieving a reconstruction error on par with a ten-dimensional PCA model. This further highlights the ability of the RHVAE to capture the underlying nonlinear structure of the phenotype space while maintaining high reconstruction accuracy.\nOne of the main advantages of the RHVAE is its ability to learn a metric tensor that captures the local geometry of the latent space. A way to visualize this geometric information is to plot the so-called metric volume, defined as the determinant of the metric tensor at each point in the latent space. Intuitively, the determinant of the metric tensor at a given point can be interpreted as the level of deformation of the latent space at that point. Small values of the metric volume indicate that the latent space is locally flat, while large values suggest significant local curvature. Figure 4(D, left) shows the metric volume of the latent space derived from the RHVAE model. The lighter the color, the larger the metric volume, and thus the larger the local deformation. When overlaying the points over this metric volume (Figure 4(D, right)), we observe that everything outside the data cloud is locally curved. Knowing that neural networks are not able to generalize well outside the data cloud, having this feature is particularly nice because the curvature in the latent space defines the regions where the trained model is able to make accurate predictions. Moreover, we note that regions of high curvature within the data cloud exactly correspond to the regions of low genotype-phenotype density we imposed in the simulation. This suggests that, for a properly sampled phenotypic space, the geometric information learned by the RHVAE is able to capture the non-uniformity of the phenotypic density as variations in the curvature of the latent space, offering a data-driven tool to identify accessible evolutionary paths in phenotype space.\n\n\n\n\n\n\nFigure 4: Geometry-informed latent space captures phenotypic structure and improves reconstruction accuracy. (A) Latent space coordinates of simulated fitness profiles colored by lineage, after alignment with the ground truth phenotype space using Procrustes analysis. The RHVAE better preserves phenotypic relationships than PCA or vanilla VAE, as evidenced by the highlighted points (diamond markers). (B) Comparison of all pairwise Euclidean distances between genotypes in the ground truth phenotypic space versus the different latent spaces, showing RHVAE better preserves the original distances. Given the large number of data points and the even larger number of pairwise comparisons, the data are shown as a smear rather than single points. (C) Reconstruction error (MSE) comparison showing 2D-RHVAE achieves accuracy comparable to a 10-dimensional PCA model. (D) Metric volume visualization of the RHVAE latent space (left) and with projected data points (right), revealing regions of high curvature that correspond to areas of low genotype-phenotype density in the simulation."
  },
  {
    "objectID": "paper.html#sec-fitness-landscapes",
    "href": "paper.html#sec-fitness-landscapes",
    "title": "Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps",
    "section": "Geometry-Informed Latent Space Reconstruction of Fitness Landscapes",
    "text": "Geometry-Informed Latent Space Reconstruction of Fitness Landscapes\nWith the ability to capture the underlying structure of the phenotypic space as shown in Figure 4, we can investigate whether the RHVAE can be used to reconstruct the underlying fitness landscapes. For this, we take advantage of the fact that the RHVAE decoder represents a continuous function from latent coordinates to fitness values in different environments. Scanning a grid of latent coordinates allows us to evaluate the fitness landscape at any point in the latent space for all three models shown in Figure 4. Figure 5 shows examples of ground truth fitness landscapes, used to generate the data (first row), and the resulting inferred fitness landscapes for the RHVAE, VAE, and PCA models (second, third, and fourth rows, respectively). Qualitatively, we see that the RHVAE is able to capture the underlying topography of the fitness landscapes, where the number of peaks and their relative positions are similar to the ground truth landscapes. Although the VAE model can also capture the general shape of the fitness landscapes, the lack of a metric in this space—limiting the predictions to the local neighborhood of the training data—makes it difficult to define where to trust the predictions of the model. The PCA model, on the other hand, is not able to capture the underlying structure of the fitness landscapes as there is a nonlinear structure that cannot be captured by a linear transformation.\n\n\n\n\n\n\nFigure 5: Geometry-informed latent space enables accurate reconstruction of complex fitness landscapes. Comparison of ground truth and reconstructed fitness landscapes across multiple environments. The first row shows examples of the original simulated fitness landscapes used to generate the data. Subsequent rows show reconstructions using the RHVAE (second row), VAE (third row), and PCA (fourth row) models. The RHVAE accurately captures the underlying topography, including the number and relative positions of fitness peaks. While the VAE captures general landscape shapes, it lacks a geometric metric to define prediction reliability regions. PCA fails to represent the nonlinear structure of the landscapes, demonstrating the limitations of linear dimensionality reduction for complex fitness landscape reconstruction."
  },
  {
    "objectID": "paper.html#evolutionary-dynamics-of-antibiotic-resistance-in-latent-space",
    "href": "paper.html#evolutionary-dynamics-of-antibiotic-resistance-in-latent-space",
    "title": "Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps",
    "section": "Evolutionary Dynamics of Antibiotic Resistance in Latent Space",
    "text": "Evolutionary Dynamics of Antibiotic Resistance in Latent Space\nHaving shown how the RHVAE can capture the underlying structure of the phenotypic space, we now turn to the application of the model to the analysis of antibiotic adaptation dynamics. We use the data from [9] where the authors evolved different E. coli strains on three different antibiotics: kanamycin (KM), norfloxacin (NFLX), and tetracycline (TET). For this, they used a multi-well plate format where the rows contained a gradient of different antibiotic concentrations, as depicted in Figure 6(A). Every day, a fresh plate with the same arrangement of antibiotics was inoculated with cells from the well in the evolving condition with the highest concentration of antibiotic that still allowed growth. This process was repeated every day for 27 days. Figure 6(B) shows the raw data output from one of the experiments. The optical density is plotted as a function of the antibiotic concentration, obtaining the expected sigmoidal curve. Since this particular example shows data from evolution in the antibiotic being used as the \\(x\\)-axis, we can see how, as expected, the sigmoidal curve shifts to the right as the population evolves increasing resistance to the antibiotic over time. From these titration curves, we can use Bayesian inference to extract the value of the antibiotic concentration at which the optical density is 50% of the maximum value. This value—known as the antibiotic \\(IC_{50}\\)—is a measure of the antibiotic resistance of the evolved strains (see Supplementary Material for more details).\nFigure 6(C) shows the time trajectories of the \\(IC_{50}\\) in all eight antibiotics for all cultures evolved in TET. Over time, all lineages increase their \\(IC_{50}\\) in TET; however, the trend for all other antibiotics is more complex. Following the same approach as in Figure 4, Figure 6(D) shows the resulting latent spaces for PCA, VAE, and RHVAE trained on the \\(IC_{50}\\) values for all eight antibiotics. Each color represents a lineage evolved on a different antibiotic. These projections show that the latent coordinates for different lineages are more disentangled in the nonlinear latent spaces than in PCA. Moreover, the rightmost panel shows that in terms of reconstruction error, RHVAE shows the lowest error—comparable to that of a 5-dimensional PCA.\n\n\n\n\n\n\nFigure 6: Evolutionary dynamics of antibiotic resistance captured in latent space reveals complex adaptation patterns. (A) Experimental setup using multi-well plates with antibiotic concentration gradients for evolution experiments. (B) Optical density measurements as a function of antibiotic concentration, showing rightward shifts of sigmoidal curves as resistance evolves over time. (C) Time trajectories of \\(IC_{50}\\) values across all eight antibiotics (tetracycline (TET), kanamycin (KM), mitomycin C (MMC), norfloxacin (NFLX), 4-nitroquinoline 1-oxide (NQO), phleomycin (PLM), sodium dichromate dihydrate (SDC), and sodium salicylate (SS)) for lineages evolved in tetracycline, demonstrating direct resistance increase in the selection antibiotic and complex collateral effects in other antibiotics. Error bars represent the 95% credible region as obtained via Bayesian inference (see Supplementary Material) (D) Latent space projections using PCA, VAE, and RHVAE (first three panels) trained on \\(IC_{50}\\) values, with colors representing different evolution conditions, showing improved disentanglement of lineages in nonlinear latent spaces and superior reconstruction accuracy of RHVAE compared to other dimensionality reduction methods. Different colors represent different lineages evolved in different antibiotics. The rightmost panel shows the reconstruction error of the three methods as a function of the number of PCs used for dimensionality reduction."
  },
  {
    "objectID": "paper.html#non-linear-latent-space-coordinates-are-more-predictive-of-out-of-sample-data",
    "href": "paper.html#non-linear-latent-space-coordinates-are-more-predictive-of-out-of-sample-data",
    "title": "Learning the Shape of Evolutionary Landscapes: Geometric Deep Learning Reveals Hidden Structure in Phenotype-to-Fitness Maps",
    "section": "Non-Linear Latent Space Coordinates are more Predictive of Out of Sample Data",
    "text": "Non-Linear Latent Space Coordinates are more Predictive of Out of Sample Data\nIn our simulations, we designed abstract phenotypic coordinates that determined fitness across environments. Applying our proposed method demonstrated that the nonlinear latent space can recover the relative positions in this abstract phenotypic space using fitness data only. While these latent coordinates don’t necessarily represent interpretable biological traits, they effectively capture the underlying structure of the space that determines the genotype fitness in different environments. This success in recovering structural relationships from simulated data encouraged us to test whether the equivalent latent coordinates recovered from experimental data can be used to make more accurate predictions of out-of-sample data.\nTo test this hypothesis, we adapted the cross-validation approach developed by [7]. Their original method assesses how linear components predict out-of-sample data by dividing the data matrix into quadrants based on genotypes and antibiotics. We modified this approach for our nonlinear models as shown in Figure 7(A). First, we train our models (VAE and RHVAE) on fitness profiles from all but one antibiotic to generate latent space coordinates. Then, we freeze these coordinates and train only the decoder part to predict responses to the missing antibiotic. This approach allows us to directly compare how well different dimensionality reduction methods capture information needed for prediction. In essence, we use seven antibiotics to place each genotype in latent space, then test how accurately these coordinates predict resistance to the eighth antibiotic. We direct interested readers to the Supplementary Material for a detailed description of the of both approaches.\nFigure 7(B) compares the predictive power of the three models across different numbers of linear latent space dimensions in terms of reconstruction error. For each panel, the accuracy of the predicted \\(IC_{50}\\) value on the missing antibiotic over the validation set is plotted as a function of the number of linear dimensions used in the SVD bi-cross validation approach. The horizontal dashed lines represent the accuracy of the predicted \\(IC_{50}\\) value for the 2D-VAE (dark blue) and 2D-RHVAE (dark red) models. For all antibiotics, the 2D nonlinear latent space coordinates are more predictive of the missing antibiotic \\(IC_{50}\\) values than any number of possible linear dimensions. This result is consistent with the hypothesis that the nonlinear latent space coordinates capture more information about the phenotypic state of the system under study, allowing for more accurate predictions of out-of-sample data. Furthermore, increasing the dimensionality of the nonlinear latent space from two to three dimensions only marginally improves the predictive power of the model, suggesting that the 2D nonlinear latent space captures most of the information about the phenotypic state of the system (see Supplementary Material for a detailed discussion).\n\n\n\n\n\n\nFigure 7: Nonlinear latent space models demonstrate superior predictive power for out-of-sample antibiotic data. (A) Schematic representation of the cross-validation approach used to test predictive power: training a full model on all but one antibiotic to generate latent space coordinates (left), then freezing the encoder parameters and training a decoder-only model on the missing antibiotic data (right). (B) Comparison of predictive accuracy across models, showing reconstruction error for the missing antibiotic as a function of linear dimensions used in SVD bi-cross validation. Horizontal dashed lines represent the accuracy of 2D-VAE (dark blue) and 2D-RHVAE (dark red) models. For all antibiotics, the 2D nonlinear latent space coordinates provide more accurate predictions than any number of linear dimensions, demonstrating that nonlinear latent space representations capture more meaningful biological information about the underlying phenotypic state."
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code Overview",
    "section": "",
    "text": "This section provides interactive code examples and demonstrations of the computational methods used in our research on evolutionary landscapes and phenotype-to-fitness maps.\n\n\nWe provide several computational notebooks that showcase different aspects of our methodology and analysis:\n\nEvolutionary Dynamics Simulation\n\n\nSimulates the dynamics of populations evolving in a 2-dimensional phenotypic space under various fitness landscapes and fixed mutational constraints. This notebook demonstrates how populations navigate phenotypic space influenced by both fitness gradients and genotype-phenotype density.\n\n\nRHVAE on Simulated Data\n\n\nImplements a Riemannian Hamiltonian Variational Autoencoder (RHVAE) to analyze fitness profiles generated from evolutionary simulations. This notebook shows how we can learn low-dimensional representations of high-dimensional fitness data while preserving the geometric structure of the underlying phenotypic space.\n\n\nBayesian Inference of IC₅₀ Values\n\n\nPerforms Bayesian inference on the IC₅₀ values of antibiotic resistance landscapes using experimental data. This notebook demonstrates model fitting of dose-response curves using different parameterizations and implements outlier detection techniques.\n\n\nMetropolis-Kimura Evolution\n\n\nExplores a biologically motivated model of evolution combining elements of the Metropolis-Hastings algorithm with Motoo Kimura’s population genetics theory. This model accounts separately for mutation accessibility and selection via fixation probability.\n\n\nMetropolis-Hastings Evolution\n\n\nImplements the Metropolis-Hastings algorithm for simulating evolutionary dynamics on fitness landscapes with mutational constraints.\n\n\n\n\nAll notebooks can be run interactively if you have Julia installed with the required packages. Each notebook is self-contained with detailed explanations, mathematical background, and visualization of results.\n\n\nTo run these notebooks, you’ll need:\n\nData Processing: DataFrames, CSV, DimensionalData\nScientific Computing: LinearAlgebra, StatsBase, Distributions, LsqFit\nBayesian Inference: Turing\nDeep Learning: Flux, AutoEncoderToolkit\nVisualization: CairoMakie, ColorSchemes\n\nClick on any of the notebooks in the sidebar to explore the code in detail.",
    "crumbs": [
      "Code Overview"
    ]
  },
  {
    "objectID": "code.html#available-notebooks",
    "href": "code.html#available-notebooks",
    "title": "Code Overview",
    "section": "",
    "text": "We provide several computational notebooks that showcase different aspects of our methodology and analysis:\n\nEvolutionary Dynamics Simulation\n\n\nSimulates the dynamics of populations evolving in a 2-dimensional phenotypic space under various fitness landscapes and fixed mutational constraints. This notebook demonstrates how populations navigate phenotypic space influenced by both fitness gradients and genotype-phenotype density.\n\n\nRHVAE on Simulated Data\n\n\nImplements a Riemannian Hamiltonian Variational Autoencoder (RHVAE) to analyze fitness profiles generated from evolutionary simulations. This notebook shows how we can learn low-dimensional representations of high-dimensional fitness data while preserving the geometric structure of the underlying phenotypic space.\n\n\nBayesian Inference of IC₅₀ Values\n\n\nPerforms Bayesian inference on the IC₅₀ values of antibiotic resistance landscapes using experimental data. This notebook demonstrates model fitting of dose-response curves using different parameterizations and implements outlier detection techniques.\n\n\nMetropolis-Kimura Evolution\n\n\nExplores a biologically motivated model of evolution combining elements of the Metropolis-Hastings algorithm with Motoo Kimura’s population genetics theory. This model accounts separately for mutation accessibility and selection via fixation probability.\n\n\nMetropolis-Hastings Evolution\n\n\nImplements the Metropolis-Hastings algorithm for simulating evolutionary dynamics on fitness landscapes with mutational constraints.",
    "crumbs": [
      "Code Overview"
    ]
  },
  {
    "objectID": "code.html#running-the-code",
    "href": "code.html#running-the-code",
    "title": "Code Overview",
    "section": "",
    "text": "All notebooks can be run interactively if you have Julia installed with the required packages. Each notebook is self-contained with detailed explanations, mathematical background, and visualization of results.\n\n\nTo run these notebooks, you’ll need:\n\nData Processing: DataFrames, CSV, DimensionalData\nScientific Computing: LinearAlgebra, StatsBase, Distributions, LsqFit\nBayesian Inference: Turing\nDeep Learning: Flux, AutoEncoderToolkit\nVisualization: CairoMakie, ColorSchemes\n\nClick on any of the notebooks in the sidebar to explore the code in detail.",
    "crumbs": [
      "Code Overview"
    ]
  },
  {
    "objectID": "code/evolutionary_dynamics.html",
    "href": "code/evolutionary_dynamics.html",
    "title": "Evolutionary Dynamics Simulation",
    "section": "",
    "text": "(c) This work is licensed under a Creative Commons Attribution License CC-BY 4.0. All code contained herein is licensed under an MIT license.",
    "crumbs": [
      "Notebooks",
      "Evolutionary Dynamics Simulation"
    ]
  },
  {
    "objectID": "code/evolutionary_dynamics.html#setup-environment",
    "href": "code/evolutionary_dynamics.html#setup-environment",
    "title": "Evolutionary Dynamics Simulation",
    "section": "Setup Environment",
    "text": "Setup Environment\nFirst, let’s import the necessary packages for our simulation.\n\n# Import custom project package\nimport Antibiotic\nimport Antibiotic.mh as mh # Metropolis-Hastings dynamics module\n\n# Import packages for storing results\nimport DimensionalData as DD\n\n# Import JLD2 for saving results\nimport JLD2\n\n# Import IterTools for iterating over Cartesian products\nimport IterTools\n\n# Import basic math libraries\nimport StatsBase\nimport LinearAlgebra\nimport Random\nimport Distributions\nimport Distances\n\n# Packages for visualization\nusing CairoMakie\nimport ColorSchemes\n\n# Activate Makie backend\nCairoMakie.activate!()\n\n# Set custom plotting style\nAntibiotic.viz.theme_makie!()\n\n# Set random seed for reproducibility\nRandom.seed!(42)",
    "crumbs": [
      "Notebooks",
      "Evolutionary Dynamics Simulation"
    ]
  },
  {
    "objectID": "code/evolutionary_dynamics.html#simulation-parameters",
    "href": "code/evolutionary_dynamics.html#simulation-parameters",
    "title": "Evolutionary Dynamics Simulation",
    "section": "Simulation Parameters",
    "text": "Simulation Parameters\nLet’s define the parameters that govern our evolutionary simulation. The things we need to define are:\n\nThe dimensionality of the phenotype space n_dim\nThe number of phenotypic initial conditions, i.e., number of lineages (positions in phenotype space) n_sim\nThe number of replicates (evolving strains per lineage) n_rep\nThe inverse temperature (controls selection strength) β\nThe mutation step size µ\nThe number of evolution steps n_steps\nThe number of fitness landscapes n_fit_lans\nThe range of peak means peak_mean_min and peak_mean_max\nThe range of fitness amplitudes fit_amp_min and fit_amp_max\nThe range of fitness covariances fit_cov_min and fit_cov_max\nThe number of fitness peaks n_fit_peaks_min and n_fit_peaks_max\n\n\n# Phenotype space dimensionality\nn_dim = 2\n# Number of initial conditions (positions in phenotype space)\nn_sim = 10\n# Number of replicates (evolving strains per initial condition)\nn_rep = 2\n# Inverse temperature (controls selection strength)\nβ = 10.0\n# Mutation step size\nµ = 0.1\n# Number of evolution steps\nn_steps = 300\n\n# Define number of fitness landscapes\nn_fit_lans = 50\n\n# Define range of peak means\npeak_mean_min = -4.0\npeak_mean_max = 4.0\n\n# Define range of fitness amplitudes\nfit_amp_min = 1.0\nfit_amp_max = 5.0\n\n# Define covariance range\nfit_cov_min = 3.0\nfit_cov_max = 10.0\n\n# Define possible number of fitness peaks\nn_fit_peaks_min = 1\nn_fit_peaks_max = 4\n\nHaving defined these parameters, we must define the two functions that govern the dynamics of the system:\n\nThe genotype-phenotype density function \\(GP(\\underline{p})\\)\nThe fitness functions \\(F_E(\\underline{p})\\) for each environment \\(E\\).\n\nLet’s start by defining the genotype-phenotype density function.",
    "crumbs": [
      "Notebooks",
      "Evolutionary Dynamics Simulation"
    ]
  },
  {
    "objectID": "code/evolutionary_dynamics.html#genotype-phenotype-density-function",
    "href": "code/evolutionary_dynamics.html#genotype-phenotype-density-function",
    "title": "Evolutionary Dynamics Simulation",
    "section": "Genotype-Phenotype Density Function",
    "text": "Genotype-Phenotype Density Function\nThe genotype-phenotype density function \\(GP(\\underline{p})\\) represents constraints on phenotypic evolution due to the underlying genotype-phenotype mapping. It defines which regions of phenotypic space are more or less accessible to mutations.\nIn our simulation, we model the genotype-phenotype density function as a collection of inverted Gaussian peaks. These peaks define areas of phenotypic space with lower densities of possible mutations. In other words, we define four “wells” in which the genotype-phenotype density drops, making it more unlikely for mutations to take a population towards these regions.\nFor illustrative purposes, as done in the main text, we will define the genotype-phenotype density function as a collection of four Gaussian peaks, which we will place at the corners of a square in phenotype space.\n\n# GP density peak amplitude\ngp_evo_amplitude = 1.0\n\n# GP density peak means - defines 4 peaks in a square arrangement\ngp_means = [\n    [-1.5, -1.5],\n    [1.5, -1.5],\n    [1.5, 1.5],\n    [-1.5, 1.5],\n]\n\n# GP density peak covariance (controls spread of each peak)\ngp_evo_covariance = 0.45\n\n# Create GP density peaks object using the `GaussianPeaks` constructor\ngp_evo_peaks = mh.GaussianPeaks(\n    gp_evo_amplitude,\n    gp_means,\n    gp_evo_covariance\n)\n\n# Define grid on which to evaluate GP density landscape\ngp_evo_grid = range(peak_mean_min, peak_mean_max, length=100)\n\n# Evaluate GP density landscape on grid\ngp_evo_grid_points = mh.genetic_density(\n    tuple(repeat([gp_evo_grid], n_dim)...),\n    gp_evo_peaks\n)\n\n# Define grid of possible initial conditions\ninit_grid = [[x...] for x in IterTools.product(fill(gp_evo_grid, 2)...)]\n\nLet’s visualize this mutational landscape:\n\n# Define ranges of phenotypes to evaluate\nx = range(-4, 4, length=100)\ny = range(-4, 4, length=100)\n\n# Initialize figure\nfig = Figure(size=(350, 300))\n\n# Add axis\nax = Axis(\n    fig[1, 1],\n    xlabel=\"phenotype 1\",\n    ylabel=\"phenotype 2\",\n    aspect=AxisAspect(1),\n    title=\"Genotype-to-Phenotype\\nDensity\",\n    titlesize=14,\n    yticklabelsvisible=false,\n    xticklabelsvisible=false,\n)\n\n# Evaluate mutational landscape\nM = mh.genetic_density(x, y, gp_evo_peaks)\n\n# Plot mutational landscape\nhm_gp = heatmap!(ax, x, y, M, colormap=Reverse(ColorSchemes.Purples_9))\n\n# Add contour plot\ncontour!(ax, x, y, M, color=:white)\n\n# Add colorbar for mutational landscape\nColorbar(\n    fig[1, 2],\n    hm_gp,\n    label=\"genotype-phenotype\\ndensity\",\n    labelsize=13,\n    height=Relative(1),\n    ticksvisible=false,\n    ticklabelsvisible=false\n)\n\nfig\n\n\n\n\nThe mutational landscape shows four peaks (in darker purple) representing regions of phenotypic space that have lower accessibility through mutations. The lighter regions between these peaks represent “valleys” which are much easier for populations to traverse.\nLet’s continue by defining the fitness landscapes that we’ll use to simulate evolutionary dynamics.",
    "crumbs": [
      "Notebooks",
      "Evolutionary Dynamics Simulation"
    ]
  },
  {
    "objectID": "code/evolutionary_dynamics.html#fixed-evolution-condition",
    "href": "code/evolutionary_dynamics.html#fixed-evolution-condition",
    "title": "Evolutionary Dynamics Simulation",
    "section": "Fixed Evolution Condition",
    "text": "Fixed Evolution Condition\nWe first define a reference fitness landscape (evolution condition) that we’ll use as a baseline for comparison. This is the simplest possible fitness landscape with a single peak in the center of the phenotype space.\n\n# Evolution condition amplitude\nfit_evo_amplitude = 5.0\n# Evolution condition mean\nfit_evo_mean = [0.0, 0.0]\n# Evolution condition covariance\nfit_evo_covariance = 3.0\n# Create peak\nfit_evo_peak = mh.GaussianPeak(\n    fit_evo_amplitude,\n    fit_evo_mean,\n    fit_evo_covariance\n)\n\nLet’s visualize this reference fitness landscape:\n\n# Initialize figure\nfig = Figure(size=(350, 300))\n\n# Add axis\nax = Axis(\n    fig[1, 1],\n    xlabel=\"phenotype 1\",\n    ylabel=\"phenotype 2\",\n    aspect=AxisAspect(1),\n    title=\"reference fitness landscape\",\n    titlesize=14,\n    yticklabelsvisible=false,\n    xticklabelsvisible=false,\n)\n\n# Evaluate fitness landscape\nF = mh.fitness(x, y, fit_evo_peak)\n\n# Plot fitness landscape\nhm_fit = heatmap!(ax, x, y, F, colormap=:algae)\n\n# Add contour plot\ncontour!(ax, x, y, F, color=:white)\n\n# Add colorbar for fitness landscape\nColorbar(\n    fig[1, 2],\n    hm_fit,\n    label=\"fitness\",\n    labelsize=13,\n    height=Relative(1),\n    ticksvisible=false,\n    ticklabelsvisible=false\n)\n\nfig\n\n\n\n\nThis fitness landscape shows a single peak centered at [0, 0]. The color represents fitness, with darker regions having higher fitness. The contour lines show equal fitness values.",
    "crumbs": [
      "Notebooks",
      "Evolutionary Dynamics Simulation"
    ]
  },
  {
    "objectID": "code/evolutionary_dynamics.html#alternative-fitness-landscapes",
    "href": "code/evolutionary_dynamics.html#alternative-fitness-landscapes",
    "title": "Evolutionary Dynamics Simulation",
    "section": "Alternative Fitness Landscapes",
    "text": "Alternative Fitness Landscapes\nTo explore how evolution proceeds in different environments, we generate a variety of fitness landscapes with randomly selected numbers, positions, and shapes of fitness peaks. The sampled numbers of fitness peaks, positions, and amplitudes will be drawn from uniform distributions within the ranges defined above.\n\n# Reset random seed for reproducibility\nRandom.seed!(42)\n\n# Initialize array to hold fitness landscapes\nfit_lans = Array{mh.AbstractPeak}(undef, n_fit_lans)\n\n# Add fixed evolution condition to fitness landscapes\nfit_lans[1] = fit_evo_peak\n\n# Loop over number of fitness landscapes\nfor i in 2:n_fit_lans\n    # Sample number of fitness peaks\n    n_fit_peaks = rand(n_fit_peaks_min:n_fit_peaks_max)\n\n    # Sample fitness means as 2D vectors from uniform distribution\n    fit_means = [\n        rand(Distributions.Uniform(peak_mean_min, peak_mean_max), 2)\n        for _ in 1:n_fit_peaks\n    ]\n\n    # Sample fitness amplitudes from uniform distribution\n    fit_amplitudes = rand(\n        Distributions.Uniform(fit_amp_min, fit_amp_max), n_fit_peaks\n    )\n\n    # Sample fitness covariances from uniform distribution\n    fit_covariances = rand(\n        Distributions.Uniform(fit_cov_min, fit_cov_max), n_fit_peaks\n    )\n\n    # Check dimensionality\n    if n_fit_peaks == 1\n        # Create fitness peaks\n        fit_lans[i] = mh.GaussianPeak(\n            first(fit_amplitudes), first(fit_means), first(fit_covariances)\n        )\n    else\n        # Create fitness peaks\n        fit_lans[i] = mh.GaussianPeaks(\n            fit_amplitudes, fit_means, fit_covariances\n        )\n    end\nend\n\nLet’s visualize a sample of these fitness landscapes:\n\nRandom.seed!(42)\n\n# Define number of rows and columns\nn_rows = 4\nn_cols = 4\n\n# Define number of landscapes to display\nn_sample = n_rows * n_cols\n\n# Initialize figure\nfig = Figure(size=(600, 600))\n\n# Add global grid layout\ngl = fig[1, 1] = GridLayout()\n\n# Loop over a sample of fitness landscapes\nfor (i, idx) in enumerate(1:n_sample)\n    # Define row and column indices\n    row = (i - 1) ÷ n_cols + 1\n    col = (i - 1) % n_cols + 1\n\n    # Create subplot\n    ax = Axis(\n        gl[row, col],\n        aspect=AxisAspect(1),\n        title=\"landscape $(idx)\",\n        titlesize=12,\n        yticklabelsvisible=false,\n        xticklabelsvisible=false,\n    )\n\n    # Evaluate fitness landscape\n    F = mh.fitness(x, y, fit_lans[idx])\n\n    # Plot fitness landscape\n    hm_fit = heatmap!(ax, x, y, F, colormap=:algae)\n\n    # Add contour plot\n    contour!(ax, x, y, F, color=:white)\nend\n\n# Set global axis labels\nLabel(\n    fig[:, end, Bottom()],\n    \"phenotype 1\",\n    fontsize=14,\n    padding=(0, 0, 0, 10),\n)\n\nLabel(\n    fig[1, :, Left()],\n    \"phenotype 2\",\n    fontsize=14,\n    rotation=π / 2,\n)\n\nfig\n\n\n\n\nThese fitness landscapes represent different environments in which our populations will evolve. Each landscape has a unique configuration of fitness peaks, with varying heights, positions, and shapes.\nWe are now ready to simulate the evolutionary dynamics of our populations.",
    "crumbs": [
      "Notebooks",
      "Evolutionary Dynamics Simulation"
    ]
  },
  {
    "objectID": "code/evolutionary_dynamics.html#simulating-evolution-with-metropolis-hastings",
    "href": "code/evolutionary_dynamics.html#simulating-evolution-with-metropolis-hastings",
    "title": "Evolutionary Dynamics Simulation",
    "section": "Simulating Evolution with Metropolis-Hastings",
    "text": "Simulating Evolution with Metropolis-Hastings\nNow we’ll simulate the evolutionary dynamics using the Metropolis-Hastings algorithm. In this algorithm:\n\nWe start with an initial phenotype\nWe propose a random mutation (a step in phenotype space)\nWe accept or reject the mutation based on both fitness and genotype-phenotype density\nWe repeat for many generations\n\nThe Metropolis-Hastings acceptance probability is given by:\n\\[\nP_{\\text{accept}} = \\min\\left(\n    1,\n    \\frac{F_E(\\underline{x}')^β \\cdot GP(\\underline{x}')^β}\n    {F_E(\\underline{x})^β \\cdot GP(\\underline{x})^β}\n\\right)\n\\tag{1}\\]\nwhere \\(F_E(\\underline{x})\\) is the fitness in environment \\(E\\), \\(GP(\\underline{x})\\) is the genotype-phenotype density, and \\(\\beta\\) is the inverse temperature parameter controlling selection strength.\nOur first step is to sample initial conditions for our replicates. For this, we sample initial positions on phenotype space from a uniform distribution, taking into account the mutational landscape (not to start at a low mutational peak).\n\n# Reset random seed for reproducibility\nRandom.seed!(42)\n\n# Sample initial positions on phenotype space from uniform distribution taking\n# into account mutational landscape (not to start at a low mutational peak)\nx0 = StatsBase.sample(\n    vec(init_grid),\n    StatsBase.Weights(vec(gp_evo_grid_points)),\n    n_sim\n)\n\n# Select initial conditions for replicates from multivariate normal distribution\n# around each initial condition\nx0_reps = reduce(\n    (x, y) -&gt; cat(x, y, dims=3),\n    [\n        rand(Distributions.MvNormal(x0[i], 0.1), n_rep)\n        for i in 1:n_sim\n    ]\n)\n# Change order of dimensions\nx0_reps = permutedims(x0_reps, (1, 3, 2))\n\nNext, we initialize a very convenient data structure from the DimensionalData.jl package to hold our trajectories and fitness values for each replicate, landscape, and evolution condition.\n\n# Define dimensions to be used with DimensionalData\nphenotype = DD.Dim{:phenotype}([:x1, :x2]) # phenotype\nfitness = DD.Dim{:fitness}([:fitness]) # fitness\ntime = DD.Dim{:time}(0:n_steps) # time\nlineage = DD.Dim{:lineage}(1:n_sim) # lineage\nreplicate = DD.Dim{:replicate}(1:n_rep) # replicate\nlandscape = DD.Dim{:landscape}(1:n_fit_lans) # landscape\nevo = DD.Dim{:evo}(1:n_fit_lans) # evolution condition\n\n# Initialize DimensionalData array to hold trajectories and fitness\nphenotype_traj = DD.zeros(\n    Float32,\n    phenotype,\n    time,\n    lineage,\n    replicate,\n    landscape,\n    evo,\n)\nfitness_traj = DD.zeros(\n    Float32,\n    fitness,\n    time,\n    lineage,\n    replicate,\n    landscape,\n    evo,\n)\n\n# Stack arrays to store trajectories in phenotype and fitness dimensions\nx_traj = DD.DimStack(\n    (phenotype=phenotype_traj, fitness=fitness_traj),\n)\n\n# Store initial conditions\nx_traj.phenotype[time=1] = repeat(\n    x0_reps,\n    outer=(1, 1, 1, n_fit_lans, n_fit_lans)\n)\n\n# Map initial phenotype to fitness\nx_traj.fitness[time=1] = repeat(\n    reduce(\n        (x, y) -&gt; cat(x, y, dims=3),\n        mh.fitness.(Ref(x0_reps), fit_lans)\n    ),\n    outer=(1, 1, 1, 1, n_fit_lans)\n)\n\nNow we can loop over landscapes, lineages, and replicates to simulate the evolutionary dynamics of our populations. The core function to run the simulations is already implemented in the mh module of our custom package.\n\nRandom.seed!(42)\n\n# Loop over landscapes\nfor evo in DD.dims(x_traj, :evo)\n    # Loop over lineages\n    for lin in DD.dims(x_traj, :lineage)\n        # Loop over replicates\n        for rep in DD.dims(x_traj, :replicate)\n            # Run Metropolis-Hastings algorithm\n            trajectory = mh.evo_metropolis_hastings(\n                x_traj.phenotype[\n                    time=1,\n                    lineage=lin,\n                    replicate=rep,\n                    landscape=evo,\n                    evo=evo,\n                ].data,\n                fit_lans[evo],\n                gp_evo_peaks,\n                β,\n                µ,\n                n_steps\n            )\n\n            # Store trajectory\n            x_traj.phenotype[\n                lineage=lin,\n                replicate=rep,\n                evo=evo,\n            ] .= trajectory\n\n            # Calculate and store fitness for each point in the trajectory\n            x_traj.fitness[\n                lineage=lin,\n                replicate=rep,\n                evo=evo,\n            ] = mh.fitness(trajectory, fit_lans)\n        end\n    end\nend",
    "crumbs": [
      "Notebooks",
      "Evolutionary Dynamics Simulation"
    ]
  },
  {
    "objectID": "code/evolutionary_dynamics.html#visualize-evolutionary-trajectories",
    "href": "code/evolutionary_dynamics.html#visualize-evolutionary-trajectories",
    "title": "Evolutionary Dynamics Simulation",
    "section": "Visualize Evolutionary Trajectories",
    "text": "Visualize Evolutionary Trajectories\nHaving run the simulations, we can now visualize some of the evolutionary trajectories to understand how populations navigate the phenotypic space.\n\nRandom.seed!(42)\n\n# Define number of rows and columns\nn_rows = 4\nn_cols = 4\n\n# Define number of landscapes to display\nn_sample = n_rows * n_cols\n\n# Initialize figure\nfig = Figure(size=(600, 600))\n\n# Add global grid layout\ngl = fig[1, 1] = GridLayout()\n\n# Loop over a sample of fitness landscapes\nfor (i, idx) in enumerate(1:n_sample)\n    # Define row and column indices\n    row = (i - 1) ÷ n_cols + 1\n    col = (i - 1) % n_cols + 1\n\n    # Create subplot\n    ax = Axis(\n        gl[row, col],\n        aspect=AxisAspect(1),\n        title=\"landscape $(idx)\",\n        titlesize=12,\n        yticklabelsvisible=false,\n        xticklabelsvisible=false,\n    )\n\n    # Evaluate fitness landscape\n    F = mh.fitness(x, y, fit_lans[idx])\n\n    # Plot fitness landscape\n    hm_fit = heatmap!(ax, x, y, F, colormap=:algae)\n\n    # Add contour plot\n    contour!(ax, x, y, F, color=:black)\n\n    # Evaluate genetic density\n    M = mh.genetic_density(x, y, gp_evo_peaks)\n\n    # Plot genetic density as contour lines\n    contour!(ax, x, y, M, color=:white)\n\n    # Set limits\n    xlims!(ax, -4, 4)\n    ylims!(ax, -4, 4)\n\n    # Initialize counter for color index\n    color_idx = 0\n\n    # Plot evolutionary trajectories\n    for lin in DD.dims(x_traj, :lineage)\n        for rep in DD.dims(x_traj, :replicate)\n            # Extract x and y coordinates\n            x_data = x_traj.phenotype[\n                # time=DD.At(1:n_steps+1),\n                phenotype=DD.At(:x1),\n                lineage=lin,\n                replicate=rep,\n                landscape=idx,\n                evo=idx,\n            ].data\n            y_data = x_traj.phenotype[\n                # time=DD.At(1:n_steps+1),\n                phenotype=DD.At(:x2),\n                lineage=lin,\n                replicate=rep,\n                landscape=idx,\n                evo=idx,\n            ].data\n\n            # Plot trajectory\n            color_idx += 1\n            lines!(\n                ax,\n                x_data,\n                y_data,\n                color=ColorSchemes.glasbey_hv_n256[color_idx],\n                linewidth=1\n            )\n\n            # Plot initial condition\n            scatter!(\n                ax,\n                x_data[1],\n                y_data[1],\n                color=ColorSchemes.glasbey_hv_n256[color_idx],\n                markersize=12,\n                marker=:xcross,\n                strokecolor=:black,\n                strokewidth=1.5,\n            )\n\n            # Plot final condition\n            scatter!(\n                ax,\n                x_data[end],\n                y_data[end],\n                color=ColorSchemes.glasbey_hv_n256[color_idx],\n                markersize=12,\n                marker=:utriangle,\n                strokecolor=:black,\n                strokewidth=1.5,\n            )\n        end\n    end\nend\n\n# Set global axis labels\nLabel(\n    fig[:, end, Bottom()],\n    \"phenotype 1\",\n    fontsize=14,\n    padding=(0, 0, 0, 10),\n)\n\nLabel(\n    fig[1, :, Left()],\n    \"phenotype 2\",\n    fontsize=14,\n    rotation=π / 2,\n)\n\nfig\n\n\n\n\nIn these visualizations, we can observe several key features of evolutionary dynamics:\n\nFitness landscape influence: Populations tend to climb fitness peaks (brighter regions).\nMutational constraints: Trajectories are influenced by the genotype-phenotype density map (white contour lines), avoiding regions of low density.\nInitial condition effects: Different starting points can lead to different evolutionary outcomes.\nStochasticity: Even with the same starting point (replicate lines of the same color), stochasticity leads to different evolutionary paths.",
    "crumbs": [
      "Notebooks",
      "Evolutionary Dynamics Simulation"
    ]
  },
  {
    "objectID": "code/evolutionary_dynamics.html#fitness-trajectory-analysis",
    "href": "code/evolutionary_dynamics.html#fitness-trajectory-analysis",
    "title": "Evolutionary Dynamics Simulation",
    "section": "Fitness Trajectory Analysis",
    "text": "Fitness Trajectory Analysis\nLet’s analyze how fitness changes over time during evolution:\n\nRandom.seed!(42)\n\n# Define number of rows and columns\nn_rows = 4\nn_cols = 4\n\n# Define number of landscapes to display\nn_sample = n_rows * n_cols\n\n# Initialize figure\nfig = Figure(size=(600, 600))\n\n# Add global grid layout\ngl = fig[1, 1] = GridLayout()\n\n# Loop over a sample of fitness landscapes\nfor (i, idx) in enumerate(1:n_sample)\n    # Define row and column indices\n    row = (i - 1) ÷ n_cols + 1\n    col = (i - 1) % n_cols + 1\n\n    # Create subplot\n    ax = Axis(\n        gl[row, col],\n        xlabel=\"time (generations)\",\n        ylabel=\"fitness\",\n        title=\"landscape $(idx)\",\n        titlesize=12,\n        yticklabelsvisible=false,\n        xticklabelsvisible=false,\n        xlabelsize=10,\n        ylabelsize=10,\n    )\n\n    # Plot fitness trajectories\n    for lin in DD.dims(x_traj, :lineage)\n        for rep in DD.dims(x_traj, :replicate)\n            # Extract fitness data\n            fit_data = x_traj.fitness[\n                fitness=DD.At(:fitness),\n                lineage=lin,\n                replicate=rep,\n                landscape=idx,\n                evo=idx,\n            ].data\n\n            # Plot trajectory\n            color_idx = (lin - 1) * n_rep + rep\n            lines!(\n                ax,\n                0:n_steps,\n                vec(fit_data),\n                color=ColorSchemes.glasbey_hv_n256[color_idx],\n                linewidth=1\n            )\n        end\n    end\nend\n\nfig",
    "crumbs": [
      "Notebooks",
      "Evolutionary Dynamics Simulation"
    ]
  },
  {
    "objectID": "code/evolutionary_dynamics.html#conclusion",
    "href": "code/evolutionary_dynamics.html#conclusion",
    "title": "Evolutionary Dynamics Simulation",
    "section": "Conclusion",
    "text": "Conclusion\nIn this notebook, we’ve simulated evolutionary dynamics in a 2D phenotypic space under various fitness landscapes and a fixed mutational landscape. We’ve used the Metropolis-Hastings algorithm to model evolution as a biased random walk influenced by both fitness and mutational constraints.\nKey insights from our simulations:\n\nFitness-mutation interplay: Evolution is shaped by both the fitness landscape (selection) and the mutational landscape (genetic constraints).\nMultiple evolutionary outcomes: Different initial conditions can lead to different evolutionary endpoints, even in the same fitness landscape.\nStochastic effects: Even with identical starting points, stochasticity in the evolutionary process leads to variation in outcomes.\nLandscape navigation: Populations navigate the phenotypic space by climbing fitness gradients while being constrained by the mutational landscape.\n\nThis approach allows us to explore how populations might adapt to various environments given underlying genetic constraints, providing insights into the predictability and paths of evolution.",
    "crumbs": [
      "Notebooks",
      "Evolutionary Dynamics Simulation"
    ]
  },
  {
    "objectID": "code/iwasawa_bayesian_inference.html",
    "href": "code/iwasawa_bayesian_inference.html",
    "title": "Bayesian Inference of \\(IC_{50}\\) Values",
    "section": "",
    "text": "(c) This work is licensed under a Creative Commons Attribution License CC-BY 4.0. All code contained herein is licensed under an MIT license.\n# Import project package\nimport Antibiotic\n\n# Import package to handle DataFrames\nimport DataFrames as DF\nimport CSV\n\n# Import library for Bayesian inference\nimport Turing\n\n# Import library to list files\nimport Glob\n\n# Import packages to work with data\nimport DataFrames as DF\n\n# Load CairoMakie for plotting\nusing CairoMakie\nimport ColorSchemes\n\n# Import packages for posterior sampling\nimport PairPlots\n\n# Import basic math libraries\nimport LsqFit\nimport StatsBase\nimport LinearAlgebra\nimport Random\n\n# Activate backend\nCairoMakie.activate!()\n\n# Set custom plotting style\nAntibiotic.viz.theme_makie!()\n(c) This work is licensed under a Creative Commons Attribution License CC-BY 4.0. All code contained herein is licensed under an MIT license.",
    "crumbs": [
      "Notebooks",
      "Bayesian Inference of $IC_{50}$ Values"
    ]
  },
  {
    "objectID": "code/iwasawa_bayesian_inference.html#bayesian-model",
    "href": "code/iwasawa_bayesian_inference.html#bayesian-model",
    "title": "Bayesian Inference of \\(IC_{50}\\) Values",
    "section": "Bayesian model",
    "text": "Bayesian model\nGiven the model presented in Eq. (1), and the data, our objective is to infer the value of all parameters. By Bayes theorem, we write\n\\[\n\\pi(IC_{50}, a, b, c \\mid \\text{data}) =\n\\frac{\\pi(\\text{data} \\mid IC_{50}, a, b, c)\n\\pi(IC_{50}, a, b, c)}\n{\\pi(\\text{data})},\n\\tag{2}\\]\nwhere \\(\\text{data}\\) consists of the pairs of antibiotic concentration and optical density.\n\nLikelihood \\(\\pi(\\text{data} \\mid IC_{50}, a, b, c)\\)\nLet’s begin by defining the likelihood function. For simplicity, we assume each datum is independent and identically distributed (i.i.d.) and write\n\\[\n\\pi(\\text{data} \\mid IC_{50}, a, b, c) =\n\\prod_{i=1}^n \\pi(d_i \\mid IC_{50}, a, b, c),\n\\tag{3}\\]\nwhere \\(d_i = (x_i, y_i)\\) is the \\(i\\)-th pair of antibiotic concentration and optical density, respectively, and \\(n\\) is the total number of data points. As a first pass, we assume that our experimental measurements can be expressed as\n\\[\ny_i = f(x_i, IC_{50}, a, b, c) + \\epsilon_i,\n\\tag{4}\\]\nwhere \\(\\epsilon_i\\) is the experimental error. Furthermore, we assume that the experimental error is normally distributed, i.e.,\n\\[\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2),\n\\tag{5}\\]\nwhere \\(\\sigma^2\\) is an unknown variance parameter that must be included in our inference. Notice that we assume the same variance parameter for all data points since \\(\\sigma^2\\) is not indexed by \\(i\\).\nGiven this likelihood function, we must update our inference on the parameters as\n\\[\n\\pi(IC_{50}, a, b, c, \\sigma^2 \\mid \\text{data}) =\n\\frac{\\pi(\\text{data} \\mid IC_{50}, a, b, c, \\sigma^2)\n\\pi(IC_{50}, a, b, c, \\sigma^2)}\n{\\pi(\\text{data})},\n\\tag{6}\\]\nto include the new parameter \\(\\sigma^2\\).\nOur likelihood function is then of the form \\[\ny_i \\mid IC_{50}, a, b, c, \\sigma^2 \\sim\n\\mathcal{N}(f(x_i, IC_{50}, a, b, c), \\sigma^2).\n\\tag{7}\\]\n\n\nPrior \\(\\pi(IC_{50}, a, b, c, \\sigma^2)\\)\nFor the prior, we assume that all parameters are independent and write \\[\n\\pi(IC_{50}, a, b, c, \\sigma^2) =\n\\pi(IC_{50}) \\pi(a) \\pi(b) \\pi(c) \\pi(\\sigma^2).\n\\tag{8}\\]\nLet’s detail each prior.\n\n\\(IC_{50}\\): The \\(IC_{50}\\) is a strictly positive parameter. However, we will fit for \\(\\log_2(IC_{50})\\). Thus, we will use a normal prior for \\(\\log_2(IC_{50})\\). This means we have\n\n\\[\n\\log_2(IC_{50}) \\sim\n\\mathcal{N}(\n    \\mu_{\\log_2(IC_{50})}, \\sigma_{\\log_2(IC_{50})}^2\n).\n\\tag{9}\\]\n\n\\(a\\): This nuisance parameter scales the logistic function. Again, the natural scale for this parameter is a strictly positive real number. Thus, we will use a lognormal prior for \\(a\\). This means we have\n\n\\[\na \\sim \\text{LogNormal}(\\mu_a, \\sigma_a^2).\n\\tag{10}\\]\n\n\\(b\\): This parameter controls the steepness of the logistic function. Again, the natural scale for this parameter is a strictly positive real number. Thus, we will use a lognormal prior for \\(b\\). This means we have\n\n\\[\nb \\sim \\text{LogNormal}(\\mu_b, \\sigma_b^2).\n\\tag{11}\\]\n\n\\(c\\): This parameter controls the minimum value of the logistic function. Since this is a strictly positive real number that does not necessarily scale with the data, we will use a half-normal prior for \\(c\\). This means we have\n\n\\[\nc \\sim \\text{Half-}\\mathcal{N}(0, \\sigma_c^2).\n\\tag{12}\\]\n\n\\(\\sigma^2\\): This parameter controls the variance of the experimental error. Since this is a strictly positive real number that does not necessarily scale with the data, we will use a half-normal prior for \\(\\sigma^2\\). This means we have\n\n\\[\n\\sigma^2 \\sim \\text{Half-}\\mathcal{N}(0, \\sigma_{\\sigma^2}^2).\n\\tag{13}\\]\nWith all of this in place, we are ready to define a Turing model to perform Bayesian inference on the parameters of the model.\n\nTuring.@model function logistic_model(\n    log2x, y, prior_params::NamedTuple=NamedTuple()\n)\n    # Define default prior parameters\n    default_params = (\n        log2ic50=(0, 1),\n        a=(0, 1),\n        b=(0, 1),\n        c=(0, 1),\n        σ²=(0, 1)\n    )\n\n    # Merge default parameters with provided parameters\n    params = merge(default_params, prior_params)\n\n    # Define priors\n    log2ic50 ~ Turing.Normal(params.log2ic50...)\n    a ~ Turing.LogNormal(params.a...)\n    b ~ Turing.LogNormal(params.b...)\n    c ~ Turing.truncated(Turing.Normal(params.c...), 0, Inf)\n    σ² ~ Turing.truncated(Turing.Normal(params.σ²...), 0, Inf)\n\n    # Define likelihood\n    y ~ Turing.MvNormal(\n        logistic_log2(log2x, a, b, c, log2ic50),\n        LinearAlgebra.Diagonal(fill(σ², length(y)))\n    )\nend\n\nHaving defined the model, we can now perform inference. First, let’s perform inference on simulated data. For this, we first simulate a single titration curve with known parameters.\n\nRandom.seed!(42)\n# Define ground truth parameters\nlog2ic50_true = 0.5\na_true = 1.0\nb_true = 10.0\nσ²_true = 0.01\nc_true = 0.1\n\n# Simulate data\nlog2x = LinRange(-2.5, 2.5, 15)\n# Define mean of data\nŷ = logistic_log2(log2x, a_true, b_true, c_true, log2ic50_true)\n# Add noise\ny = ŷ .+ randn(length(log2x)) * √(σ²_true)\n\n# Initialize figure\nfig = Figure(size=(350, 300))\n# Add axis\nax = Axis(\n    fig[1, 1],\n    xlabel=\"antibiotic concentration\",\n    ylabel=\"optical density\",\n    xscale=log2\n)\n# Plot data\nscatterlines!(ax, 2.0 .^ log2x, y)\n\nfig\n\n\n\n\nWith the data simulated, let’s perform inference on the data. For this, we will use the NUTS sampler. To run multiple chains in parallel, we will use the MCMCThreads option.\n\nRandom.seed!(42)\n# Perform inference\nmodel = logistic_model(log2x, y)\n\n# Define number of steps\nn_burnin = 10_000\nn_samples = 1_000\n\n# Run sampler using\nchain = Turing.sample(\n    model, Turing.NUTS(), Turing.MCMCThreads(), n_burnin + n_samples, 4\n)\n\nSampling (4 threads)   0%|                              |  ETA: N/A\n┌ Info: Found initial step size\n└   ϵ = 0.4\n┌ Info: Found initial step size\n└   ϵ = 0.8\n┌ Info: Found initial step size\n└   ϵ = 0.8\n┌ Info: Found initial step size\n└   ϵ = 0.8\nSampling (4 threads)  25%|███████▌                      |  ETA: 0:00:39\nSampling (4 threads)  50%|███████████████               |  ETA: 0:00:13\nSampling (4 threads)  75%|██████████████████████▌       |  ETA: 0:00:04\nSampling (4 threads) 100%|██████████████████████████████| Time: 0:00:13\nSampling (4 threads) 100%|██████████████████████████████| Time: 0:00:13\n\n\nChains MCMC chain (11000×17×4 Array{Float64, 3}):\n\nIterations        = 1001:1:12000\nNumber of chains  = 4\nSamples per chain = 11000\nWall duration     = 9.23 seconds\nCompute duration  = 35.81 seconds\nparameters        = log2ic50, a, b, c, σ²\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std      mcse     ess_bulk     ess_tail      rhat ⋯\n      Symbol   Float64   Float64   Float64      Float64      Float64   Float64 ⋯\n\n    log2ic50    0.4723    0.0718    0.0005   23407.8672   14077.0993    1.0003 ⋯\n           a    0.9668    0.0621    0.0005   16034.1809   16294.6985    1.0006 ⋯\n           b    8.7902    4.6027    0.0449   13409.6206   13035.6999    1.0003 ⋯\n           c    0.1192    0.0449    0.0004   15396.5130   11022.7335    1.0004 ⋯\n          σ²    0.0111    0.0076    0.0001   12720.8496   16686.4143    1.0005 ⋯\n                                                                1 column omitted\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5%\n      Symbol   Float64   Float64   Float64   Float64   Float64\n\n    log2ic50    0.3173    0.4352    0.4745    0.5141    0.6048\n           a    0.8482    0.9279    0.9649    1.0042    1.0961\n           b    3.0834    5.7843    7.9162   10.6434   20.2429\n           c    0.0294    0.0902    0.1194    0.1475    0.2093\n          σ²    0.0038    0.0065    0.0091    0.0132    0.0302\n\n\nLet’s look at the posterior distribution of the parameters. For this, we will use the PairPlots.jl package.\n\n# Plot corner plot for chains\nPairPlots.pairplot(\n    chain[n_burnin+1:end, :, :],\n    PairPlots.Truth(\n        (;\n        log2ic50=log2ic50_true,\n        a=a_true,\n        b=b_true,\n        c=c_true,\n        σ²=σ²_true\n    )\n    )\n)\n\n\n\n\nAll parameters are well-constrained by the data, and we are able to recover the ground truth values.\nLet’s plot the posterior predictive checks with the data to see how the model performs.\n\nRandom.seed!(42)\n\n# Initialize matrix to store samples\ny_samples = Array{Float64}(undef, length(log2x), n_samples)\n\n# Loop through samples\nfor i in 1:n_samples\n    # Generate mean for sample\n    y_samples[:, i] = logistic_log2(\n        log2x,\n        chain[:a][i],\n        chain[:b][i],\n        chain[:c][i],\n        chain[:log2ic50][i]\n    )\n    # Add noise\n    y_samples[:, i] .+= randn(length(log2x)) * sqrt(chain[:σ²][i])\nend # for\n\n# Initialize figure\nfig = Figure(size=(350, 300))\n# Add axis\nax = Axis(\n    fig[1, 1],\n    xlabel=\"antibiotic concentration\",\n    ylabel=\"optical density\",\n    title=\"Posterior Predictive Check\",\n    xscale=log2\n)\n\n# Plot samples\nfor i in 1:n_samples\n    lines!(ax, 2.0 .^ log2x, y_samples[:, i], color=(:gray, 0.05))\nend # for\n\n# Plot data\nscatterlines!(ax, 2.0 .^ log2x, y)\n\nfig\n\n\n\n\nThe fit looks excellent. Let’s now perform inference on the Iwasawa data. We will use one example dataset. Moreover, we will set informative priors for the parameters.\n\nRandom.seed!(42)\n# Group data by antibiotic, environment, and day\ndf_group = DF.groupby(\n    df[(.!df.blank).&(df.concentration_ugmL.&gt;0), :],\n    [:antibiotic, :env, :day]\n)\n\n# Extract data\ndata = df_group[2]\n\n# Define prior parameters\nprior_params = (\n    a=(log(0.1), 0.1),\n    b=(0, 1),\n    c=(0, 1),\n    σ²=(0, 0.1)\n)\n# Perform inference\nmodel = logistic_model(\n    log2.(data.concentration_ugmL),\n    data.OD,\n    prior_params\n)\n\n# Define number of steps\nn_burnin = 10_000\nn_samples = 1_000\n\nchain = Turing.sample(\n    model, Turing.NUTS(), Turing.MCMCThreads(), n_burnin + n_samples, 4\n)\n\nSampling (4 threads)   0%|                              |  ETA: N/A\n┌ Info: Found initial step size\n└   ϵ = 0.05\n┌ Info: Found initial step size\n└   ϵ = 0.2\n┌ Info: Found initial step size\n└   ϵ = 0.2\n┌ Info: Found initial step size\n└   ϵ = 0.05\nSampling (4 threads)  25%|███████▌                      |  ETA: 0:00:15\nSampling (4 threads)  50%|███████████████               |  ETA: 0:00:05\nSampling (4 threads)  75%|██████████████████████▌       |  ETA: 0:00:02\nSampling (4 threads) 100%|██████████████████████████████| Time: 0:00:05\nSampling (4 threads) 100%|██████████████████████████████| Time: 0:00:05\n\n\nChains MCMC chain (11000×17×4 Array{Float64, 3}):\n\nIterations        = 1001:1:12000\nNumber of chains  = 4\nSamples per chain = 11000\nWall duration     = 4.57 seconds\nCompute duration  = 17.42 seconds\nparameters        = log2ic50, a, b, c, σ²\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std      mcse     ess_bulk     ess_tail      rhat ⋯\n      Symbol   Float64   Float64   Float64      Float64      Float64   Float64 ⋯\n\n    log2ic50    3.1585    0.0611    0.0004   32325.5670   25247.0891    1.0001 ⋯\n           a    0.1013    0.0074    0.0000   28541.0328   27631.5354    1.0001 ⋯\n           b   15.8931   10.4326    0.0655   31581.5518   26387.0900    1.0000 ⋯\n           c    0.0873    0.0073    0.0000   27591.1350   24819.1854    1.0002 ⋯\n          σ²    0.0015    0.0002    0.0000   37231.0264   30213.0584    1.0000 ⋯\n                                                                1 column omitted\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5%\n      Symbol   Float64   Float64   Float64   Float64   Float64\n\n    log2ic50    3.0427    3.1192    3.1589    3.1963    3.2817\n           a    0.0872    0.0962    0.1012    0.1063    0.1162\n           b    4.6324    9.2475   13.4282   19.5750   41.4435\n           c    0.0726    0.0824    0.0874    0.0923    0.1013\n          σ²    0.0011    0.0013    0.0014    0.0016    0.0020\n\n\nLet’s look at the corner plot for the chains.\n\n# Plot corner plot for chains\nPairPlots.pairplot(chain[n_burnin+1:end, :, :])\n\n\n\n\nThis looks good. Let’s now plot the posterior predictive check.\n\nRandom.seed!(42)\n\n# Define unique concentrations\nunique_concentrations = sort(unique(data.concentration_ugmL))\n\n# Initialize matrix to store samples\ny_samples = Array{Float64}(\n    undef, length(unique_concentrations), n_samples\n)\n\n# Loop through samples\nfor i in 1:n_samples\n    # Generate mean for sample\n    y_samples[:, i] = logistic_log2(\n        log2.(unique_concentrations),\n        chain[:a][i],\n        chain[:b][i],\n        chain[:c][i],\n        chain[:log2ic50][i]\n    )\n    # Add noise\n    y_samples[:, i] .+= randn(length(unique_concentrations)) * √(chain[:σ²][i])\n    √(chain[:σ²][i])\nend # for\n\n# Initialize figure\nfig = Figure(size=(350, 300))\n# Add axis\nax = Axis(\n    fig[1, 1],\n    xlabel=\"antibiotic concentration\",\n    ylabel=\"optical density\",\n    title=\"Posterior Predictive Check\",\n    xscale=log2\n)\n\n# Plot samples\nfor i in 1:n_samples\n    lines!(\n        ax,\n        unique_concentrations,\n        y_samples[:, i],\n        color=(ColorSchemes.Paired_12[1], 0.5)\n    )\nend # for\n\n# Plot data\nscatter!(ax, data.concentration_ugmL, data.OD)\n\nfig\n\n\n\n\nThe presence of the outlier measurements expands the posterior predictive checks uncertainty.\nLet’s look into how to detect outliers in the data.",
    "crumbs": [
      "Notebooks",
      "Bayesian Inference of $IC_{50}$ Values"
    ]
  },
  {
    "objectID": "code/iwasawa_bayesian_inference.html#residual-based-outlier-detection",
    "href": "code/iwasawa_bayesian_inference.html#residual-based-outlier-detection",
    "title": "Bayesian Inference of \\(IC_{50}\\) Values",
    "section": "Residual-based outlier detection",
    "text": "Residual-based outlier detection\nOur naive approach to detect outliers is to fit the logistic model deterministically and then identify points that are more than 3 standard deviations from the mean. This is known as “residual-based outlier detection”.\n\nfunction logistic_log2(log2x, params)\n    return logistic_log2(log2x, params...)\nend\n\n\"\"\"\n    fit_logistic_and_detect_outliers(log2x, y; threshold=3)\n\nFit a logistic model to the given data and detect outliers based on residuals.\n\nThis function performs the following steps:\n1. Fits a logistic model to the log2-transformed x-values and y-values.\n2. Calculates residuals between the fitted model and actual y-values.\n3. Identifies outliers as points with residuals exceeding a specified threshold.\n\n# Arguments\n- `log2x`: Array of log2-transformed x-values (typically concentrations).\n- `y`: Array of y-values (typically optical density measurements).\n- `threshold`: Number of standard deviations beyond which a point is considered an outlier. Default is 3.\n\n# Returns\n- A boolean array indicating which points are outliers (true for outliers).\n\n# Notes\n- The function uses a logistic model of the form: \n    y = a / (1 + exp(b * (log2x - log2ic50))) + c\n- Initial parameter guesses are made based on the input data.\n- The LsqFit package is used for curve fitting.\n- Outliers are determined by comparing the absolute residuals to the threshold * standard deviation of residuals.\n\"\"\"\nfunction fit_logistic_and_detect_outliers(log2x, y; threshold=3)\n    # Initial parameter guess\n    p0 = [0.1, 1.0, maximum(y) - minimum(y), StatsBase.median(log2x)]\n\n    # Fit the logistic model\n    fit = LsqFit.curve_fit(logistic_log2, log2x, y, p0)\n\n    # Calculate residuals\n    residuals = y - logistic_log2(log2x, fit.param)\n\n    # Calculate standard deviation of residuals\n    σ = StatsBase.std(residuals)\n\n    # Identify outliers\n    outliers_idx = abs.(residuals) .&gt; threshold * σ\n\n    # Return outlier indices\n    return outliers_idx\nend\n\nLet’s test this function by trying to remove the outliers from the data we used in the previous section.\n\n# Locate outliers\noutliers_idx = fit_logistic_and_detect_outliers(\n    log2.(data.concentration_ugmL), data.OD, threshold=2\n)\n\n# Plot the results\nfig = Figure(size=(450, 300))\n\nax = Axis(\n    fig[1, 1],\n    xlabel=\"antibiotic concentration\",\n    ylabel=\"optical density\",\n    # xscale=log2\n)\n\n# Plot the original data\nscatter!(\n    ax,\n    data.concentration_ugmL,\n    data.OD,\n    color=ColorSchemes.Paired_12[1],\n    label=\"data\"\n)\n\n# Plot the cleaned data\nscatter!(\n    ax,\n    data.concentration_ugmL[.!outliers_idx] .+ 0.5,\n    data.OD[.!outliers_idx], color=ColorSchemes.Paired_12[2],\n    label=\"cleaned data\"\n)\n\n# Add legend\nLegend(fig[1, 2], ax)\n\nfig\n\n\n\n\nThe detection of outliers worked really well. Let’s now perform inference on the cleaned data.\n\nRandom.seed!(42)\n# Group data by antibiotic, environment, and day\ndf_group = DF.groupby(\n    df[(.!df.blank).&(df.concentration_ugmL.&gt;0), :],\n    [:antibiotic, :env, :day]\n)\n\n# Extract data\ndata = df_group[2]\n\n# Find outliers\noutliers_idx = fit_logistic_and_detect_outliers(\n    log2.(data.concentration_ugmL), data.OD, threshold=2\n)\n# Remove outliers\ndata_clean = data[.!outliers_idx, :]\n\n# Define prior parameters\nprior_params = (\n    a=(log(0.1), 0.1),\n    b=(0, 1),\n    c=(0, 1),\n    σ²=(0, 0.1)\n)\n\n# Perform inference\nmodel = logistic_model(\n    log2.(data_clean.concentration_ugmL),\n    data_clean.OD,\n    prior_params\n)\n\n# Define number of steps\nn_burnin = 10_000\nn_samples = 1_000\n\nchain = Turing.sample(\n    model, Turing.NUTS(), Turing.MCMCThreads(), n_burnin + n_samples, 4\n)\n\nSampling (4 threads)   0%|                              |  ETA: N/A\n┌ Info: Found initial step size\n└   ϵ = 0.05\n┌ Info: Found initial step size\n└   ϵ = 0.2\n┌ Info: Found initial step size\n└   ϵ = 0.05\n┌ Info: Found initial step size\n└   ϵ = 0.2\nSampling (4 threads)  25%|███████▌                      |  ETA: 0:00:17\nSampling (4 threads)  50%|███████████████               |  ETA: 0:00:06\nSampling (4 threads)  75%|██████████████████████▌       |  ETA: 0:00:02\nSampling (4 threads) 100%|██████████████████████████████| Time: 0:00:05\nSampling (4 threads) 100%|██████████████████████████████| Time: 0:00:05\n\n\nChains MCMC chain (11000×17×4 Array{Float64, 3}):\n\nIterations        = 1001:1:12000\nNumber of chains  = 4\nSamples per chain = 11000\nWall duration     = 5.1 seconds\nCompute duration  = 19.61 seconds\nparameters        = log2ic50, a, b, c, σ²\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std      mcse     ess_bulk     ess_tail      rhat ⋯\n      Symbol   Float64   Float64   Float64      Float64      Float64   Float64 ⋯\n\n    log2ic50    3.1001    0.0297    0.0002   25150.3685   27935.5938    1.0002 ⋯\n           a    0.1163    0.0038    0.0000   16860.1510   22570.9089    1.0001 ⋯\n           b    6.7350    1.0786    0.0069   26759.0743   25218.0167    1.0001 ⋯\n           c    0.0712    0.0034    0.0000   16323.0552   21352.9296    1.0001 ⋯\n          σ²    0.0001    0.0000    0.0000   32838.7703   28866.3125    1.0001 ⋯\n                                                                1 column omitted\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5%\n      Symbol   Float64   Float64   Float64   Float64   Float64\n\n    log2ic50    3.0410    3.0804    3.1004    3.1202    3.1580\n           a    0.1089    0.1138    0.1163    0.1188    0.1238\n           b    5.0241    5.9818    6.5979    7.3261    9.2492\n           c    0.0643    0.0689    0.0712    0.0735    0.0778\n          σ²    0.0001    0.0001    0.0001    0.0001    0.0002\n\n\nLet’s look again at the corner plot for the chains.\n\n# Plot corner plot for chains\nPairPlots.pairplot(chain[n_burnin+1:end, :, :])\n\n\n\n\nCompared to the previous example where we didn’t remove the outliers, the posterior distributions are more concentrated, especially for the \\(b\\) parameter.\nLet’s now plot the posterior predictive check.\n\nRandom.seed!(42)\n\n# Define unique concentrations\nunique_concentrations = sort(unique(data_clean.concentration_ugmL))\n\n# Initialize matrix to store samples\ny_samples = Array{Float64}(\n    undef, length(unique_concentrations), n_samples\n)\n\n# Loop through samples\nfor i in 1:n_samples\n    # Generate mean for sample\n    y_samples[:, i] = logistic_log2(\n        log2.(unique_concentrations),\n        chain[:a][i],\n        chain[:b][i],\n        chain[:c][i],\n        chain[:log2ic50][i]\n    )\n    # Add noise\n    y_samples[:, i] .+= randn(length(unique_concentrations)) * √(chain[:σ²][i])\n    √(chain[:σ²][i])\nend # for\n\n# Initialize figure\nfig = Figure(size=(350, 300))\n# Add axis\nax = Axis(\n    fig[1, 1],\n    xlabel=\"antibiotic concentration\",\n    ylabel=\"optical density\",\n    title=\"Posterior Predictive Check\",\n    xscale=log2\n)\n\n# Plot samples\nfor i in 1:n_samples\n    lines!(\n        ax,\n        unique_concentrations,\n        y_samples[:, i],\n        color=(ColorSchemes.Paired_12[1], 0.5)\n    )\nend # for\n\n# Plot data\nscatter!(ax, data_clean.concentration_ugmL, data_clean.OD)\n\nfig\n\n\n\n\nThis look much better. Removing the outliers improved the fit significantly.",
    "crumbs": [
      "Notebooks",
      "Bayesian Inference of $IC_{50}$ Values"
    ]
  },
  {
    "objectID": "code/iwasawa_bayesian_inference.html#alternative-parameterization",
    "href": "code/iwasawa_bayesian_inference.html#alternative-parameterization",
    "title": "Bayesian Inference of \\(IC_{50}\\) Values",
    "section": "Alternative parameterization",
    "text": "Alternative parameterization\nIn the data, there are several concentrations at zero, which is not compatible with the log-scale model. However, we can rewrite Equation 1 without using the exponent as\n\\[\nf(x) = \\frac{a}\n{1+ \\left(\\frac{x}{\\mathrm{IC}_{50}}\\right)^b} + c\n\\tag{14}\\]\nLet’s define a new model using this parameterization.\n\n@doc raw\"\"\"\n    logistic_alt(x, a, b, c, ic50)\n\nCompute the logistic function used to model the relationship between antibiotic\nconcentration and bacterial growth.\n\nThis function implements the following equation:\n\nf(x) = a / (1 + (x / IC₅₀) ^ b) + c\n\n# Arguments\n- `x`: Antibiotic concentration (input variable)\n- `a`: Maximum effect parameter (difference between upper and lower asymptotes)\n- `b`: Slope parameter (steepness of the curve)\n- `c`: Minimum effect parameter (lower asymptote)\n- `ic50`: IC₅₀ parameter (concentration at which the effect is halfway between\n  the minimum and maximum)\n\n# Returns\nThe computed effect (e.g., optical density) for the given antibiotic\nconcentration and parameters.\n\nNote: This function is vectorized and can handle array inputs for `x`.\n\"\"\"\nfunction logistic_alt(x, a, b, c, ic50)\n    return @. a / (1.0 + (x / ic50)^b) + c\nend\n\nfunction logistic_alt(x, params)\n    return logistic_alt(x, params...)\nend\n\nWe will use the previous function to detect outliers in the data. Now, we re-define the Bayesian model using this parameterization.\n\nTuring.@model function logistic_alt_model(\n    x, y, prior_params::NamedTuple=NamedTuple()\n)\n    # Define default prior parameters\n    default_params = (\n        ic50=(0, 1),\n        a=(0, 1),\n        b=(0, 1),\n        c=(0, 1),\n        σ²=(0, 1)\n    )\n\n    # Merge default parameters with provided parameters\n    params = merge(default_params, prior_params)\n\n    # Define priors\n    ic50 ~ Turing.LogNormal(params.ic50...)\n    a ~ Turing.LogNormal(params.a...)\n    b ~ Turing.LogNormal(params.b...)\n    c ~ Turing.truncated(Turing.Normal(params.c...), 0, Inf)\n    σ² ~ Turing.truncated(Turing.Normal(params.σ²...), 0, Inf)\n\n    # Define likelihood\n    y ~ Turing.MvNormal(\n        logistic_alt(x, a, b, c, ic50),\n        LinearAlgebra.Diagonal(fill(σ², length(y)))\n    )\nend\n\nLet’s now perform inference on synthetic data using this new parameterization.\n\nRandom.seed!(42)\n# Define ground truth parameters\nic50_true = 2^0.5\na_true = 1.0\nb_true = 10.0\nσ²_true = 0.01\nc_true = 0.1\n\n# Simulate data\nx = 2 .^ LinRange(-2.5, 2.5, 15)\n# Define mean of data\nŷ = logistic_alt(x, a_true, b_true, c_true, ic50_true)\n# Add noise\ny = ŷ + randn(length(x)) * sqrt(σ²_true)\n\n# Initialize figure\nfig = Figure(size=(350, 300))\n# Add axis\nax = Axis(\n    fig[1, 1],\n    xlabel=\"antibiotic concentration\",\n    ylabel=\"optical density\",\n    xscale=log2\n)\n# Plot data\nscatterlines!(ax, x, y)\n\nfig\n\n\n\n\nWith the data simulated, let’s perform inference on the data.\n\nRandom.seed!(42)\n# Perform inference\nmodel = logistic_alt_model(x, y)\n\n# Define number of steps\nn_burnin = 10_000\nn_samples = 1_000\n\nchain = Turing.sample(\n    model, Turing.NUTS(), Turing.MCMCThreads(), n_burnin + n_samples, 4\n)\n\nSampling (4 threads)   0%|                              |  ETA: N/A\n┌ Info: Found initial step size\n└   ϵ = 0.4\n┌ Info: Found initial step size\n└   ϵ = 0.8\n┌ Info: Found initial step size\n└   ϵ = 0.8\n┌ Info: Found initial step size\n└   ϵ = 1.6\nSampling (4 threads)  25%|███████▌                      |  ETA: 0:00:19\nSampling (4 threads)  50%|███████████████               |  ETA: 0:00:06\nSampling (4 threads)  75%|██████████████████████▌       |  ETA: 0:00:02\nSampling (4 threads) 100%|██████████████████████████████| Time: 0:00:06\nSampling (4 threads) 100%|██████████████████████████████| Time: 0:00:06\n\n\nChains MCMC chain (11000×17×4 Array{Float64, 3}):\n\nIterations        = 1001:1:12000\nNumber of chains  = 4\nSamples per chain = 11000\nWall duration     = 5.62 seconds\nCompute duration  = 21.91 seconds\nparameters        = ic50, a, b, c, σ²\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std      mcse     ess_bulk     ess_tail      rhat ⋯\n      Symbol   Float64   Float64   Float64      Float64      Float64   Float64 ⋯\n\n        ic50    1.3615    0.0783    0.0006   21996.9818   15834.2853    1.0001 ⋯\n           a    0.9776    0.0641    0.0005   14338.2651   16755.4871    1.0000 ⋯\n           b    7.5525    2.9801    0.0243   15179.8530   18757.0920    1.0000 ⋯\n           c    0.1183    0.0456    0.0004   14321.6452   10118.2654    1.0002 ⋯\n          σ²    0.0102    0.0066    0.0001   14265.0218   17759.7256    1.0002 ⋯\n                                                                1 column omitted\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5%\n      Symbol   Float64   Float64   Float64   Float64   Float64\n\n        ic50    1.1980    1.3177    1.3639    1.4079    1.5098\n           a    0.8545    0.9372    0.9761    1.0169    1.1094\n           b    3.6070    5.6275    6.9973    8.7969   14.6326\n           c    0.0281    0.0886    0.1180    0.1470    0.2112\n          σ²    0.0037    0.0062    0.0085    0.0121    0.0270\n\n\nLet’s look at the posterior distribution of the parameters.\n\n# Plot corner plot for chains\nPairPlots.pairplot(\n    chain[n_burnin+1:end, :, :],\n    PairPlots.Truth(\n        (;\n        ic50=ic50_true,\n        a=a_true,\n        b=b_true,\n        c=c_true,\n        σ²=σ²_true\n    )\n    )\n)\n\n\n\n\nThe model is still able to recover the ground truth parameters. Let’s now plot the posterior predictive check.\n\nRandom.seed!(42)\n\n# Initialize matrix to store samples\ny_samples = Array{Float64}(undef, length(x), n_samples)\n\n# Loop through samples\nfor i in 1:n_samples\n    # Generate mean for sample\n    y_samples[:, i] = logistic_alt(\n        x,\n        chain[:a][i],\n        chain[:b][i],\n        chain[:c][i],\n        chain[:ic50][i]\n    )\n    # Add noise\n    y_samples[:, i] .+= randn(length(x)) * √(chain[:σ²][i])\nend # for\n\n# Initialize figure\nfig = Figure(size=(350, 300))\n# Add axis\nax = Axis(\n    fig[1, 1],\n    xlabel=\"antibiotic concentration\",\n    ylabel=\"optical density\",\n    title=\"Posterior Predictive Check\",\n    xscale=log2\n)\n\n# Plot samples\nfor i in 1:n_samples\n    lines!(ax, x, y_samples[:, i], color=(:gray, 0.05))\nend # for\n\n# Plot data\nscatterlines!(ax, x, y)\n\nfig\n\n\n\n\nEverything looks good. Let’s again test it on the Iwasawa et al. (2022) data.\n\nRandom.seed!(42)\n# Define prior parameters\nprior_params = (\n    a=(log(0.1), 0.1),\n    b=(0, 1),\n    c=(0, 1),\n    σ²=(0, 0.01)\n)\n# Define data\ndata = df_group[2]\n# Clean data\noutliers_idx = fit_logistic_and_detect_outliers(\n    log2.(data.concentration_ugmL), data.OD, threshold=2\n)\ndata_clean = data[.!outliers_idx, :]\n\n# Perform inference\nmodel = logistic_alt_model(\n    data_clean.concentration_ugmL,\n    data_clean.OD,\n    prior_params\n)\n\n# Define number of steps\nn_burnin = 10_000\nn_samples = 1_000\n\nchain = Turing.sample(\n    model, Turing.NUTS(), Turing.MCMCThreads(), n_burnin + n_samples, 4\n)\n\nSampling (4 threads)   0%|                              |  ETA: N/A\n┌ Info: Found initial step size\n└   ϵ = 0.00625\n┌ Info: Found initial step size\n└   ϵ = 0.00625\n┌ Info: Found initial step size\n└   ϵ = 0.00625\n┌ Info: Found initial step size\n└   ϵ = 0.0125\nSampling (4 threads)  25%|███████▌                      |  ETA: 0:00:17\nSampling (4 threads)  50%|███████████████               |  ETA: 0:00:06\nSampling (4 threads)  75%|██████████████████████▌       |  ETA: 0:00:02\nSampling (4 threads) 100%|██████████████████████████████| Time: 0:00:06\nSampling (4 threads) 100%|██████████████████████████████| Time: 0:00:06\n\n\nChains MCMC chain (11000×17×4 Array{Float64, 3}):\n\nIterations        = 1001:1:12000\nNumber of chains  = 4\nSamples per chain = 11000\nWall duration     = 5.48 seconds\nCompute duration  = 20.73 seconds\nparameters        = ic50, a, b, c, σ²\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std      mcse     ess_bulk     ess_tail      rhat ⋯\n      Symbol   Float64   Float64   Float64      Float64      Float64   Float64 ⋯\n\n        ic50    8.5831    0.1769    0.0011   25568.1627   25963.0962    1.0003 ⋯\n           a    0.1165    0.0038    0.0000   18041.1443   20901.4168    1.0002 ⋯\n           b    9.6366    1.5316    0.0097   26259.3918   25025.7026    1.0000 ⋯\n           c    0.0710    0.0035    0.0000   17294.7757   21118.9348    1.0002 ⋯\n          σ²    0.0001    0.0000    0.0000   29855.3383   28397.5187    1.0001 ⋯\n                                                                1 column omitted\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5%\n      Symbol   Float64   Float64   Float64   Float64   Float64\n\n        ic50    8.2417    8.4631    8.5828    8.6994    8.9339\n           a    0.1091    0.1139    0.1164    0.1190    0.1241\n           b    7.2053    8.5622    9.4488   10.4964   13.1592\n           c    0.0640    0.0687    0.0711    0.0734    0.0777\n          σ²    0.0001    0.0001    0.0001    0.0001    0.0002\n\n\nLet’s look at the posterior distribution of the parameters.\n\n# Plot corner plot for chains\nPairPlots.pairplot(chain[n_burnin+1:end, :, :])\n\n\n\n\nNothing looks obviously wrong. Let’s plot the posterior predictive check.\n\nRandom.seed!(42)\n\n# Define unique concentrations\nunique_concentrations = sort(unique(data_clean.concentration_ugmL))\n\n# Initialize matrix to store samples\ny_samples = Array{Float64}(\n    undef, length(unique_concentrations), n_samples\n)\n\n# Loop through samples\nfor i in 1:n_samples\n    # Generate mean for sample\n    y_samples[:, i] = logistic_alt(\n        unique_concentrations,\n        chain[:a][i],\n        chain[:b][i],\n        chain[:c][i],\n        chain[:ic50][i]\n    )\n    # Add noise\n    y_samples[:, i] .+= randn(length(unique_concentrations)) * √(chain[:σ²][i])\nend # for\n\n# Initialize figure\nfig = Figure(size=(350, 300))\n# Add axis\nax = Axis(\n    fig[1, 1],\n    xlabel=\"antibiotic concentration\",\n    ylabel=\"optical density\",\n    title=\"Posterior Predictive Check\",\n    xscale=log2\n)\n\n# Plot samples\nfor i in 1:n_samples\n    lines!(\n        ax,\n        unique_concentrations,\n        y_samples[:, i],\n        color=(ColorSchemes.Paired_12[1], 0.5)\n    )\nend # for\n\n# Plot data\nscatter!(ax, data_clean.concentration_ugmL, data_clean.OD)\n\nfig\n\n\n\n\nWith this parameterization, the model performs as well as the previous one.",
    "crumbs": [
      "Notebooks",
      "Bayesian Inference of $IC_{50}$ Values"
    ]
  },
  {
    "objectID": "code/iwasawa_bayesian_inference.html#conclusion",
    "href": "code/iwasawa_bayesian_inference.html#conclusion",
    "title": "Bayesian Inference of \\(IC_{50}\\) Values",
    "section": "Conclusion",
    "text": "Conclusion\nIn this notebook, we have performed Bayesian inference on the data from Iwasawa et al. (2022) using two different parameterizations of the logistic function. Both parameterizations perform equally well, and we are able to recover the ground truth parameters from simulated data. Furthermore, implementing an outlier detection scheme improved the fit significantly.",
    "crumbs": [
      "Notebooks",
      "Bayesian Inference of $IC_{50}$ Values"
    ]
  }
]