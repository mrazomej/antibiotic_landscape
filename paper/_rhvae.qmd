---
editor:
    render-on-save: true
bibliography: references.bib
csl: ieee.csl
---

## Riemannian Hamiltonian Variational Autoencoders Mathematical Background

The bulk of this work makes use of the variational autoencoder variation
developed by [@chadebec2020] known as Riemannian Hamiltonian Variational
Autoencoder (RHVAE). Understanding this model requires some background that we
provide in the following sections. It is not necessary to follow every
mathematical derivation in detail for these sections, as they don't directly
pertain to the main contribution of this work. However, we believe that some
intuition behind some of the concepts that inspired the RHVAE model can go a
long way to understand the model. We invite the reader to refer to the original
paper [@chadebec2020] and the references therein for additional details.

Let us start by reviewing the variational autoencoder model setting and the
variational inference framework.

### Variational Autoencoder Model Setting

Given a data set $\underline{x}^{1:N} \in \mathcal{X} \subseteq \mathbb{R}^D$,
where the superscript denotes a set of $N$ observations, i.e.,
$$
\underline{x}^{1:N} = 
\left\{
    \underline{x}_1, \underline{x}_2, \ldots, \underline{x}_N
\right\},
$${#eq-data-set}
where $\underline{x}_i \in \mathbb{R}^D$ for $i = 1, 2, \ldots, N$, the the
variational autoencoder aims to fit a joint distribution
$π_\theta(\underline{x}, \underline{z})$ where the data generative process is
assumed to involve a set of latent---unobserved---continuous variables
$\underline{z} \in \mathcal{Z} \subseteq \mathbb{R}^d$, where usually $d \ll D$.
In other words, the VAE assumes that the data we get to observe is a consequence
of a set of hidden variables $\underline{z}$ that we cannot measure directly. We
assume that these variables live in a much lower-dimensional space than the one
we observe and we are trying to learn this joint distribution such that, if
desired, we can generate new data points by sampling from this distribution.
Since the data we observe is a "consequence" of the latent variables, we express
this joint distribution as

$$
π_\theta(\underline{x}, \underline{z}) = 
π_\theta(\underline{x}|\underline{z}) \, π_\text{prior}(\underline{z}),
$${#eq-joint-dist}

where $π_\text{prior}(\underline{z})$ is the prior distribution over latent
variables and $π_\theta(\underline{x}|\underline{z})$ is the likelihood of the
data given the latent variables. This looks exactly as the problem one confronts
when performing Bayesian inference. What distinguishes the VAE from other
Bayesian inference problems, as we will see, is that in many cases, we do not
know the functional form of both the prior and the likelihood function that
generated our data, but we have enough data samples that we hope to be able to
learn these function using the neural networks function approximation
capabilities. Thus, the objective of the VAE is to parameterize this likelihood
function via a neural network with parameters $\theta$ given some prior
distribution over the latent variables. For simplicity, it is common to choose a
standard normal prior distribution, i.e., $π_\text{prior}(\underline{z}) =
\mathcal{N}(\underline{0}, \underline{\underline{I}}_d)$, where
$\underline{\underline{I}}_d$ is the $d$-dimensional identity matrix
[@kingma2014a;@kingma2019]. This way, once we have learned the parameters of the
neural network, we can generate new data points by sampling from this prior
distribution and running the resulting samples through the likelihood function
encoded by this neural network.

Our objective then becomes to find the parameters $\theta$ that maximize the
probability of observing the data we actually observed. This is equivalent to
maximizing the so-called marginal likelihood of the data, i.e.,

$$
\max_\theta \, π_\theta(\underline{x}) = 
\max_\theta \int d^d\underline{z} \, π_\theta(\underline{x}, \underline{z}).
$${#eq-marginal-likelihood}

But there are two problems with this objective:

1. The marginalization over the latent variables is intractable for most
   interesting distributions.
2. The distribution $π_\theta(\underline{x})$ is in general unknown and we
   need to somehow approximate it.

The way around these problems is to introduce a family of parametric
distributions $q_\phi(\underline{z} \mid \underline{x})$, i.e., distributions
that can be completely characterized by a set of parameters (think of Gaussians
being a parametric family on the mean and the variance), and to approximate the
true marginal distribution with one of these parametric distributions. This
conceptual change is at the core of approximate methods for Bayesian inference
known as variational inference. The objective then becomes to find the
parameters $\phi$ that, when used to approximate the posterior distribution
$π_\theta(\underline{z}|\underline{x})$, it is as close as possible to this
posterior distribution. To matematize this objective, we establish that we want
to minimize the Kullback-Leibler divergence (also known as the relative entropy)
between the variational distribution
$q_\phi(\underline{z} \mid \underline{x})$ and the posterior distribution
$π_\theta(\underline{z} \mid \underline{x})$, i.e., we aim to find $q_\phi^*$
such that

$$
q_\phi^*(\underline{z} \mid \underline{x}) = 
\min_\phi \, D_{KL}\left(
    q_\phi(\underline{z} \mid \underline{x}) \| 
    π_\theta(\underline{z} \mid \underline{x})
\right).
$${#eq-variational-approx}

After some algebraic manipulation involving the well-known Gibbs inequality, we
can show that this objective is equivalent to maximizing the so-called evidence
lower bound (ELBO), $\mathcal{L}$, of the marginal likelihood, i.e.,

$$
\mathcal{L} = \left\langle 
    \log π_\theta(\underline{x}, \underline{z}) - 
    \log q_\phi(\underline{z} \mid \underline{x}) 
\right\rangle_{q_\phi} 
\leq \log π_\theta(\underline{x}).
$${#eq-elbo}

Thus, the optimization problem we want to solve invovles estimating the ELBO
given some data $\underline{x}$. However, the ELBO is an expectation over the
variational distribution $q_\phi(\underline{z} \mid \underline{x})$, which is
unknown. Therefore, we need to somehow estimate this expectation. Arguably, the
most important contribution of the VAE framework is the introduction of the
reparametrization trick [@kingma2014a]. This seemingly innocuous trick allows us
to compute an unbiased estimate of the ELBO and its gradient, such that we can
maximize it via gradient ascent. For more details on the reparametrization
trick, we refer the reader to [@kingma2014a].

More recent attempts have focused on modifying $q_\phi(\underline{z} \mid
\underline{x})$ to get a better approximation of the true posterior
$π_\theta(\underline{z} \mid \underline{x})$. The basis of the Riemannian
Hamiltonian Variational Autoencoder (RHVAE) framework takes inspiration from
some of these approaches. Let's gain some intuition on each of them.

### MCMC and VI (Salimans & Kingma 2015)

One such approach to improve the accuracy of the variational posterior
approximation consists in adding a fix number of Markov Chain Monte Carlo (MCMC)
steps to the variational posterior approximation, targeting the true posterior
[@salimans2015]. In MCMC, rather than optimizing the parameters of a parametric
distribution, we sample an initial position $\underline{z}_0$ from a simple
distribution $q(\underline{z}_0)$ or $q(\underline{z}_0 \mid \underline{x})$ and
subsequently, apply a stochastic transition operator $T(\underline{z}_{t+1} \mid
\underline{z}_t)$ to draw a new value $\underline{z}_{t+1}$ from the
distribution

$$
\underline{z}_{t + 1} \sim T(\underline{z}_{t+1} \mid \underline{z}_t).
$${#eq-mcmc-transition}

Applying this operator over and over again, we build a Markov chain

$$
\underline{z}_0 \to \underline{z}_1 \to \underline{z}_2 \to \cdots 
\to \underline{z}_\tau,
$${#eq-mcmc-chain}

whose long-term behavior---the so-called stationary distribution---is a very
good approximation of the true posterior distribution $π_\theta(\underline{z}
\mid \underline{x})$. In other words, by carefully engineering the transition
operator $T(\underline{z}_{t+1} \mid \underline{z}_t)$, and applying it over and
over again, the "histogram" of the samples $\{\underline{z}_t\}_{t=0}^\tau$ will
converge to the true posterior distribution $π_\theta(\underline{z} \mid
\underline{x})$.

The central idea to combine MCMC with variational inference is to interpret the
Markov chain

$$
q_\phi(\underline{z} | \underline{x}) = 
q_\phi(\underline{z}_0 \mid \underline{x}) 
\prod_{t=1}^\tau T(\underline{z}_t \mid \underline{z}_{t-1}, \underline{x})
$${#eq-mcmc-chain-def}

as a variational approximation in an expanded space of variables that includes
the original latent variables $\underline{z}$ and the auxiliary variables
$\underline{Z} = \{\underline{z}_0, \underline{z}_1, \ldots,
\underline{z}_{\tau-1}\}$. Note that $\underline{Z}$ reaches up to $\tau-1$
steps, since we consider the final step $\underline{z}_\tau$ to be the original
latent variable $\underline{z}$. In other words, our variational distribution
now writes
$$
q_\phi(\underline{z}_\tau, \underline{Z} \mid \underline{x}) = 
q_\phi(\underline{z}_0 \mid \underline{x}) 
\prod_{t=1}^\tau T(\underline{z}_t \mid \underline{z}_{t-1}, \underline{x}).
$${#eq-mcmc-chain-def-expanded}

Integrating this expression into the ELBO ([@eq-elbo]) consists of substituting
the of substituting the variational distribution $q_\phi(\underline{z} \mid
\underline{x})$ with the expression in [@eq-mcmc-chain-def-expanded], i.e.,
$$
\mathcal{L}_{\text {aux }} =
\left\langle\log 
    \left[
        \pi_\theta\left(\underline{x}, \underline{z}_T\right) 
        r\left(\underline{Z} \mid \underline{z}_T, \underline{x}\right)
    \right] - 
    \log q\left(\underline{Z}, \underline{z}_T \mid \underline{x}\right)
\right\rangle_{q\left(\underline{Z}, \underline{z}_T \mid \underline{x}\right)}
$${#eq-elbo-aux}

where $r(\underline{Z} \mid \underline{z}_T, \underline{x})$ is an auxiliary
inference distribution that we can choose freely. This auxiliary distribution is
necessary to ensure we account for the auxiliary variables $\underline{Z}$ that
are now part of our variational distribution. By splitting the joint
distribution in [@eq-mcmc-chain-def-expanded] as

$$
q_\phi(\underline{z}_\tau, \underline{Z} \mid \underline{x}) =
q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x}) q_\phi(\underline{z}_\tau \mid \underline{x}),
$${#eq-mcmc-chain-def-expanded-2}

and substituting this expression into [@eq-elbo-aux], we get

$$
\mathcal{L}_{\text {aux }} =
\left\langle\log 
    \left[
        \pi_\theta\left(\underline{x}, \underline{z}_T\right) 
        r\left(\underline{Z} \mid \underline{z}_T, \underline{x}\right)
    \right] - 
    \log \left[q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x}) q_\phi(\underline{z}_\tau \mid \underline{x})\right]
\right\rangle_{q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x}) q_\phi(\underline{z}_\tau \mid \underline{x})}.
$${#eq-elbo-aux-2}

Rearranging the terms, we get
$$
\begin{aligned}
\mathcal{L}_{\text {aux }} 
& = \left\langle 
    \log \frac{\pi_\theta\left(\underline{x}, \underline{z}_\tau\right)}{q_\phi(\underline{z}_\tau \mid \underline{x})} - 
    \log \frac{q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x})}{r(\underline{Z} \mid \underline{z}_\tau, \underline{x})}
\right\rangle_{
    q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x})
    q_\phi(\underline{z}_\tau \mid \underline{x})
}, \\
& = \left\langle 
    \log \frac{\pi_\theta\left(\underline{x}, \underline{z}_\tau\right)}{q_\phi(\underline{z}_\tau \mid \underline{x})}
\right\rangle_{
    q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x})
    q_\phi(\underline{z}_\tau \mid \underline{x})
} - 
\left\langle
    \log \frac{q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x})}{r(\underline{Z} \mid \underline{z}_\tau, \underline{x})}
\right\rangle_{
    q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x})
    q_\phi(\underline{z}_\tau \mid \underline{x})
}.
\end{aligned}
$${#eq-elbo-aux-3}

From here, we note that the first term in [@eq-elbo-aux-3] is the original ELBO
with the extra expectation taken over the auxiliary variables $\underline{Z}$.
However, this term does not depend on the auxiliary variables $\underline{Z}$,
thus, we can take it out of the expectation, i.e.,

$$
\mathcal{L}_{\text {aux }} = \mathcal{L} - 
\left\langle 
    \log \frac{q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x})}{r(\underline{Z} \mid \underline{z}_\tau, \underline{x})}
\right\rangle_{
    q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x})
    q_\phi(\underline{z}_\tau \mid \underline{x})
}.
$${#eq-elbo-aux-4}

For the second term, taking the expectation over the auxiliary variables
makes it equivalent to the KL divergence between the variational distribution
$q_\phi(\underline{Z} \mid \underline{z}_\tau, \underline{x})$ and the auxiliary
distribution $r(\underline{Z} \mid \underline{z}_\tau, \underline{x})$, i.e.,

$$
\mathcal{L}_{\text {aux }} = \mathcal{L} - 
\left\langle 
    D_{K L}\left[
        q\left(\underline{Z} \mid \underline{z}_T, \underline{x}\right) \| 
        r\left(\underline{Z} \mid \underline{z}_T, \underline{x}\right)
    \right]
\right\rangle_{q\left(\underline{z}_T \mid \underline{x}\right)}
\leq \mathcal{L} \leq \log [\pi_\theta(\underline{x})].
$${#eq-elbo-aux-5}

where the inequality follows from the non-negativity of the KL divergence and
the fact that the ELBO is a lower bound to the marginal likelihood. Since the
variational distribution $q_\phi(\underline{z}_\tau,\mid \underline{x})$ we
care about comes from the marginalization of the variational distribution over
the auxiliary variables $\underline{Z}$, 

$$
q_\phi(\underline{z}_\tau \mid \underline{x}) = 
\int d\underline{Z} \, 
q_\phi(\underline{Z}, \underline{z}_\tau \mid \underline{x}),
$${#eq-mcmc-marginalization}

it is now a very rich family of distributions that can be used to better
approximate the true posterior distribution $π_\theta(\underline{z} \mid
\underline{x})$. However, we are still left with the task of choosing the
auxiliary distribution $r(\underline{Z} \mid \underline{z}_\tau,
\underline{x})$. What Salimans & Kingma [@salimans2015] propose is to define 
this distribution also as a Markov chain, but in reverse order, i.e.,

$$
r(\underline{Z} \mid \underline{z}_\tau, \underline{x}) = 
\prod_{t=1}^\tau 
T^\dagger(\underline{z}_{t-1} \mid \underline{z}_t, \underline{x}),
$${#eq-mcmc-chain-def-aux}

where $T^\dagger(\underline{z}_{t-1} \mid \underline{z}_t, \underline{x})$ is
the reverse of the transition operator $T(\underline{z}_{t+1} \mid
\underline{z}_t, \underline{x})$.  Notice that the dependence on
$\underline{z}_\tau$ is dropped. 

This choice of the auxiliary distribution leads to an upper bound on the
marginal log likelihood, of the form

$$
\begin{aligned}
\log \pi_\theta(\underline{x}) & \geq 
\left\langle
    \log \pi_\theta\left(\underline{x}, \underline{z}_\tau\right) -
    \log q_\phi\left(
        \underline{z}_0, \ldots, \underline{z}_\tau \mid \underline{x}
    \right) +
    \log r\left(
        \underline{z}_0, \ldots, \underline{z}_{\tau-1} \mid \underline{x}, \underline{z}_\tau
    \right)
\right\rangle_{q_\phi} \\
& = \left\langle
    \log \left[\frac{
        \pi_\theta\left(\underline{x}, \underline{z}_\tau\right)
        }{
            q_\phi\left(\underline{z}_0 \mid \underline{x}\right)
        }
    \right] +
    \sum_{t=1}^\tau \log \left[
        \frac{
            T^\dagger\left(\underline{z}_{t-1} \mid \underline{z}_t, \underline{x}\right)
        }{
            T\left(\underline{z}_t \mid \underline{z}_{t-1}, \underline{x}\right)
        }
    \right]
\right\rangle_{q_\phi}.
\end{aligned}
$${#eq-elbo-aux-6}

In other words, we can interpret the Markov chain as a parametric distribution
over the latent variables $\underline{z}$ that we can use to approximate the
true posterior distribution $π_\theta(\underline{z} \mid \underline{x})$.
The unbiased estimator of the marginal likelihood is then given by

$$
\hat{π}_\theta(\underline{x}) = 
\frac{
    π_\theta(\underline{x}, \underline{z}_T) 
    \prod_{t=1}^T T^\dagger(\underline{z}_{t-1} \mid \underline{z}_t, \underline{x})
}{
    q_\phi(\underline{z}_0 \mid \underline{x}) 
    \prod_{t=1}^T T(\underline{z}_t \mid \underline{z}_{t-1}, \underline{x})
}.
$${#eq-mcmc-approx}

Let's unpack this expression. When performing MCMC, we build a Markov chain

$$
\underline{z}_0 \to \underline{z}_1 \to \underline{z}_2 \to \cdots 
\to \underline{z}_T,
$${#eq-mcmc-chain}

where $\underline{z}_0 \sim q_\phi(\underline{z} \mid \underline{x})$ is the
initial position in the chain. At each step, we sample a new position from the
transition kernel $T(\underline{z}_{t} \mid \underline{z}_{t-1},
\underline{x})$, which is a function of the current position and the observed
data. This way, we build a Markov chain that converges to the true posterior
distribution. Therefore, the denominator of [@eq-mcmc-approx] computes the
probability of the initial position of the chain $q_\phi(\underline{z}_0 \mid
\underline{x})$ times the product of the transition probabilities for each of
the $\tau$ steps, $\prod_{t=1}^\tau T(\underline{z}_{t} \mid
\underline{z}_{t-1}, \underline{x})$.

The numerator of [@eq-mcmc-approx] computes the equivalent probability, but in
reverse order, i.e., starts by sampling the final position of the chain
$\underline{z}_T$ from the joint distribution $π_\theta(\underline{x},
\underline{z}_T)$ and then samples the previous positions of the chain
$\underline{z}_{T-1}, \underline{z}_{T-2}, \ldots, \underline{z}_0$ from the
transition kernel run backwards, $T^\dagger(\underline{z}_{t-1} \mid
\underline{z}_{t}, \underline{x})$. Including a Markov chain into the
variational posterior approximation has the effect of trading off computational
efficiency for better posterior approximation. In other words, to improve the
quality of the posterior approximation we make use of arguably the most accurate
family of inference methods known to date---Markov Chain Monte Carlo (MCMC)---at
the cost of increased computational cost. Intuitively, our estimate of the
unbiased gradient of the ELBO becomes more and more accurate the more steps we
include in the Markov chain, but at the same time, the cost of computing this
gradient becomes higher and higher.

### Normalizing Flows (Rezende & Mohamed 2015) {#sec-normalizing-flows}

Another approach to imrpove the posterior approximation is to consider the
composition of simple smooth invertible transformations [@rezende2016], such
that a variable $\underline{z}_K$ consists of $K$ of such transformations of a
random variable $\underline{z}_0$, sampled from a simple distribution, i.e.,
$\underline{z}_K$ is built from chaining $K$ simple transformations, $f_x^k$, of
$\underline{z}_0$,

$$
\underline{z}_K = 
f_x^K \circ f_x^{K-1} \circ \cdots \circ f_x^1(\underline{z}_0),
$${#eq-normalizing-flow}

where $f_i \circ f_j(x) = f_i(f_j(x))$. Because we define the transformations to
be smooth and invertible, the change of variables formula tells us that the
density of $\underline{z}_K$ writes

$$
q_\phi(\underline{z}_K \mid \underline{x}) = 
q_\phi(\underline{z}_0 \mid \underline{x}) \prod_{k=1}^K |\det J_{f_x^k}|^{-1},
$${#eq-flow-density}

where

$$
J_{f_x^k} = \frac{\partial f_x^k}{\partial \underline{z}},
$${#eq-jacobian}

is the Jacobian matrix of the transformation $f_x^k$. In other words, the
transformation of the random variable upon a composition of simple
transformations is a product of the Jacobians of each transformation. Thus, if
we choose a sequence of simple transformations, with a simple Jacobian matrix,
we can construct a more complex distribution over the latent variables. Again,
this has the effect of trading off computational efficiency for better posterior
approximation, this time in the form of smooth invertible transformations of the
original variational distribution. In other words, say that approximating the
posterior distribution using a simple distribution, such as a Gaussian, is
too simple. We can use a sequence of smooth invertible transformations to
transform this simple distribution into a more complex one that can better
approximate the true posterior. This again, comes at the cost of increased
computational cost.

### Hamiltonian Markov Chain Monte Carlo (HMC)

Although seemingly unrelated to the previous two approaches, the Hamiltonian
Markov Chain Monte Carlo (HMC) framework provides a way to construct a Markov
chain that is informed by the target distribution we are trying to approximate.
This property is exploited by the predecessor of the RHVAE framework, the
Hamiltonian Variational Autoencoder that we will review in the next section. But
first, let us give a brief overview of the main ideas of the HMC framework. We
direct the interested reader to the excellent review [@betancourt2017] for a
more detailed treatment of the topic.

In the HMC framework, a random variable $\underline{z}$ is assumed to live in an
Euclidean space with a target density $π(\underline{z} \mid \underline{x})$
derived from a potential $U_{\underline{x}}(\underline{z})$, such that the
distribution writes

$$
π(\underline{z} \mid \underline{x}) = 
\frac{e^{-U_{\underline{x}}(\underline{z})}}
{\int d^d\underline{z} \, e^{-U_{\underline{x}}(\underline{z})}},
$${#eq-hmc-dist}

where we define the potential energy as

$$
U_{\underline{x}}(\underline{z}) \equiv -\log π(\underline{z} \mid \underline{x}).
$${#eq-potential-energy}

Notice that upon substitution of this definition into
[@eq-hmc-dist], we get the trivial equality

$$
π(\underline{z} \mid \underline{x}) = 
\frac{\exp[-(-\log π(\underline{z} \mid \underline{x}))]}
{\int d^d\underline{z} \, \exp[-(-\log π(\underline{z} \mid \underline{x}))]} = 
\frac{
    π(\underline{z} \mid \underline{x})
}{
    \int d^d\underline{z} \, π(\underline{z} \mid \underline{x})
} = 
π(\underline{z} \mid \underline{x}).
$${#eq-trivial-equality}

Since it is most of the time impossible to sample directly from $π(\underline{z}
\mid \underline{x})$, an independent auxiliary variable $\underline{\rho} \in
\mathbb{R}^d$ is introduced and used to sample $\underline{z}$ more efficiently.
This variable---referred as the momentum---is such that:

$$
\underline{\rho} \sim \mathcal{N}(\underline{0}, \underline{\underline{M}}),
$${#eq-momentum-dist}

where $\underline{\underline{M}}$ is the so-called mass matrix. The idea behind
HMC is then to work with a distribution that extends the target distribution
$π(\underline{z} \mid \underline{x})$ to a distribution over both position and
momentum variables, i.e.,

$$
π(\underline{z}, \underline{\rho} \mid \underline{x}) = 
π(
    \underline{z} \mid \underline{x}, \underline{\rho})π(\underline{\rho} 
    \mid \underline{x}) = 
π(\underline{z} \mid \underline{x})π(\underline{\rho}),
$${#eq-extended-target}

where we assume the value of $\underline{z}$ does not directly depend on the
momentum variable $\underline{\rho}$ but only on the observed data
$\underline{x}$, and that the momentum distribution $π(\underline{\rho})$ is
independent of all other variables. Using [@eq-hmc-dist], the density of this
extended distribution can be analogously written as

$$
π(\underline{z}, \underline{\rho} \mid \underline{x}) = 
\frac{e^{-H_{\underline{x}}(\underline{z}, \underline{\rho})}}
{\displaystyle\iint 
    d^d\underline{z} \, 
    d^d\underline{\rho} \, 
    e^{-H_{\underline{x}}(\underline{z}, \underline{\rho})}
}.
$${#eq-extended-density}

where $H_{\underline{x}}(\underline{z}, \underline{\rho})$ is the so-called
Hamiltonian, defined as

$$
H_{\underline{x}}(\underline{z}, \underline{\rho}) = 
-\log π(\underline{z}, \underline{\rho} \mid \underline{x}) = 
-\log π(\underline{z} \mid \underline{x}) - \log π(\underline{\rho}).
$${#eq-hamiltonian}

Substituting $π(\underline{\rho})$ from [@eq-momentum-dist] into 
[@eq-hamiltonian] gives

$$
\begin{aligned}
H_{\underline{x}}(\underline{z}, \underline{\rho}) &= 
-\log π(\underline{z} \mid \underline{x}) + 
\frac{1}{2}
\left[\log((2π)^d|\underline{\underline{M}}|) 
+ \underline{\rho}^T \underline{\underline{M}}^{-1} \underline{\rho}\right] \\
&= U_{\underline{x}}(\underline{z}) + \kappa(\underline{\rho}).
\end{aligned}
$${#eq-hamiltonian-expanded}

In physical systems, the Hamiltonian gives the total energy of a system having a
position $\underline{z}$ and a momentum $\underline{\rho}$.
$U_{\underline{x}}(\underline{z})$ is referred as the potential energy (since it
is position-dependent) and $\kappa(\underline{\rho})$ is the kinetic energy,
since it is momentum-dependent.

The point of writing the extended target distribution in this form is that we
can take advantage of one specific desirable property of Hamiltonian dynamics:
Hamiltonian flows are volume preserving [@betancourt2017]. In other words, the
volume in phase space defined by the position and momentum variables is
invariant under Hamiltonian dynamics. What this means for our purposes of
sampling from a posterior distribution $\pi(\underline{z} \mid \underline{x})$
is that a walker starting in some position $\underline{z}$ can "traverse" the
volume of the target distribution very efficiently traveling through lines of
equal probability. In this sense, the Hamiltonian dynamics can be seen as a way
to explore the posterior distribution $π(\underline{z} \mid \underline{x})$ very
efficiently. To do so, we compute the time evolution of a walker exploring the
space of the position and momentum variables using Hamilton's equations $$
\begin{aligned} \frac{d\underline{z}}{dt} &= \frac{\partial
H_{\underline{x}}}{\partial \underline{\rho}}, \\ \frac{d\underline{\rho}}{dt} &=
-\frac{\partial H_{\underline{x}}}{\partial \underline{z}}. \end{aligned}
$${#eq-hamilton-eqs}

One can show that for the particular choice of momentum distribution in
[@eq-momentum-dist], these equations take the form
$$
\begin{aligned}
\frac{d\underline{z}}{dt} &= \underline{\underline{M}}^{-1} \underline{\rho}, \\
\frac{d\underline{\rho}}{dt} &= 
-\nabla_{\underline{z}} \log \pi(\underline{z} \mid \underline{x}),
\end{aligned}
$${#eq-hamilton-eqs-expanded}

where $\nabla_{\underline{z}}$ is the gradient with respect to the position
variable $\underline{z}$. Unfortunately, the resulting system of partial
differential equations is almost always intractable analytically. Therefore, we
must use specialized numerical integrators to solve it. The most popular choice
is the so-called leapfrog integrator, which is a symplectic integrator that
preserves the volume of the phase space. To update the position and momentum
variables, the leapfrog integrator takes three steps:

1. A half step for the momentum:

$$
\underline{\rho}\left(t + \frac{\epsilon}{2}\right) = 
\underline{\rho}(t) - 
\frac{\epsilon}{2} \nabla_{\underline{z}} H_{\underline{x}}\left(
    \underline{z}(t), \underline{\rho}(t)
\right).
$${#eq-leapfrog-half-step}

2. A full step for the position:

$$
\underline{z}\left(t + \epsilon\right) = 
\underline{z}(t) + 
\epsilon \nabla_{\underline{\rho}} H_{\underline{x}}\left(
    \underline{z}(t), \underline{\rho}\left(t + \frac{\epsilon}{2}\right)
\right).
$${#eq-leapfrog-full-step}

3. A half step for the momentum:

$$
\underline{\rho}\left(t + \frac{\epsilon}{2}\right) = 
\underline{\rho}\left(t + \frac{\epsilon}{2}\right) - 
\frac{\epsilon}{2} \nabla_{\underline{z}} H_{\underline{x}}\left(
    \underline{z}(t + \epsilon), \underline{\rho}\left(t + \frac{\epsilon}{2}\right)
\right),
$${#eq-leapfrog-half-step-2}

where $\epsilon$ is the so-called leapfrog step size.

Without going further into the details of the HMC framework, we hope the reader
can imagine how using this symplectic integrator, a walker aiming to explore the
posterior distribution $π(\underline{z} \mid \underline{x})$ can traverse the
volume of the target distribution very efficiently.

With this conceptual background, we are now ready to review the predecessor of
the RHVAE framework, the Hamiltonian Variational Autoencoder framework.

### Hamiltonian Variational Autoencoder (HVAE)

Given the dynamics defined in [@eq-hamilton-eqs-expanded], and the numerical
integrator defined in [@eq-leapfrog-half-step], [@eq-leapfrog-full-step], and
[@eq-leapfrog-half-step-2], we can now take $K$ iterations of the leapfrog
integrator to sample a point $(\underline{z}_K, \underline{\rho}_K)$ from the
extended target distribution defined in [@eq-extended-target]. This transition
from $(\underline{z}_0, \underline{\rho}_0)$ to $(\underline{z}_K,
\underline{\rho}_K)$ via the symplectic integrator can be thought of as a
transformation of the form

$$
\{\phi_{\epsilon,\underline{x}}^{(K)} : 
\mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}^d \times \mathbb{R}^d\},
$${#eq-iterates}

i.e., $\phi_{\epsilon,\underline{x}}^{(K)}$ is a function that takes some
input value $(\underline{z}, \underline{\rho})$ and advances it $K$ steps in
phase space using the leapfrog integrator. The function includes $\epsilon$ to
remind the step size of the integrator and $\underline{x}$ to remind its data
dependence. To advance $\ell$ steps, we simply compose a one step iteration
$(\ell=1)$ with the $\ell-1$ function, i.e.,

$$
\phi_{\epsilon,\underline{x}}^{(\ell)} = 
\phi_{\epsilon,\underline{x}}^{(\ell-1)} \circ \phi_{\epsilon,\underline{x}}^{(1)}
$${#eq-composition}

By induction, we can see that $\phi_{\epsilon,\underline{x}}^{(\ell)}$ can be
built by $\ell$ compositions of $\phi_{\epsilon,\underline{x}}^{(1)}$,
defining the entire transformation as

$$
\phi_{\epsilon,\underline{x}}^{(K)} = \phi_{\epsilon,\underline{x}}^{(1)} \circ
\phi_{\epsilon,\underline{x}}^{(1)} \circ \cdots \circ \phi_{\epsilon,\underline{x}}^{(1)}.
$${#eq-transformation}

It is no coincidence that this resembles the composition of functions used in
the normalizing flows framework described earlier in [@eq-normalizing-flow]. We
can then think of a $K$-step leapfrog integrator trajectory that takes

$$
\left(\underline{z}_0, \underline{\rho}_0\right) \rightarrow
\left(\underline{z}_K, \underline{\rho}_K\right)
$${#eq-trajectory}

as an invertible transformation formed by composing $K$ one-step leapfrog
integrator trajectories. The original HVAE framework [@caterini2018] includes an
extra step in between each leapfrog step that we now review. This step consisted
of a *tempering* step where an initial temperature $β_0$ is proposed and the
momentum is decreased by a factor

$$
α_k = \sqrt{\frac{β_{k-1}}{β_k}},
$${#eq-momentum-factor}

after each leapfrog step $k$. This simply means that for step $k$, the momentum
is computed as

$$
\underline{\rho}_k = α_k \, \underline{\rho}_{k-1}.
$${#eq-momentum-update}

The temperature is then updated as

$$
\sqrt{β_k} = \left[
    \left(1-\frac{1}{\sqrt{β_0}}\right)\frac{k^2}{K^2} + \frac{1}{\sqrt{β_0}}
\right]^{-1}.
$${#eq-temp-update}

This tempering step tries to produce an effect similar to that of Annealed
Importance Sampling (AIS) [@neal], where the temperature is decreased gradually
to produce a smoother distribution that can be better approximated by the
variational distribution. Thus, the transformation $\mathcal{H}_{\underline{x}}$
used in [@caterini2018] takes the form

$$
\mathcal{H}_{\underline{x}} = g^K \circ \phi_{\epsilon,\underline{x}}^{(1)}
\circ g^{K-1} \circ \cdots \circ \phi_{\epsilon,\underline{x}}^{(1)} \circ g^0
\circ \phi_{\epsilon,\underline{x}}^{(1)}.
$${#eq-transformation-hvae}

Since each transformation is smooth and differentiable, this entire
transformation is amenable to the reparameterization trick used in the
variational autoencoder framework. We can then use this transformation to 
access an unbiased estimator of the gradient of the ELBO with respect to the
variational parameters $\phi$.

As mentioned in [@sec-normalizing-flows], for a series of smooth invertible
transformations that define a normalizing flow, the change of variables formula
tells us that the density of the transformed variable writes

$$
q_\phi(\underline{z}_K \mid \underline{x}) = 
q_\phi(\underline{z}_0 \mid \underline{x}) 
|\det J_{\mathcal{H}_{\underline{x}}}|^{-1}.
$${#eq-flow-density-hvae}

For our extended posterior distribution, we have that the initial point is 
sampled as

$$
(\underline{z}_0, \underline{\rho}_0) \sim 
q_\phi(\underline{z}_0, \underline{\rho}_0 \mid \underline{x}) =
q_\phi(\underline{z}_0 \mid \underline{x}) \, q_\phi(\underline{\rho}_0),
$${#eq-extended-posterior-init}

meaning that only the initial position is sampled from the variational
distribution, while the momentum is sampled from a standard normal distribution.
For the Jacobian, each step consists of the composition of two functions:

1. The leapfrog integrator step $\phi_{\epsilon,\underline{x}}^{(1)}$.
2. The tempering step $g^k$.

Therefore, the determinant of the Jacobian of the transformation is given by the
product of the determinants of each of the steps, i.e.,

$$
|\det J_{\mathcal{H}_{\underline{x}}}| = 
\prod_{k=0}^K |\det J_{g^k}| 
|\det J_{\phi_{\epsilon,\underline{x}}^{(1)}}|.
$${#eq-hvae-jacobian}

The resulting variational distribution is then given by

$$
\begin{aligned}
q_{\phi}(\underline{z}_K, \underline{\rho}_K \mid \underline{x}) &= 
    q_{\phi}(\underline{z}_0 \mid \underline{x}) 
    q_{\phi}(\underline{\rho}_0) 
    |\det J_{\mathcal{H}_{\underline{x}}}|^{-1} \\
    &= 
    q_{\phi}(\underline{z}_0 \mid \underline{x}) 
    q_{\phi}(\underline{\rho}_0) 
    \left[
        \prod_{k=0}^K \left| \det J_{g^k} \right| 
        \left| \det J_{\phi_{\epsilon,\underline{x}}^{(1)}} \right|
    \right]^{-1}
\end{aligned}
$${#eq-hvae-variational-distribution}

However, recall that Hamiltonian flows are volume preserving. This means that
the volume of the phase space is invariant under the transformation and thus the
determinant of the Jacobian is equal to one, i.e.,

$$
|\det J_{\phi_{\epsilon,\underline{x}}^{(1)}}| = 1.
$${#eq-hvae-jacobian-det}

For the tempering step, since $(\underline{z}, \underline{\rho}) \rightarrow
(\underline{z}, \alpha_k\underline{\rho})$, the determinant of the Jacobian is
given by

$$
|\det J_{g^k}| = \alpha_k^d = \left(\frac{β_{k-1}}{β_k}\right)^{\frac{d}{2}}.
$${#eq-hvae-tempering-step-det}

With this setting in hand [@caterini2018] proposes an algorithm to estimate the
gradient of the ELBO with respect to the variational parameters that expands the
the usual procedure to includes a series of leapfrog steps and a tempering step
(see Algorithm 1 in [@caterini2018] for details).

### Riemannian Hamiltonian MCMC

To improve the representational power of the variational posterior
approximation, we can make the reasonable assumption that the latent variables
$\underline{z}$ live in a Riemannian manifold $\mathcal{M}$ endowed with a
Riemannian metric $\underline{\underline{G}}(\underline{z})$. Although not
entirely correct, one can think of this Riemannian metric as some sort of
"position dependent scale bar" for a flat map representing a curved space. In
the context of the HMC framework, this Riemannian metric can be included if we
allow the momentum variable to be given by

$$
\underline{\rho} \sim 
\mathcal{N}\left(\underline{0}, \underline{\underline{G}}(\underline{z})\right),
$${#eq-momentum-dist-rhmc}

i.e., the auxuliary momentum variable is no longer independent of the position
variable $\underline{z}$ but rather is distributed according to a normal
distribution with mean zero and covariance matrix given by the
position-dependent Riemannian metric $\underline{\underline{G}}(\underline{z})$.
The Hamiltonian governing the dynamics of the system then becomes

$$
H_{\underline{x}}^{\mathcal{M}}(\underline{z}, \underline{\rho}) = 
U_{\underline{x}}^{\mathcal{M}}(\underline{z}) + 
\kappa_{\underline{x}}^{\mathcal{M}}(\underline{z}, \underline{\rho}),
$${#eq-hamiltonian-rhmc}

where the $\mathcal{M}$ superscript reminds us that we now consider the latent
variables $\underline{z}$ living on a Riemannian manifold. Although the potential
energy $U_{\underline{x}}^{\mathcal{M}}(\underline{z})$ is still given by
[@eq-potential-energy], the kinetic energy term is now position-dependent, given
by

$$
\kappa_{\underline{x}}^{\mathcal{M}}(\underline{z}, \underline{\rho}) = 
\frac{1}{2} 
\log((2π)^d|\underline{\underline{G}}(\underline{z})|) +
\frac{1}{2} 
\underline{\rho}^T \underline{\underline{G}}(\underline{z})^{-1} \underline{\rho}
$${#eq-kinetic-energy-rhmc}

Our target distribution $\pi(\underline{z} \mid \underline{x})$ is still given
by [@eq-extended-target], but with the Hamiltonian given by
[@eq-hamiltonian-rhmc]. To show that we still recover the target distribution
with this change in the Hamiltonian, let us substitute [@eq-hamiltonian-rhmc]
into [@eq-extended-target]. This results in

$$
\pi(\underline{z} \mid \underline{x}) = 
\frac{
    \displaystyle\int d^d\underline{\rho} \, 
    \exp\left\{
        -U_{\underline{x}}^{\mathcal{M}}(\underline{z}) - 
        \frac{1}{2} \log((2π)^d|\underline{\underline{G}}(\underline{z})|) -
        \frac{1}{2} \underline{\rho}^T \underline{\underline{G}}(\underline{z})^{-1} \underline{\rho}
    \right\}
}{
    \displaystyle\iint d^d\underline{z} \, d^d\underline{\rho} \, 
    \exp\left\{
        -U_{\underline{x}}^{\mathcal{M}}(\underline{z}) - 
        \frac{1}{2} \log((2π)^d|\underline{\underline{G}}(\underline{z})|) -
        \frac{1}{2} \underline{\rho}^T \underline{\underline{G}}(\underline{z})^{-1} \underline{\rho}
    \right\}
}.
$${#eq-target-rhmc}

Rearranging terms, we can see that the target distribution is given by

$$
\pi(\underline{z} \mid \underline{x}) = 
\frac{
    e^{-U_{\underline{x}}^{\mathcal{M}}(\underline{z})} 
    \displaystyle\int d^d\underline{\rho} \, 
    \left[ 
        \frac{
            e^{-\frac{1}{2} 
            \underline{\rho}^T 
            \underline{\underline{G}}(\underline{z})^{-1} 
            \underline{\rho}}
        }{
            (2\pi)^d \sqrt{|\underline{\underline{G}}(\underline{z})|}
        } 
    \right]
}
{
    \displaystyle\int d^d\underline{z} \, 
    e^{-U_{\underline{x}}^{\mathcal{M}}(\underline{z})} 
    \displaystyle\int d^d\underline{\rho} \, 
    \left[ 
        \frac{
            e^{
                -\frac{1}{2}
                \underline{\rho}^T
                \underline{\underline{G}}(\underline{z})^{-1} 
                \underline{\rho}
            }
        }{
            (2\pi)^d \sqrt{|\underline{\underline{G}}(\underline{z})|}
        } 
    \right]
}.
$${#eq-target-rhmc-expanded}

where we can recognize the terms in square brackets as the probability density
function of a normal distribution. Thus, integrating out the momentum variable
$\underline{\rho}$ from the target distribution, we obtain

$$
\pi(\underline{z} \mid \underline{x}) = 
\frac{e^{-U_{\underline{x}}^{\mathcal{M}}(\underline{z})}}
{
    \displaystyle\int d^d\underline{z} \, 
    e^{-U_{\underline{x}}^{\mathcal{M}}(\underline{z})}
},
$${#eq-target-rhmc-marginal}

i.e., the correct target distribution as defined in [@eq-hmc-dist]. Therefore,
including the position-dependent Riemannian metric in the momentum variable
does not change the target distribution. However, the same cannot be said for
the Hamiltonian equations of motion, as we will see next.

Recall that the Hamiltonian equations in [@eq-hamilton-eqs] require us to
compute the gradient of the potential energy with respect to the position
variable $\underline{z}$ and the gradient of the kinetic energy with respect to
the momentum variable $\underline{\rho}$. For the position variable, our result
from [@eq-hamilton-eqs-expanded] still holds, with the only difference that the
mass matrix is now given by the Riemannian metric
$\underline{\underline{G}}(\underline{z})$. For a single entry of
$z_i$, dynamics are given by

$$
\frac{d z_i}{d t} = 
\frac{\partial H_{\underline{x}}^{\mathcal{M}}(\underline{z}, \underline{\rho})}
{
    \partial \rho_i
} = 
\left(
    \nabla_{\underline{z}} H_{\underline{x}}^{\mathcal{M}}
    (\underline{z}, \underline{\rho})
\right)_i = 
\left(
    \underline{\underline{G}}(\underline{z})^{-1} \underline{\rho}
\right)_i,
$${#eq-gradient-hamiltonian-position-rhmc}
where $\left(\cdot\right)_i$ denotes the $i$-th entry of a vector.

For the momentum variable, we use three relatively standard results from matrix
calculus:

1. The gradient of the inverse of a matrix function with respect to the argument
is given by

$$
\frac{
    \partial \underline{\underline{G}}(\underline{z})^{-1}
}{
    \partial \underline{z}
} = 
-\underline{\underline{G}}(\underline{z})^{-1} 
\frac{d \underline{\underline{G}}(\underline{z})}{d \underline{z}} 
\underline{\underline{G}}(\underline{z})^{-1}.
$${#eq-gradient-inverse-matrix}

2. The gradient of the determinant of a matrix function with respect to the 
argument is given by

$$
\frac{
    d
}{
    d \underline{z}
} \operatorname{det} \underline{\underline{G}}(\underline{z}) = 
\operatorname{tr}\left(
    \operatorname{adj}(\underline{\underline{G}}(\underline{z})) 
    \frac{d \underline{\underline{G}}(\underline{z})}{d \underline{z}}
\right) = 
\operatorname{det} \underline{\underline{G}}(\underline{z}) 
\operatorname{tr}\left(
    \underline{\underline{G}}(\underline{z})^{-1} 
    \frac{d \underline{\underline{G}}(\underline{z})}{d \underline{z}}
\right),
$${#eq-gradient-determinant-matrix}
where $\operatorname{adj}$ is the adjoint operator, $\operatorname{tr}$ is the
trace operator, and $\operatorname{det}$ is the determinant operator.

3. The gradient of the logarithm of the determinant of a matrix function with 
respect to the argument is given by
$$  
\frac{d}{d \underline{z}} 
\log (\operatorname{det} \underline{\underline{G}}(\underline{z})) = 
\operatorname{tr}\left(
    \underline{\underline{G}}(\underline{z})^{-1} 
    \frac{d \underline{\underline{G}}(\underline{z})}{d \underline{z}}
\right).
$${#eq-gradient-log-determinant-matrix}

Given these results, we can now compute the dynamics of the momentum variable.
For a single entry of $\underline{\rho}$, $\rho_i$, the resulting expression
is of the form

$$
\frac{d \rho_i}{d t} = 
\frac{
    \partial H_{\underline{x}}^{\mathcal{M}}(\underline{z}, \underline{\rho})
}{
    \partial z_i
} = 
\frac{\partial \ln \pi(\underline{z} \mid \underline{x})}{\partial z_i} -
\frac{1}{2} \operatorname{tr}\left(
    \underline{\underline{G}}(\underline{z}) 
    \frac{\partial \underline{\underline{G}}(\underline{z})}{\partial z_i}
\right) +
\frac{1}{2} 
\underline{\rho}^T 
\underline{\underline{G}}(\underline{z})^{-1} 
\frac{\partial \underline{\underline{G}}(\underline{z})}{\partial z_i} 
\underline{\underline{G}}(\underline{z})^{-1} \underline{\rho}.
$${#eq-gradient-hamiltonian-momentum-rhmc}

As before, we can adapt the leapfrog integrator to this setting to produce an
unbiased estimator of the gradient of the ELBO with respect to the variational
parameters. However, the previously presented leapfrog integrator is no longer
volume preserving in this setting due to the position-dependent Riemannian
metric. Thus, we need to use a generalized version of the leapfrog integrator.
This has been previously derived and [@chadebec2020] lists the following steps
for the generalized leapfrog integrator as

1. A half step for the momentum variable

$$
\underline{\rho}\left(t+\frac{\varepsilon}{2}\right) = 
\underline{\rho}(t) - 
\frac{\varepsilon}{2} 
\nabla_{\underline{z}} H_{\underline{x}}^{\mathcal{M}}\left(
    \underline{z}(t), 
    \underline{\rho}\left(t+\frac{\varepsilon}{2}\right)
\right).
$${#eq-generalized-leapfrog-momentum-half-step}

2. A full step for the position variable

$$
\underline{z}(t+\varepsilon) = 
\underline{z}(t) + 
\frac{\varepsilon}{2}
\left[
    \nabla_{\underline{z}} H_{\underline{x}}^{\mathcal{M}}\left(
        \underline{z}(t), 
        \underline{\rho}\left(t+\frac{\varepsilon}{2}\right)
    \right) +
    \nabla_{\underline{\rho}} H_{\underline{x}}^{\mathcal{M}}\left(
        \underline{z}(t+\varepsilon), 
        \underline{\rho}\left(t+\frac{\varepsilon}{2}\right)
    \right)
\right].
$${#eq-generalized-leapfrog-position-full-step}

3. A half step for the momentum variable

$$
\underline{\rho}(t+\varepsilon) = 
\underline{\rho}\left(t+\frac{\varepsilon}{2}\right) - 
\frac{\varepsilon}{2} 
\nabla_{\underline{z}} H_{\underline{x}}^{\mathcal{M}}\left(
    \underline{z}(t+\varepsilon), 
    \underline{\rho}\left(t+\frac{\varepsilon}{2}\right)
\right).
$${#eq-generalized-leapfrog-momentum-full-step}

Since the left hand side terms appear on the right hand side, these equations
must be solved using fixed point iterations. This means that the numerical
impolementation of this generalized integrator iterates a step multiple times 
until it finds a "fixed point", i.e., a point that, when fed to the right hand
side of the equation, does not change the value of the left hand side. In 
practice, we set the number of iterations to a small number.

### Riemannian Hamiltonian Variational Autoencoder

The Riemannian Hamiltonian Variational Autoencoder (RHVAE) method extends the
HVAE idea to account for non-Euclidean geometry of the latent space. This means
that the latent variables $\underline{z}$ live on a Riemannian manifold. This
manifold is endowed with a Riemannian metric
$\underline{\underline{G}}(\underline{z})$, that is position-dependent. We 
navigate through this manifold using the Hamiltonian equations of motion, 
utilizing the geometric information encoded in the Riemannian metric.

As with the HAVE, using the generalized leapfrog integrator
(@eq-generalized-leapfrog-momentum-half-step,
@eq-generalized-leapfrog-position-full-step,
@eq-generalized-leapfrog-momentum-full-step) along with a tempering step creates
a smooth mapping

$$
(\underline{z}_0, \underline{\rho}_0) \rightarrow 
(\underline{z}_K, \underline{\rho}_K),
$${#eq-rhvae-mapping}

that can be thought of as a kind of normalizing flow informed by the target
distribution and the geometry of the latent space. Using the same procedure as
the HVAE that includes a tempering step, the resulting variational distribution
takes the form of @eq-hvae-variational-distribution, with the only difference
that the momentum variable is now position-dependent, i.e.,

$$
q_{\phi}(\underline{z}_K, \underline{\rho}_K \mid \underline{x}) = 
q_{\phi}(\underline{z}_0 \mid \underline{x}) 
q_{\phi}(\underline{\rho}_0 \mid \underline{z}_0) 
\left[
    \prod_{k=1}^{K} 
    \left| \det \underline{\underline{J}}_{g^k} \right|
    \left| \det \underline{\underline{J}}_{\phi_{\epsilon,\underline{x}}^{(1)}} \right|
\right]^{-1},
$${#eq-rhvae-variational-distribution}

where, as before,
$\underline{\underline{J}}_{\phi_{\epsilon,\underline{x}}^{(1)}}$ is the
Jacobian of the generalized leapfrog integrator with step size $\epsilon$, and
$\underline{\underline{J}}_{g^k}$ is the Jacobian of the tempering step. Since
we established that Hamiltonian dynamics are volume preserving, the determinant
of the Jacobian of the tempering step is one, i.e., $\left| \det
\underline{\underline{J}}_{\phi_{\epsilon,\underline{x}}^{(1)}} \right| = 1$.
Moreover, since we are using the same type of tempering step as in the HVAE,
we have the same result for the determinant of the Jacobian of the tempering
step as in @eq-hvae-tempering-step-det. After substituting these results into
[@eq-rhvae-variational-distribution], we obtain

$$
q_{\phi}(\underline{z}_K, \underline{\rho}_K \mid \underline{x}) = 
q_{\phi}(\underline{z}_0 \mid \underline{x}) 
q_{\phi}(\underline{\rho}_0 \mid \underline{z}_0) 
\beta_0^{d / 2},
$${#eq-rhvae-variational-distribution-expanded}

where $d$ is the dimension of the latent space. The main difference from the
HVAE is that the term $q_{\phi}(\underline{\rho}_0 \mid \underline{z}_0)$
depends on the position $\underline{z}_0$ of the latent variables. To establish
this dependence, we must define the functional form for the metric tensor
$\underline{\underline{G}}(\underline{z})$.

In RHVAE, the metric is learned from the data. However, looking at the
generalized leapfrog integrator, we see that the inverse and the determinant of
the metric tensor are required. Thus, rather than defining the metric tensor
directly, we define the inverse of the metric tensor
$\underline{\underline{G}}(\underline{z})^{-1}$, and use the fact that

$$
\det \underline{\underline{G}}(\underline{z}) = 
\det \underline{\underline{G}}(\underline{z})^{-1}.
$${#eq-metric-determinant-equivalence}

By doing so, we do not have to invert the metric tensor for every leapfrog step.
@chadebec2020 proposes an inverse metric tensor of the form

$$
\underline{\underline{G}}(\underline{z}) =
\sum_{i=1}^N 
\underline{\underline{L}}_{\Psi_i} 
\underline{\underline{L}}_{\Psi_i}^{\top} 
\exp \left(
    -\frac{
        \left\| \underline{z} - \underline{\mu}{(\underline{x}_i)} \right\|_2^2
        }{
            T^2
        } 
    \right) + 
\lambda \underline{\underline{\mathbb{I}}}_l,
$${#eq-metric-tensor-rhvae}

where $\underline{\underline{L}}_{\Psi_i}$ is a lower triangular matrix with
positive diagonal entries, $\underline{\mu}(\underline{x}_i)$ is the mean of the
$i$-th component of the dataset, $T$ is a temperature parameter to smooth the
metric tensor, $\lambda$ is a regularization parameter, and
$\underline{\underline{\mathbb{I}}}_l$ is the $l \times l$ identity matrix. This
last term with $\lambda$ is set for the metric tensor not to be zero. However,
usually $\lambda$ is set to a small value, e.g., $10^{-3}$. The terms
$\underline{\mu}{(\underline{x}_i)}$ are referred to as the "*centroids*" and
are given by the mean of the variational posterior, such that,

$$
q_{\phi}(\underline{z}_i \mid \underline{x}_i) =
\mathcal{N}\left(
    \underline{\mu}{(\underline{x}_i)}, 
    \underline{\underline{\Sigma}}(\underline{x}_i)
\right).
$${#eq-metric-tensor-rhvae-centroids}

Intuitively, we can think of $\underline{\underline{L}}_{\Psi_i}$ as the
triangular matrix in the Cholesky decomposition of
$\underline{\underline{G}}^{-1}(\underline{\mu}{(\underline{x}_i)})$ up to a
regularization factor. This matrix is learned using a neural network with
parameters $\Psi_i$, mapping

$$
\underline{x}_i \rightarrow \underline{\underline{L}}_{\Psi_i}.
$${#eq-metric-tensor-rhvae-centroids-network}

The hyperparameters $T$ and $\lambda$ could be learned from the data as well,
but, for simplicity, we set them to constants. We invite the reader to refer to
[@chadebec2020] for a more detailed discussion on the training procedure.